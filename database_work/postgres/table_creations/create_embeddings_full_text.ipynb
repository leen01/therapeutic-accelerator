{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run /home/ubuntu/work/therapeutic_accelerator/scripts/base.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from transformers import T5Tokenizer # AutoModel, AutoTokenizer, BertTokenizer,BioGptModel, BioGptConfig, BioGptTokenizer\n",
    "\n",
    "import torch"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenizers and Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_sequence_length = 1200\n",
    "embedding_size = 200\n",
    "T5tokens = T5Tokenizer.from_pretrained('t5-base', model_max_length = max_sequence_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# bio_bert_model = AutoModel.from_pretrained(\"gsarti/biobert-nli\")\n",
    "# bio_bert_tokenizer = AutoTokenizer.from_pretrained(\"gsarti/biobert-nli\")\n",
    "# original_bert_tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "# T5Abstract_model = TFT5ForConditionalGeneration.from_pretrained('t5-base')\n",
    "# biogpttokenizer = BioGptTokenizer.from_pretrained(\"microsoft/biogpt\")\n",
    "# biogptmodel = BioGptModel.from_pretrained(\"microsoft/biogpt\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Custom embeddings function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from chromadb.api.types import Documents, EmbeddingFunction, Embeddings\n",
    "\n",
    "class MyEmbeddingFunction(EmbeddingFunction):\n",
    "    def __call__(self, texts: Documents) -> Embeddings:\n",
    "        #create document embeddings with T5\n",
    "        embeddings = []\n",
    "        for text in texts:\n",
    "            input_ids = T5tokens.encode(text)\n",
    "            input_ids = torch.tensor(input_ids).unsqueeze(0)\n",
    "            with torch.no_grad():\n",
    "                output = T5Abstract_model(input_ids)\n",
    "            embeddings.append(output[0][0][0].numpy())\n",
    "        return embeddings"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Langchain Sentence Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from abc import ABC\n",
    "from typing import List, Optional, Any\n",
    "\n",
    "import chromadb\n",
    "from langchain.docstore.document import Document\n",
    "from langchain.embeddings.base import Embeddings\n",
    "from langchain.vectorstores import Chroma\n",
    "from langchain.embeddings.openai import OpenAIEmbeddings\n",
    "from langchain.embeddings import OpenAIEmbeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from abc import ABC\n",
    "from typing import List, Optional, Any\n",
    "\n",
    "\n",
    "class CachedChroma(Chroma, ABC):\n",
    "    \"\"\"\n",
    "    Wrapper around Chroma to make caching embeddings easier.\n",
    "    \n",
    "    It automatically uses a cached version of a specified collection, if available.\n",
    "        Example:\n",
    "            .. code-block:: python\n",
    "                    from langchain.vectorstores import Chroma\n",
    "                    from langchain.embeddings.openai import OpenAIEmbeddings\n",
    "                    embeddings = OpenAIEmbeddings()\n",
    "                    vectorstore = CachedChroma.from_documents_with_cache(\n",
    "                        \".persisted_data\", texts, embeddings, collection_name=\"fun_experiement\"\n",
    "                    )\n",
    "        \"\"\"\n",
    "    \n",
    "    @classmethod\n",
    "    def from_documents_with_cache(\n",
    "            cls,\n",
    "            persist_directory: str,\n",
    "            documents: List[Document],\n",
    "            embedding: Optional[Embeddings] = None,\n",
    "            ids: Optional[List[str]] = None,\n",
    "            collection_name: str = Chroma._LANGCHAIN_DEFAULT_COLLECTION_NAME,\n",
    "            client_settings: Optional[chromadb.config.Settings] = None,\n",
    "            **kwargs: Any,\n",
    "    ) -> Chroma:\n",
    "        settings = chromadb.config.Settings(\n",
    "            chroma_db_impl=\"duckdb+parquet\",\n",
    "            persist_directory=persist_directory\n",
    "        )\n",
    "        client = chromadb.Client(settings)\n",
    "        collection_names = [c.name for c in client.list_collections()]\n",
    "\n",
    "        if collection_name in collection_names:\n",
    "            return Chroma(\n",
    "                collection_name=collection_name,\n",
    "                embedding_function=embedding,\n",
    "                persist_directory=persist_directory,\n",
    "                client_settings=client_settings,\n",
    "            )\n",
    "\n",
    "        return Chroma.from_documents(\n",
    "            documents=documents,\n",
    "            embedding=embedding,\n",
    "            ids=ids,\n",
    "            collection_name=collection_name,\n",
    "            persist_directory=persist_directory,\n",
    "            client_settings=client_settings,\n",
    "            **kwargs\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# openai embeddings model\n",
    "embeddings_model = OpenAIEmbeddings(openai_api_key=keys['openai_api_key'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# vectorstore = CachedChroma.from_documents_with_cache(\n",
    "#     \".persisted_data\", texts, embeddings, collection_name=\"fun_experiement\"\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a function that uses langchain to embed sentences\n",
    "def embed_sentences(sentences, tokenizer, model, max_sequence_length):\n",
    "    # tokenize the sentences\n",
    "    tokenized_sentences = tokenizer(sentences, padding=True, truncation=True, return_tensors=\"pt\", max_length=max_sequence_length)\n",
    "    # get the embeddings\n",
    "    with torch.no_grad():\n",
    "        model_output = model(**tokenized_sentences)\n",
    "    # get the embeddings from the model output\n",
    "    embeddings = model_output[0][:,0,:].numpy()\n",
    "    return embeddings"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create Encodings using Dask"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Full Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import dask\n",
    "import sqlalchemy as sa\n",
    "from dask import dataframe as dd\n",
    "from dask.delayed import delayed\n",
    "from dask.diagnostics import ProgressBar\n",
    "import json\n",
    "import re\n",
    "# from glob import glob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "table_name = 'fulltext'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dictionary(text):\n",
    "    \"\"\" Turn string containing list of dictionaries into a dictionary\"\"\"\n",
    "    \n",
    "    # remove new line characters\n",
    "    categories = re.sub(r'[\\[\\]\\'\\\\]', '', text)\n",
    "\n",
    "    # remove outer brackets, quotes, and split on commas\n",
    "    categories = categories.strip('{}').strip('\"').split('\",\"')\n",
    "\n",
    "    # create list with unique values from category\n",
    "    # categories = pd.Series([json.loads(t)['category'] for t in categories]).unique().tolist()\n",
    "    categories = [json.loads(t) for t in categories]\n",
    "    \n",
    "    return categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pull fulltext table and combine with attributes table for metadata embeddings\n",
    "sql = sa.text(f''' \n",
    "    SELECT * FROM {table_name} LEFT JOIN attributes ON ({table_name}.corpusid = CAST(attributes.corpusid as text)) LIMIT 10;\n",
    "''')\n",
    "\n",
    "with engine.connect() as conn: \n",
    "    query = conn.execute(sql)\n",
    "    \n",
    "ft = pd.DataFrame(query.fetchall())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get locations of figure captions to remvoe from full text\n",
    "ft['annotations.figurecaption'] = ft['annotations.figurecaption'].apply(json.loads)\n",
    "\n",
    "# turn strings into list of dictionaries\n",
    "ft['s2fieldsofstudy'] = ft['s2fieldsofstudy'].apply(create_dictionary).apply(lambda x: pd.Series([d['category'] for d in x]).unique().tolist())\n",
    "ft['authors'] = ft['authors'].apply(create_dictionary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_text_from_index(text, indexes): \n",
    "    \"\"\" index is a list of dictionary with start and end keys\"\"\"\n",
    "    # Looking at sections\n",
    "    section = {}\n",
    "    for i in indexes: \n",
    "        section['name'] = text['text'][i['start']:i['end']]\n",
    "        section['start'] = i['start']\n",
    "        section['end'] = i['end']\n",
    "        \n",
    "    return section"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Keep relevant columns\n",
    "ft = ft[\n",
    "    [\n",
    "        \"text\",\n",
    "        \"corpusid\",\n",
    "        \"title\",\n",
    "        \"s2fieldsofstudy\",\n",
    "        \"authors\",\n",
    "        \"venue\",\n",
    "        \"year\",\n",
    "        \"referencecount\",\n",
    "        \"citationcount\",\n",
    "        \"influentialcitationcount\",\n",
    "        \"isopenaccess\",\n",
    "        \"s2fieldsofstudy\",\n",
    "        \"publicationtypes\",\n",
    "        \"publicationdate\",\n",
    "        \"journal\",\n",
    "    ]\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.text_splitter import CharacterTextSplitter, RecursiveCharacterTextSplitter\n",
    "import tiktoken\n",
    "\n",
    "def token_len(text): \n",
    "    \"\"\" Get the length of tokens from text\"\"\"\n",
    "    tokens = T5tokens.encode(text)\n",
    "    return len(tokens)\n",
    "    \n",
    "# create text splitters for processing the texts\n",
    "text_splitter = CharacterTextSplitter(\n",
    "    separator = \"\\n\\n\",\n",
    "    chunk_size = 400,\n",
    "    chunk_overlap  = 200,\n",
    "    length_function = token_len,\n",
    ")\n",
    "\n",
    "recursive_splitter = RecursiveCharacterTextSplitter(\n",
    "    separators = [\"\\n\\n\", \"\\n\", \".\", \"?\", \"!\"],\n",
    "    chunk_size = 400,\n",
    "    chunk_overlap  = 20,\n",
    "    length_function = token_len,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from langchain.document_loaders.csv_loader import CSVLoader\n",
    "# loader = CSVLoader(file_path='./example_data/mlb_teams_2012.csv')\n",
    "# data = loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split the text into chunks\n",
    "documents = text_splitter.create_documents([ft.loc[0, 'text']])\n",
    "documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Create unique id for each chunk\n",
    "# import hashlib\n",
    "# m = hashlib.md5()\n",
    "# uid = m.hexdigest()[:12]\n",
    "\n",
    "# data = [{\n",
    "#     'id': f'{uid}-{i}',\n",
    "#     'text': chunk,\n",
    "#     'source': 'prompt',\n",
    "# } for i, chunk in enumerate(chunks)\n",
    "# ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create data for chroma\n",
    "data = [\n",
    "    {\n",
    "        'id': f'{ft.loc[0, \"corpusid\"]}-{i}',\n",
    "        'text': chunk,\n",
    "        'metadata': ft.iloc[0, 1:].to_dict()\n",
    "    } for i, chunk in enumerate(documents)\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create metadata dictionary\n",
    "metadata = ft.iloc[0, 1:].to_dict()\n",
    "\n",
    "for i, d in enumerate(documents): \n",
    "    d.metadata = metadata\n",
    "    \n",
    "documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# embed the documents\n",
    "encoded_documents = [\n",
    "    T5tokens.encode(d.page_content, return_tensors=\"pt\", max_length=512, truncation=True) for d in documents\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create me a function that will preprocess text to prepare to be used in a nlp model\n",
    "import unidecode\n",
    "\n",
    "def remove_accented_chars(text):\n",
    "    \"\"\"remove accented characters from text, e.g. caf√©\"\"\"\n",
    "    text = unidecode.unidecode(text)\n",
    "    return text\n",
    "\n",
    "def preprocess_text(text):\n",
    "    # remove the new lines\n",
    "    text = text.replace('\\n', ' ')\n",
    "    # remove multiple spaces\n",
    "    text = re.sub(r'\\s+', ' ', text)\n",
    "    \n",
    "    text = remove_accented_chars(text)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs = [Document(page_content=t) for t in text[:3]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create connection to postgres db\n",
    "# from sqlalchemy.engine.url import URL\n",
    "\n",
    "# postgres_db = {'drivername': 'postgres',\n",
    "#                'database': 'postgres',\n",
    "#                'username': 'postgres',\n",
    "#                'password': keys[\"postgres\"],\n",
    "#                'host': config[\"database\"][\"host\"],\n",
    "#                'port': 5432}\n",
    "# print(URL(**postgres_db))\n",
    "# postgres = URL(**postgres_db)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_sql_table('fulltext', con = f'postgresql://postgres:{keys[\"postgres\"]}@{config[\"database\"][\"host\"]}:5432/postgres')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ddf = dd.read_sql_table('fulltext', \n",
    "                        con = f'postgresql://postgres:{keys[\"postgres\"]}@{config[\"database\"][\"host\"]}:5432/postgres',\n",
    "                        index_col = 'id',\n",
    "                        head_rows = 10,\n",
    "                        npartitions = 100)\n",
    "\n",
    "# Remove unnecessary columns\n",
    "# ddf = ddf.drop(columns = ['index'])\n",
    "\n",
    "# Remove empty abstract rows\n",
    "# ddf = ddf.dropna(how = 'all', subset='abstract').reset_index(drop = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "for x in xl_files:\n",
    "    parts = dask.delayed(try_to_read)(x)\n",
    "    # filter_df = dask.delayed(get_techniques)(parts)\n",
    "    output.append(parts)\n",
    "\n",
    "# convert to a single dataframe\n",
    "df_total = dd.from_delayed(output)\n",
    "\n",
    "# df_total.visualize()\n",
    "\n",
    "with ProgressBar():\n",
    "    ddf = df_total.compute()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Upload to Postgresql DB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sql = text(''' \n",
    "#     SELECT EXISTS (\n",
    "#         SELECT FROM information_schema.tables \n",
    "#         WHERE    table_name   = 'abstracts'\n",
    "#     );\n",
    "# ''')\n",
    "\n",
    "# with engine.connect() as conn: \n",
    "#     conn.execute(sql)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create array columns to store encoding and mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "table_name = 'abstracts_encodings'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "delete_table = False\n",
    "\n",
    "if delete_table: \n",
    "    sql = text(f''' \n",
    "        DROP TABLE IF EXISTS {table_name}};\n",
    "    ''')\n",
    "\n",
    "    with engine.connect() as conn: \n",
    "        query = conn.execute(sql)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create table\n",
    "# Create Table in DB first before uploading\n",
    "from sqlalchemy import MetaData, Table, Column, Integer, String, ARRAY\n",
    "\n",
    "metadata_obj = MetaData()\n",
    "\n",
    "# Create abstracts metadata\n",
    "abstracts = Table(\n",
    "    table_name,\n",
    "    metadata_obj,\n",
    "    Column(\"paperId\", String, nullable = True),\n",
    "    Column(\"corpusId\", String, nullable=True),\n",
    "    Column(\"abstract\", String, nullable = True),\n",
    "    Column(\"input_ds\", ARRAY(Integer), nullable=True),\n",
    "    Column(\"attention_mask\", ARRAY(Integer), nullable=True),\n",
    ")\n",
    "\n",
    "metadata_obj.create_all(engine)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Upload dask dataframe to psql\n",
    "ddf = ddf.to_sql(name = table_name, uri = str(url_object), if_exists = 'replace', index = False, chunksize = 10000, method = 'multi')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if it worked\n",
    "import pandas as pd\n",
    "from sqlalchemy import text\n",
    "\n",
    "table_name = 'abstracts_encodings'\n",
    "\n",
    "sql = text(f''' \n",
    "    SELECT * FROM {table_name} LIMIT 5;\n",
    "''')\n",
    "\n",
    "with engine.connect() as conn: \n",
    "    query = conn.execute(sql)\n",
    "\n",
    "test = pd.DataFrame(query.fetchall())\n",
    "test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if it worked\n",
    "import pandas as pd\n",
    "from sqlalchemy import text\n",
    "\n",
    "table_name = 'fulltext'\n",
    "\n",
    "sql = text(f''' \n",
    "    SELECT * FROM {table_name} LIMIT 5;\n",
    "''')\n",
    "\n",
    "with engine.connect() as conn: \n",
    "    query = conn.execute(sql)\n",
    "\n",
    "test = pd.DataFrame(query.fetchall())\n",
    "test.head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create Vectore Store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.embeddings.openai import OpenAIEmbeddings\n",
    "from langchain.vectorstores import Chroma\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "from langchain.llms import OpenAI\n",
    "from langchain.chains import RetrievalQA\n",
    "\n",
    "llm = OpenAI(temperature=0)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
