{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import sqlalchemy as sa\n",
    "\n",
    "from transformers import T5Tokenizer\n",
    "\n",
    "import dask\n",
    "from dask import dataframe as dd\n",
    "from dask.diagnostics import ProgressBar\n",
    "import dask.distributed as distributed\n",
    "\n",
    "import warnings\n",
    "import logging\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "%run /home/ubuntu/work/therapeutic_accelerator/scripts/base.py\n",
    "\n",
    "max_sequence_length = 1200\n",
    "embedding_size = 200\n",
    "\n",
    "# Create tokenizer for T5 model\n",
    "T5tokens = T5Tokenizer.from_pretrained('t5-base', model_max_length = max_sequence_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create dask cluster\n",
    "dask.config.set(scheduler='processes')  # overwrite default with multiprocessing scheduler\n",
    "\n",
    "cluster = distributed.LocalCluster(name='local', n_workers=7, memory_limit = '4GiB', threads_per_worker=4)  # Launches a scheduler and workers locally\n",
    "client = distributed.client._get_global_client() or distributed.Client(cluster)\n",
    "client"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "# import tiktoken\n",
    "\n",
    "# @dask.delayed\n",
    "def token_len(text): \n",
    "    \"\"\" Get the length of tokens from text\"\"\"\n",
    "    tokens = T5tokens.encode(text)\n",
    "    return len(tokens)\n",
    "    \n",
    "chunk_size = 2000\n",
    "\n",
    "# create text splitters for processing the texts\n",
    "text_splitter = CharacterTextSplitter(\n",
    "    separator = \"\\n\\n\",\n",
    "    chunk_size = chunk_size,\n",
    "    chunk_overlap  = 20,\n",
    "    length_function = token_len\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Now with Dask"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Functions to clean up dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @dask.delayed\n",
    "def create_doc(split_text, corpusid):\n",
    "    \"\"\" Create documents for each chunk \"\"\"\n",
    "        \n",
    "    try:\n",
    "        docs = {\n",
    "            \"documents\": split_text, # list of all documents [doc1, doc2, doc3, ...]\n",
    "            'ids': [f'{corpusid}-{i}' for i in range(len(split_text))], # list of all ids [id1, id2, id3, ...]\n",
    "            'metadatas': [{'corpusid': int(corpusid), 'chunk': i} for i in range(len(split_text))] # list of dictionaries with metadata for each document\n",
    "        }\n",
    "        return docs\n",
    "\n",
    "    except Exception as e:\n",
    "        logging.error(e)\n",
    "\n",
    "\n",
    "def mp_create_doc(ddf): \n",
    "    \"\"\" Used for mapping partitions\"\"\"\n",
    "    return ddf.apply(lambda x: create_doc(x['split_text'], x['corpusid']), axis=1)\n",
    "\n",
    "\n",
    "def split_text(ddf):\n",
    "    \"\"\" Split text into chunks \"\"\"\n",
    "    return ddf['text'].apply(text_splitter.split_text)\n",
    "\n",
    "\n",
    "def add_to_collection(text, corpusid):\n",
    "    \n",
    "    doc = create_document(text, corpusid)\n",
    "    \n",
    "    try:\n",
    "        dask.delayed(collection.add)(**doc)\n",
    "    except Exception as e:\n",
    "        logging.error(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in fulltext from csvs for dask\n",
    "ft = dd.read_parquet('/home/ubuntu/work/data/fulltext_parquets/fulltext-*.parquet', sample=10000000,\n",
    "                     sample_rows=10,\n",
    "                     lineterminator=None,\n",
    "                     dtype={'corpusid': 'int', 'text': 'object'})\n",
    "\n",
    "# Cleanup dataframes\n",
    "ft = ft.map_partitions(pd.DataFrame.dropna, subset='text')\n",
    "\n",
    "ft = ft.map_partitions(pd.DataFrame.drop_duplicates, subset='text')\n",
    "\n",
    "ft = ft.map_partitions(pd.DataFrame.reset_index, drop=True)\n",
    "\n",
    "ft = ft.persist()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split the full text into chunks and add to collection\n",
    "ft_split = ft.assign(split_text=ft.map_partitions(split_text, meta=('text', 'object')))\n",
    "\n",
    "# No longer need text column\n",
    "ft_split = ft_split.drop('text', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create documents for chroma\n",
    "ft_docs = ft_split.map_partitions(mp_create_doc, meta=('docs', 'object'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ft_docs.to_csv('/home/ubuntu/work/data/fulltext_docs_csvs/fulltext_docs-*.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add documents to collection\n",
    "def add_to_collection(docs):\n",
    "    \"\"\" Add documents to collection \"\"\"\n",
    "    try:\n",
    "        collection.add(**docs)\n",
    "    except Exception as e:\n",
    "        logging.error(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ft_docs.apply(add_to_collection, axis=1, meta=('docs', 'object'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "collection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_to_csv(partition):\n",
    "    cli_comp = [client.persist(x) for x in partition] # gets delayed dask objects and puts them in cluster\n",
    "    result = [x.persist() for x in cli_comp]\n",
    "    path = f'/home/ubuntu/work/data/fulltext_docs/fulltext_docs-{i}.csv'\n",
    "    if os.path.exists(path): \n",
    "        next\n",
    "    else:\n",
    "        pd.Series(result).to_csv(path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Llama Indexing for Chroma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import chromadb\n",
    "from chromadb.config import Settings\n",
    "from chromadb.utils import embedding_functions\n",
    "\n",
    "# Create chroma client\n",
    "# chroma_client = chromadb.Client()\n",
    "chroma = chromadb.Client(Settings(chroma_api_impl=\"rest\",\n",
    "                                chroma_server_host=\"44.204.90.95\",  # EC2 instance public IPv4\n",
    "                                chroma_server_http_port=8000))\n",
    "\n",
    "# returns a nanosecond heartbeat. Useful for making sure the client remains connected.\n",
    "print(\"Nanosecond heartbeat on server\", chroma_client.heartbeat())\n",
    "\n",
    "# Check Existing connections\n",
    "chroma_client.list_collections()\n",
    "\n",
    "default_ef = embedding_functions.DefaultEmbeddingFunction()\n",
    "collection = chroma_client.get_or_create_collection(\"fulltext\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    # chroma = chromadb.Client(Settings(chroma_api_impl=\"rest\",\n",
    "    #                                 chroma_server_host=\"44.204.90.95\",  # EC2 instance public IPv4\n",
    "    #                                 chroma_server_http_port=8000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index import VectorStoreIndex, SimpleDirectoryReader\n",
    "from llama_index.vector_stores import ChromaVectorStore\n",
    "from IPython.display import Markdown, display\n",
    "from llama_index.storage.storage_context import StorageContext\n",
    "\n",
    "vector_store = ChromaVectorStore(chroma_collection=collection)\n",
    "storage_context = StorageContext.from_defaults(vector_store=vector_store)\n",
    "# index = VectorStoreIndex.from_documents(documents, storage_context=storage_context)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load documents\n",
    "documents = SimpleDirectoryReader(\n",
    "    \"/home/ubuntu/work/data/fulltext_docs/\"\n",
    ").load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set up ChromaVectorStore and load in data\n",
    "vector_store = ChromaVectorStore(chroma_collection=chroma_collection)\n",
    "storage_context = StorageContext.from_defaults(vector_store=vector_store)\n",
    "index = VectorStoreIndex.from_documents(\n",
    "    documents, storage_context=storage_context, embed_model=embed_model\n",
    ")\n",
    "\n",
    "# Query Data\n",
    "query_engine = index.as_query_engine(chroma_collection=chroma_collection)\n",
    "response = query_engine.query(\"What did the author do growing up?\")\n",
    "display(Markdown(f\"<b>{response}</b>\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check Work"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "collection.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "collection.query(\n",
    "    n_results=10,\n",
    "    where={\"corpusid\": \"1353942\"}\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Embed the text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModel\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained('allenai/specter')\n",
    "model = AutoModel.from_pretrained('allenai/specter')\n",
    "\n",
    "# @dask.delayed\n",
    "def tokenize(text):\n",
    "    inputs = tokenizer(text, padding=True, truncation=True, return_tensors=\"pt\", max_length=512)\n",
    "    return inputs\n",
    "\n",
    "# @dask.delayed\n",
    "def get_embeddings(inputs):\n",
    "    result = model(**inputs).last_hidden_state[:, 0, :].tolist()\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = tokenize(res[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embed = get_embeddings(inputs)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "chroma",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
