{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bertqa = TFBertForQuestionAnswering.from_pretrained('bert-base-uncased')\n",
    "berttokenizer = BertTokenizer.from_pretrained('bert-base-uncased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "53\n",
      ". but it's precisely to avoid a concentration of\n"
     ]
    }
   ],
   "source": [
    "sample_question = \"question: What is the capital city of Switzerland?\"\n",
    "sample_context = \"context: Many people are surprised when they hear that pretty little Bern is the Swiss capital. Surely industrial Zurich or international Geneva would be more logical, they say. But it's precisely to avoid a concentration of power that Bern was chosen as the “federal city” exactly 170 years ago.\"\n",
    "\n",
    "tokenized_qa = berttokenizer([sample_question + sample_context], return_tensors ='tf')\n",
    "model_output = bertqa(**tokenized_qa)\n",
    "\n",
    "answer_start_index = int(tf.math.argmax(model_output.start_logits, axis=-1)[0])\n",
    "answer_end_index = int(tf.math.argmax(model_output.end_logits, axis=-1)[0])\n",
    "print(answer_end_index)\n",
    "answer_encoded = tokenized_qa.input_ids[0,answer_start_index:answer_end_index]\n",
    "start_answer = tokenized_qa.input_ids[0][answer_start_index]\n",
    "print(berttokenizer.decode(answer_encoded, skip_special_tokens=True, clean_up_tokenization_spaces=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Bern']\n"
     ]
    }
   ],
   "source": [
    "qa_tokens = T5tokens([sample_question + sample_context], return_tensors='tf')\n",
    "t5_model_out = T5Abstract_model.generate(qa_tokens.input_ids,\n",
    "                                         num_beams=3,\n",
    "                                         no_repeat_ngram_size=2,\n",
    "                                         max_length = 3,\n",
    "                                         min_length=2)\n",
    "print([T5tokens.decode(i, skip_special_tokens=True, clean_up_tokenization_spaces=True) for i in t5_model_out])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
