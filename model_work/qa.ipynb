{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a0d0a1c8",
   "metadata": {},
   "source": [
    "# Document Question Answering\n",
    "\n",
    "An example of using Chroma DB and LangChain to do question answering over documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52da15b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.text_splitter import CharacterTextSplitter, RecursiveCharacterTextSplitter\n",
    "from langchain.vectorstores import Chroma\n",
    "\n",
    "from langchain.embeddings import OpenAIEmbeddings\n",
    "\n",
    "from langchain.text_splitter import CharacterTextSplitter, RecursiveCharacterTextSplitter\n",
    "\n",
    "from langchain.llms import OpenAI\n",
    "from langchain.chains import VectorDBQA\n",
    "from langchain.document_loaders import TextLoader\n",
    "\n",
    "import sqlalchemy\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4cb901f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import functions from a file stored in a different directory\n",
    "import sys\n",
    "sys.path.append('/home/ubuntu/work/therapeutic_accelerator/scripts/utils')\n",
    "sys.path.append('/home/ubuntu/work/therapeutic_accelerator/scripts/database')\n",
    "\n",
    "from db_tools import db_connection\n",
    "from utils import import_config\n",
    "\n",
    "config, keys = import_config()\n",
    "\n",
    "engine = db_connection(\n",
    "    password=keys[\"postgres\"], host=config[\"database\"][\"host\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01fe5351",
   "metadata": {},
   "source": [
    "## Load documents\n",
    "\n",
    "Load documents to do question answering over. If you want to do this over your documents, this is the section you should replace."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b352847",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retreive Full Text from Table\n",
    "table_name = \"fulltext\"\n",
    "\n",
    "sql = sqlalchemy.text(\n",
    "    f\"\"\" \n",
    "    SELECT * FROM {table_name} LIMIT 10;\n",
    "    \"\"\"\n",
    ")\n",
    "\n",
    "with engine.connect() as conn:\n",
    "    query = conn.execute(sql)\n",
    "    full_text = pd.DataFrame(query.fetchall())\n",
    "\n",
    "# full_text.head()\n",
    "\n",
    "example = full_text.loc[0, 'text']\n",
    "\n",
    "# loader = TextLoader('state_of_the_union.txt')\n",
    "# documents = loader.load()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "478be861",
   "metadata": {},
   "source": [
    "## Split documents\n",
    "\n",
    "Split documents into small chunks. This is so we can find the most relevant chunks for a query and pass only those into the LLM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dd1adb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=1000, chunk_overlap=0, separators = ['\\n\\n', '\\n'])\n",
    "\n",
    "# texts = text_splitter.split_documents(example)\n",
    "\n",
    "texts = text_splitter.create_documents([example])\n",
    "texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a55b1c6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index import Document, VectorStoreIndex\n",
    "import re\n",
    "\n",
    "text_list = [re.sub(\"\\n\", \" \", p.page_content) for p in texts]\n",
    "documents = [Document(text=re.sub(\"\\s\\s+\", \" \", t)) for t in text_list]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9b30aff",
   "metadata": {},
   "source": [
    "## Initialize ChromaDB\n",
    "\n",
    "Create embeddings for each chunk and insert into the Chroma vector database."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d8f5d4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # create T5 model for summarization\n",
    "# from transformers import (\n",
    "#     T5Tokenizer,\n",
    "#     TFT5Model,\n",
    "#     TFT5ForConditionalGeneration)\n",
    "\n",
    "# T5tokens = T5Tokenizer.from_pretrained(\"t5-base\")\n",
    "\n",
    "# # tokenize text\n",
    "# T5tokens.tokenize(example)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d27429e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModel\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"allenai/specter\")\n",
    "\n",
    "model = AutoModel.from_pretrained(\"allenai/specter\")\n",
    "\n",
    "# take the first token in the batch as the embedding\n",
    "\n",
    "class specter_embeddings: \n",
    "    def __init__(self, tokenizer, model):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.model = model\n",
    "        \n",
    "    def embed_documents(self, text):\n",
    "        inputs = self.tokenizer(item, padding=True, truncation=True, return_tensors=\"pt\", max_length=512)\n",
    "        result = self.model(**inputs)\n",
    "        embeddings = result.last_hidden_state[:, 0, :]\n",
    "        return embeddings\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e4f6841",
   "metadata": {},
   "outputs": [],
   "source": [
    "spect_embeds = specter_embeddings(tokenizer, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63b2448a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Spectre embeddings are trained on a corpus of 1.5 billion words from Wikipedia and the web.\n",
    "# # Use a pipeline as a high-level helper\n",
    "# from transformers import pipeline\n",
    "\n",
    "# pipe = pipeline(\"feature-extraction\", model=\"allenai/specter\", tokenizer=\"allenai/specter\")\n",
    "# inputs = pipe.tokenizer(example, padding=True, truncation=True, return_tensors=\"pt\", max_length=512)\n",
    "# pipe.model(**inputs).last_hidden_state[:, 0, :]\n",
    "\n",
    "# pipe.model(**pipe.tokenizer(example))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b497b6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import chromadb\n",
    "\n",
    "client = chromadb.Client()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0d2a049",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.environ[\"OPENAI_API_KEY\"] = keys[\"openai\"]\n",
    "\n",
    "from chromadb.api.types import Documents, EmbeddingFunction, Embeddings\n",
    "from chromadb.utils import embedding_functions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4dc2a32",
   "metadata": {},
   "outputs": [],
   "source": [
    "from chromadb.api.types import Documents, EmbeddingFunction, Embeddings\n",
    "\n",
    "class MyEmbeddingFunction(EmbeddingFunction):\n",
    "    def __init__(self, model, tokenizer):\n",
    "        self.model = model\n",
    "        self.tokenizer = tokenizer\n",
    "    \n",
    "    def embed_documents(self, texts: Documents) -> Embeddings:\n",
    "        texts = [t.replace(\"\\n\", \" \") for t in texts]\n",
    "        # embed the documents somehow\n",
    "        embeddings = []\n",
    "        for text in texts:\n",
    "            inputs = tokenizer(text, padding=True, truncation=True, return_tensors=\"pt\", max_length=512)\n",
    "            result = model(**inputs)\n",
    "            embeddings.append(result.last_hidden_state[:, 0, :])\n",
    "        \n",
    "        return embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "722b8c4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "chroma_client = chromadb.Client()\n",
    "chroma_collection = chroma_client.create_collection(\"fulltext_specter\", embedding_function=MyEmbeddingFunction(model, tokenizer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97834f81",
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the the current cell or a previous cell. Please review the code in the cell(s) to identify a possible cause of the failure. Click <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "vectordb = Chroma.from_documents(texts, MyEmbeddingFunction(tokenizer, model), persist_directory='/home/ubuntu/work/therapeutic_accelerator/chroma/fulltext')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60d7edba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# specter_ef = embedding_functions.HuggingFaceEmbeddingFunction(\n",
    "#     api_key=keys['huggingFace'], # Replace with your own HuggingFace API key\n",
    "#     model_name=\"allenai/specter\"\n",
    "# )\n",
    "\n",
    "# huggingface_collection = client.create_collection(name=\"specter_embeddings\", embedding_function=specter_ef)\n",
    "\n",
    "# # embeddings = OpenAIEmbeddings()\n",
    "# # def embedding_fn(text):\n",
    "# #     return spect_embeds.embed_documents(text)\n",
    "\n",
    "# vectordb = Chroma.from_documents(texts, specter_ef, persist_directory='/home/ubuntu/work/therapeutic_accelerator/chroma/fulltext')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eafb329e",
   "metadata": {},
   "outputs": [],
   "source": [
    "dir(specter_ef)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ddb7866",
   "metadata": {},
   "source": [
    "## Create the chain\n",
    "\n",
    "Initialize the chain we will use for question answering."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35164427",
   "metadata": {},
   "outputs": [],
   "source": [
    "qa = VectorDBQA.from_chain_type(\n",
    "    llm=OpenAI(), chain_type=\"stuff\", vectorstore=vectordb)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cff96efe",
   "metadata": {},
   "source": [
    "## Ask questions!\n",
    "\n",
    "Now we can use the chain to ask questions!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5851c0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"What did the president say about Ketanji Brown Jackson\"\n",
    "qa.run(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24cf9016",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
