{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BioGpt\n",
    "- Create connection between chroma and BioGPT\n",
    "- Create connection between postgresql DB and BioGPT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "# setup\n",
    "\n",
    "# Base\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "\n",
    "# LLM packages\n",
    "from transformers import pipeline, set_seed\n",
    "from transformers import AutoTokenizer, AutoModelWithLMHead, BioGptTokenizer, BioGptForCausalLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chunk context into 512  tokens\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "# import tiktoken\n",
    "\n",
    "# @dask.delayed\n",
    "def token_len(text): \n",
    "    \"\"\" Get the length of tokens from text\"\"\"\n",
    "    tokens = tokenizer(text, padding=True, truncation=True, return_tensors=\"pt\", max_length=512)['input_ids'][0]\n",
    "    return len(tokens)\n",
    "    \n",
    "chunk_size = 1024\n",
    "\n",
    "# create text splitters for processing the texts\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    # separator = [\"\\n\\n\", \"\\n\", \". \", \"? \", \"! \", \"; \"],\n",
    "    chunk_size = chunk_size,\n",
    "    chunk_overlap  = 20,\n",
    "    length_function = token_len\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Specter Embedder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create embeddings function with specter model\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained('allenai/specter')\n",
    "model = AutoModel.from_pretrained('allenai/specter')\n",
    "\n",
    "from chromadb.api.types import Documents, EmbeddingFunction, Embeddings\n",
    "\n",
    "class specter_ef(EmbeddingFunction):\n",
    "    def __init__(self, model, tokenizer):\n",
    "        self.model = model\n",
    "        self.tokenizer = tokenizer\n",
    "    \n",
    "    def embed_documents(self, texts: Documents) -> Embeddings:\n",
    "        \n",
    "        text_list = [re.sub(\"\\n\", \" \", p) for p in texts]\n",
    "        texts = [re.sub(\"\\s\\s+\", \" \", t) for t in text_list]\n",
    "        \n",
    "        # embed the documents somehow\n",
    "        embeddings = []\n",
    "        \n",
    "        for text in texts:\n",
    "            inputs = tokenizer(text, padding=True, truncation=True, return_tensors=\"pt\", max_length=512)\n",
    "            result = model(**inputs)\n",
    "            embeddings.append(result.last_hidden_state[:, 0, :])\n",
    "        \n",
    "        return embeddings\n",
    "    \n",
    "specter_embeder = specter_ef(model, tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q&A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Define your desired data structure.\n",
    "# class qa(BaseModel):\n",
    "#     setup: str = Field(description=\"Ask a question\")\n",
    "#     answer: str = Field(description=\"Answer to the question\")\n",
    "    \n",
    "#     # You can add custom validation logic easily with Pydantic.\n",
    "#     @validator('setup')\n",
    "#     def question_ends_with_question_mark(cls, field):\n",
    "#         if field[-1] != '?':\n",
    "#             raise ValueError(\"Badly formed question!\")\n",
    "#         return field"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chroma Connection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nanosecond heartbeat on server 1689689383721754531000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[Collection(name=langchain_store),\n",
       " Collection(name=abstracts),\n",
       " Collection(name=fulltext),\n",
       " Collection(name=specter_abstracts)]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import chromadb\n",
    "from chromadb.config import Settings\n",
    "\n",
    "# Create chroma client\n",
    "chroma = chromadb.Client(Settings(chroma_api_impl=\"rest\",\n",
    "                                  chroma_server_host=\"34.238.51.66\", # EC2 instance public IPv4\n",
    "                                  chroma_server_http_port=8000))\n",
    "\n",
    "print(\"Nanosecond heartbeat on server\", chroma.heartbeat()) # returns a nanosecond heartbeat. Useful for making sure the client remains connected.\n",
    "\n",
    "# Check Existing connections\n",
    "display(chroma.list_collections())\n",
    "\n",
    "collection = chroma.get_or_create_collection(\"specter_abstracts\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # ChromaDB Query\n",
    "\n",
    "# collection.query(\n",
    "#     query_texts=[\"doc10\", \"thus spake zarathustra\", ...],\n",
    "#     n_results=10,\n",
    "#     where={\"metadata_field\": \"is_equal_to_this\"},\n",
    "#     where_document={\"$contains\":\"search_string\"}\n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Work"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Xformers is not installed correctly. If you want to use memory_efficient_attention to accelerate training use the following command to install Xformers\n",
      "pip install xformers.\n"
     ]
    }
   ],
   "source": [
    "biogpttokenizer = BioGptTokenizer.from_pretrained(\"microsoft/biogpt\")\n",
    "biogptmodel = BioGptForCausalLM.from_pretrained(\"microsoft/biogpt\")\n",
    "generator = pipeline('text-generation', model=biogptmodel, tokenizer=biogpttokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# User Input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>User</th>\n",
       "      <th>Task</th>\n",
       "      <th>Prompt</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>General</td>\n",
       "      <td>QA</td>\n",
       "      <td>What is the most current research on pancreati...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>General</td>\n",
       "      <td>QA</td>\n",
       "      <td>What recent therapeutics have come out for lun...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>General</td>\n",
       "      <td>QA</td>\n",
       "      <td>How does ELISA assays work?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>General</td>\n",
       "      <td>QA</td>\n",
       "      <td>What are the common use cases for flow cytometry?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>General</td>\n",
       "      <td>QA</td>\n",
       "      <td>How does lentivirus transductions work?</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      User Task                                             Prompt\n",
       "0  General   QA  What is the most current research on pancreati...\n",
       "1  General   QA  What recent therapeutics have come out for lun...\n",
       "2  General   QA                        How does ELISA assays work?\n",
       "3  General   QA  What are the common use cases for flow cytometry?\n",
       "4  General   QA            How does lentivirus transductions work?"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompts = pd.read_csv('/home/ubuntu/work/therapeutic_accelerator/data/prompts.csv')\n",
    "# testing prompt one\n",
    "prompts.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "biogpt_tests = [\n",
    "    'How the ELISA assay works', \n",
    "    'Effective treatments of pancreatic cancer are',\n",
    "    'Effective treatments of alzheizmers are',\n",
    "    'Current treatments for parkinson\\'s disease are'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'generated_text': 'How the ELISA assay works for the detection of all forms of the human immunodeficiency virus (HIV) using a single antibody-capture technique.'},\n",
       " {'generated_text': 'How the ELISA assay works with serum, we then examined the relationship between serum IGF-I and the duration of exposure to lead.'},\n",
       " {'generated_text': 'How the ELISA assay works best for detection of IgG.'},\n",
       " {'generated_text': 'How the ELISA assay works for the detection ofF.F.n. with different species might be a good strategy to overcome some of the current problems, such as the lack of an appropriate reference sera.'},\n",
       " {'generated_text': 'How the ELISA assay works reliably does not suggest that the IgG-isotype response is a specific biomarker for disease status of HCV infection and has a poor correlation with the levels of IgM or IgA.'}]"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generator(\"How the ELISA assay works\", max_length=150, num_return_sequences=5, do_sample=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generator(\"COVID-19 is\", max_length=20, num_return_sequences=5, do_sample=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# User input\n",
    "# question = input(\"What is your question? \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_question_embeddings(question): \n",
    "    # Embed question\n",
    "    question_embeddings = specter_embeder.embed_documents([question])[0][0].tolist()\n",
    "    \n",
    "    return question_embeddings\n",
    "\n",
    "def query_chroma(question_embeddings):\n",
    "    # Query ChromaDB with Embeddings\n",
    "    results = collection.query(\n",
    "        query_embeddings=[question_embeddings],\n",
    "        n_results=10\n",
    "        # where={\"metadata_field\": \"is_equal_to_this\"},\n",
    "        # where_document={\"$contains\":\"search_string\"}\n",
    "    )\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "question_embeddings = get_question_embeddings(question)\n",
    "results = query_chroma(question_embeddings)\n",
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Microvascular density (MVD), a marker for tumor angiogenesis, has been demonstrated to have prognostic significance in various malignancies. Previous studies have demonstrated that MVD is an independent prognostic factor in pancreatic adenocarcinoma and that longer survival is associated with hypovascular tumors. The prognostic importance of MVD in pancreatic neuroendocrine tumor (NET) has not been documented. We evaluated MVD in pancreatic NET and correlated it with clinicopathologic features and patient outcome to determine whether MVD is a useful prognostic indicator for these patients. Twenty-five pancreatic NETs from our archival files resected between 1981 and 2000 were identified. The mean MVD was determined for each tumor from the 3 most vascularized 200 × fields. Clinical follow-up ranged from 1 to 19 years, with a mean of 4.9 years. At last follow-up, 6 patients were dead of disease, 10 patients were alive without disease, 4 patients were alive with disease, and 5 patients were alive with disease status unknown. Mean MVD ranged from 43 to 527 microvessels per 200 × field. MVD did not correlate with tumor size, the examined histologic parameters, or patient outcome. MVD in pancreatic NET does not correlate with the clinicohistologic features evaluated in this study or with the patient outcome and is not a useful prognostic indicator in these patients. These results suggest that factors other than the simple number of microvessels are important in determining pancreatic NET behavior. However, most tumors were highly vascular, and additional studies may be helpful to clarify further the role of vascularity and assess the utility of antiangiogenic agents in the treatment of pancreatic NET.',\n",
       " 'Background/Aim: This study aimed to identify risk factors for recurrence of patients with stage III colorectal cancer by assessing clinicopathological features. Patients and Methods: The study included 231 patients with stage III colorectal cancer who underwent curative resection between 2006 and 2012 at the Department of Surgery of the Jikei University Hospital, Tokyo, Japan. Clinicopathological data of the patients were retrospectively evaluated. Results: The recurrence rate was 27.7% (64/231) in the study group. The univariate analysis for recurrence identified five risk factors: site of primary tumor (rectal cancer), surgical procedure (open surgery), preoperative serum CEA level (>5 ng/ml), preoperative serum CA19-9 level (>37 U/ml), and number of metastatic lymph nodes (over three metastases). The multivariate analysis for recurrence identified three risk factors: rectal cancer, preoperative serum CEA level >5.0 ng/ml 95%, and more than three metastatic lymph nodes. Conclusion: The risk factors for stage III colorectal cancer recurrence seem to be rectal cancer, preoperative serum CEA level >5.0 ng/ml, and more than three metastatic lymph nodes.',\n",
       " 'Abstract Purpose Colon cancer is one of the malignant tumors that threatens human health. miR-510 was demonstrated to play roles in the progression of various cancers; its dysregulation was speculated to be associated with the development of colon cancer. Methods One hundred and thirteen colon cancer patients participated in this research. With the help of RT-qPCR, the expression of miR-510 in collected tissues and cultured cells was analyzed. The association between miR-510 expression level and clinical features and prognosis of patients was evaluated. Moreover, the effects of miR-510 on cell proliferation, migration, and invasion of colon cancer were assessed by CCK8 and Transwell assay. Results miR-510 significantly upregulated in colon cancer tissues and cell lines relative to the adjacent normal tissues and colonic cells. The expression of miR-510 was significantly associated with the TNM stage and poor prognosis of patients, indicating miR-510 was involved in the disease progression and clinical prognosis of colon cancer. Additionally, the upregulation of miR-510 significantly promoted cell proliferation, migration, and invasion of colon cancer, while its knockdown significantly inhibited these cellular processes. SRCIN 1 was the direct target of miR-510 during its promoted effect on the development of colon cancer. Conclusion The upregulation of miR-510 acts as an independent prognostic indicator and a tumor promoter by targeting SRCIN 1 in colon cancer, which provides novel therapeutic strategies for colon cancer.',\n",
       " 'Prostate cancer (PC) is a heterogeneous disease characterized by variable morphological patterns. Thus, establishing a patient-derived xenograft (PDX) model that retains the key features of the primary tumor for each type of PC is important for appropriate evaluation. In this study, we established PDX models of hormone-naïve (D17225) and castration-resistant (B45354) PC by implanting fresh tumor samples, obtained from patients with advanced PC under the renal capsule of immune-compromised mice. Supplementation with exogenous androgens shortened the latent period of tumorigenesis and increased the tumor formation rate. The PDX models exhibited the same major genomic and phenotypic features of the disease in humans and maintained the main pathological features of the primary tumors. Moreover, both PDX models showed different outcomes after castration or docetaxel treatment. The hormone-naïve D17225 PDX model displayed a range of responses from complete tumor regression to overt tumor progression, and the development of castrate-resistant PC was induced after castration. The responses of the two PDX models to androgen deprivation and docetaxel were similar to those observed in patients with advanced PC. These new preclinical PC models will facilitate research on the mechanisms underlying treatment response and resistance.',\n",
       " 'Patient: Female, 67 Final Diagnosis: Intrahepatic cholangiocarcinoma Symptoms: Atypical chest pain Medication: — Clinical Procedure: Liver biopsy Specialty: Internal Medicine/Oncology Objective: Rare disease Background: Intrahepatic cholangiocarcinoma is a rare condition which typically occurs in males between 50 and 70 years of age, and presents with symptoms related to biliary obstruction including jaundice, pruritus, and dark urine. Other common symptoms at presentation include abdominal pain, weight loss, and fever. Case Report: We present a case of a 67-year-old female initially presenting with chest pain at rest, found to have a lung nodule on diagnostic imaging at the time of admission. On further imaging, a 9 cm liver lesion was incidentally discovered, initially suspicious for hepatocellular carcinoma on imaging, with initial biopsy staining CK7 positive, and CK20 negative. The patient also had an elevated alpha-fetoprotein level. Biopsy results were later confirmed as moderately differentiated adenocarcinoma consistent with intrahepatic cholangiocarcinoma. Conclusions: This report illustrates an unusual presentation of intrahepatic cholangiocarcinoma. Although rare, cholangio-carcinoma is diagnosed most frequently as an incidental finding on imaging studies. With quick work-up and successful biopsy results, patients can undergo surgical or chemo-radiation therapy earlier, potentially leading to a longer survival time.',\n",
       " 'Purpose: Recent studies have suggested that osteopontin is induced by hypoxia in head and neck cancer cell lines and its plasma level may serve as a surrogate marker for tumor hypoxia and treatment outcome in head and neck cancer. We investigated the response of osteopontin to in vitro hypoxia in nasopharyngeal carcinoma cell lines, and determined plasma osteopontin levels in nasopharyngeal carcinoma patients, nonnasopharyngeal carcinoma head and neck cancer patients, and healthy controls. We explored the relationship of plasma osteopontin and response to radiotherapy in nasopharyngeal carcinoma. Experimental Design: Nasopharyngeal carcinoma cell lines HK1, HONE-1, C666-1, and CNE-2 were treated with 0 to 48 hours of hypoxia or normoxia, +/− reoxygenation. Osteopontin secretion in the supernatant was measured by ELISA assay. Cellular osteopontin protein and mRNA were detected by Western blotting and reverse transcription-PCR, respectively. Plasma osteopontin levels in patients (n = 66; 44 nasopharyngeal carcinoma, 22 head and neck cancer) and controls (n = 29) were measured by ELISA. Results: Hypoxia has no effect on osteopontin protein and mRNA level in nasopharyngeal carcinoma cells. Only CNE-2 secreted osteopontin, and there was no significant induction by hypoxia. Plasma osteopontin levels in patients of metastatic nasopharyngeal carcinoma and head and neck cancer, but not in locoregional nasopharyngeal carcinoma, were significantly higher than in controls. In patients with locoregional nasopharyngeal carcinoma receiving curative radiotherapy (n = 31), a high (>median) pretreatment plasma osteopontin level was a significant predictor of poor response to radiotherapy (complete response rate, 40% versus 88%; P = 0.009), which remained significant in multivariate analysis. Conclusion: Our results suggested that the pretreatment plasma osteopontin level may be a useful biomarker of response to radiotherapy in nasopharyngeal carcinoma.',\n",
       " 'Transarterial chemoembolization (TACE) is a therapeutic option for patients with intermediate‐stage hepatocellular carcinoma (HCC) or metastatic liver cancers. Identifying those patients who particularly benefit from TACE remains challenging. Macrophage migration inhibitory factor (MIF) represents is an inflammatory protein described in patients with liver cancer, but no data on its prognostic relevance in patients undergoing TACE exist. Here, we evaluate MIF serum concentrations as a potential biomarker in patients undergoing TACE for primary and secondary hepatic malignancies. MIF serum concentrations were measured by multiplex immunoassay in 50 patients (HCC: n = 39, liver metastases: n = 11) before and 1 day after TACE as well as in 51 healthy controls. Serum concentrations of MIF did not differ between patients and healthy controls. Interestingly, in the subgroup of patients with larger tumor size, significantly more patients had increased MIF concentrations. Patients with an objective tumor response to TACE therapy showed comparable concentrations of serum MIF compared to patients who did not respond. MIF concentrations at day 1 after TACE were significantly higher compared to baseline concentrations. Importantly, baseline MIF concentrations above the optimal cutoff value (0.625 ng/ml) turned out as a significant and independent prognostic marker for a reduced overall survival (OS) following TACE: patients with elevated MIF concentrations showed a significantly reduced median OS of only 719 days compared to patients below the cutoff value (median OS: 1430 days, p = 0.021). Baseline MIF serum concentrations are associated with tumor size of intrahepatic malignancies and predict outcome of patients with liver cancer receiving TACE.',\n",
       " 'Cancer vaccines are emerging as a viable strategy for cancer treatment. In the current study, we screened for genes associated with the prognosis of patients with lung adenocarcinoma and positively correlated with antigen-presenting cell infiltration and identified KLRG1 and CBFA2T3 as potential tumor antigens for mRNA vaccines in lung adenocarcinoma (LUAD). Further analyses of immune subtypes revealed that patients with early-stage LUAD, high immune cell infiltration, high immune checkpoint expression, and low tumor mutation burden might benefit from mRNA vaccination. Moreover, we identified four biomarkers that can be used to assess mRNA vaccination suitability. We also identified potentially sensitive anti-cancer drugs for populations not suitable for vaccination by means of anti-cancer drug susceptibility prediction. Overall, we provided a new perspective for mRNA vaccine treatment strategies for LUAD and emphasized the importance of precise and personalized treatments.',\n",
       " 'Background: Recurrent laryngeal nerve paralysis (RLNP), a severe complication of mini-invasive esophagectomy, usually occurs during lymphadenectomy adjacent to recurrent laryngeal nerve. This systematic review and meta-analysis aimed to evaluate the efficacy of intraoperative nerve monitoring (IONM) in reducing RLNP incidence during mini-invasive esophagectomy. Methods: Systematic literature search of PubMed, EMBASE, EBSCO, Web of Knowledge, and Cochrane Library until June 4, 2021 was performed using the terms “(nerve monitoring) OR neuromonitoring OR neural monitoring OR recurrent laryngeal nerve AND (esophagectomy OR esophageal).” Primary outcome was postoperative RLNP incidence. Secondary outcomes were sensitivity, specificity, and positive and negative predictive values for IONM; complications after esophagectomy; number of dissected lymph nodes; operation time; and length of hospital stay. Results: Among 2,330 studies, five studies comprising 509 patients were eligible for final analysis. The RLNP incidence was significantly lower (odds ratio [OR] 0.33, 95% confidence interval [CI] 0.12–0.88, p < 0.05), the number of dissected mediastinal lymph nodes was significantly higher (mean difference 4.30, 95%CI 2.75–5.85, p < 0.001), and the rate of hoarseness was significantly lower (OR 0.14, 95%CI 0.03–0.63, p = 0.01) in the IONM group than in the non-IONM group. The rates of aspiration (OR 0.31, 95%CI 0.06–1.64, p = 0.17), pneumonia (OR 1.08, 95%CI 0.70–1.67, p = 0.71), and operation time (mean difference 7.68, 95%CI −23.60–38.95, p = 0.63) were not significantly different between the two groups. The mean sensitivity, specificity, and positive and negative predictive values for IONM were 53.2% (0–66.7%), 93.7% (54.8–100%), 71.4% (0–100%), and 87.1% (68.0–96.6%), respectively. Conclusion: IONM was a feasible and effective approach to minimize RLNP, improve lymphadenectomy, and reduce hoarseness after thoracoscopic esophagectomy for esophageal cancer, although IONM did not provide significant benefit in reducing aspiration, pneumonia, operation time, and length of hospital stay.',\n",
       " 'Aim: The Italian Piedmont region sponsored in 2005 a population‐based registry to assess the epidemiology of childhood chronic organ failure involving kidneys, liver, heart or lungs.']"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results['documents'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_large_text(texts, max_chunk_tokens, tokenizer, large_language_model):\n",
    "    \n",
    "    # Recursive text spliter\n",
    "    texts = text_splitter.split_texts(texts)\n",
    "    \n",
    "    token_chunks = [] \n",
    "    \n",
    "    for text in texts: \n",
    "        context = \"context: \" + text\n",
    "        token_chunks.append(tokenizer(context, return_tensors='tf'))\n",
    "    \n",
    "    # Process each chunk through the model\n",
    "    results = []\n",
    "    for chunk in token_chunks:\n",
    "        chunk_text = tokenizer.convert_tokens_to_string(chunk)\n",
    "        result = large_language_model.generate(chunk_text)\n",
    "        results.append(result)\n",
    "\n",
    "    # Combine results if needed\n",
    "    final_result = combine_results(results)\n",
    "\n",
    "    return final_result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "context = results['documents'][0]\n",
    "\n",
    "new_texts = []\n",
    "\n",
    "for text in texts: \n",
    "    new_texts.append(text_splitter.split_text(text))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "token_chunks = []\n",
    "\n",
    "for text in new_texts:\n",
    "    for t in text:\n",
    "        context = \"context: \" + t\n",
    "        token_chunks.append(biogpttokenizer(context, return_tensors='pt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'input_ids': tensor([[    2,  1544,    20, 34175,   654,    12, 20364,    11,     7,    14,\n",
       "           1400,    16,   186,  3688,     7,    57,    58,   301,    13,    47,\n",
       "           1428,  1347,    10,   376,  3836,     4,  2994,   100,    47,   301,\n",
       "             22, 20364,    21,    32,   646,  1428,   189,    10,  1414,  2700,\n",
       "              8,    22,  1348,   294,    21,    73,    15,  2917,   647,   538,\n",
       "              4,    18,  1428,   923,     5, 20364,    10,  1414,  6492,   186,\n",
       "             12, 15988,    11,    57,    41,    58,  2348,     4,    52,   330,\n",
       "          20364,    10,  1414, 15988,     8,   650,   114,    15, 12744,   585,\n",
       "              8,   125,   497,    13,   340,   373, 20364,    21,    14,   745,\n",
       "           1428,  3595,    16,    55,    28,     4,  1723,     9,   508,  1414,\n",
       "          17686,    29,   218, 28837, 11041,  6074,    45,  9901,     8,  2488,\n",
       "             19,   220,     4,    18,   187, 20364,    17,   346,    16,   221,\n",
       "            186,    29,     6,    54,   130, 14100,  1437,  2520,  2673,     4,\n",
       "           1119,   389,     9,   160,  2053,    29,    36,    13,   656,   113,\n",
       "              7,    15,    14,   187,     5,  5827,   113,     4,   701,  1337,\n",
       "            389,     9,   160,     7,   116,    28,    19,  8194,     5,    76,\n",
       "              7,    91,    28,    19,  8202,   205,    76,     7,    85,    28,\n",
       "             19,  8202,    15,    76,     7,     8,    82,    28,    19,  8202,\n",
       "             15,    76,   517,  1286,     4,  2204, 20364,  2053,    29,  1446,\n",
       "             13,   351,  1031, 19117,   336,  1437,  2520,   610,     4, 20364,\n",
       "            214,    41,  3161,    15,   186,   436,     7,     6,   325,  3776,\n",
       "            493,     7,    30,   125,   497,     4, 20364,    10,  1414, 15988,\n",
       "            844,    41,  3161,    15,     6, 35085,   581,  3776,   585]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])},\n",
       " {'input_ids': tensor([[    2,  1544,    20,    15,     6, 35085,   581,  3776,   585,   330,\n",
       "             10,    35,    38,    30,    15,     6,   125,   497,     8,    21,\n",
       "             41,    14,   745,  1428,  3595,    10,    55,    28,     4,   124,\n",
       "             62,   224,    22,   153,    98,    48,     6,  1052,   191,     5,\n",
       "          19117,    31,   193,    10,  2350,  1414, 15988,   717,     4,   158,\n",
       "              7,   130,   538,    19,   541,   647,     7,     8,   881,   100,\n",
       "             63,    33,  4615,    13,  4502,   345,     6,   151,     5, 19704,\n",
       "              8,   574,     6,  2872,     5, 22683,   764,    10,     6,    53,\n",
       "              5,  1414, 15988,     4]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])},\n",
       " {'input_ids': tensor([[    2,  1544,    20,  2298,    26, 10626,    20,    56,    38,  1070,\n",
       "             13,   554,   105,   153,    16,  1246,     5,    28,    15,   469,\n",
       "            784,  1981,   101,    23,  2573,  8653,   585,     4,   565,     8,\n",
       "           1379,    20,    18,    38,   311,  9455,    28,    15,   469,   784,\n",
       "           1981,   101,   149,   665,  6179,  1230,    45,  4603,     8,  2466,\n",
       "             34,     6,  4469,     5,  4661,     5,     6, 21962,  5318,   390,\n",
       "           2202,  1794,     7, 25389,     7,  3711,     4, 34572,    84,     5,\n",
       "              6,    28,    19,  2623,   330,     4,   605,    20,    18,  1246,\n",
       "            134,    17, 28738,    24,    12,  1682,    26,  9455,    11,    10,\n",
       "              6,    38,    64,     4,    18,  6605,    78,    16,  1246,   220,\n",
       "            508,   105,   153,    20,   455,     5,   235,   186,    12,  3339,\n",
       "            101,    11,     7,   428,   662,    12,   975,   240,    11,     7,\n",
       "           1789,   246,  6691,   166,    12,   361,    82,  1329,    26,   573,\n",
       "             11,     7,  1789,   246, 31168,     9,   348,   166,    12,   361,\n",
       "           1172,  1002,    26,   573,    11,     7,     8,   191,     5,  1633,\n",
       "           1502,  2509,    12,   222,   142,  1973,    11,     4,    18,  2258,\n",
       "             78,    16,  1246,   220,   142,   105,   153,    20,  3339,   101,\n",
       "              7,  1789,   246,  6691,   166,   361,  4156,  1329,    26,   573,\n",
       "            203,    24,     7,     8,    77,    48,   142,  1633,  1502,  2509,\n",
       "              4,  2213,    20,    18,   105,   153,    16,   469,   784,  1981,\n",
       "            101,  1246,  3269,    13,    33,  3339,   101,     7,  1789,   246,\n",
       "           6691,   166,   361,  4156,  1329,    26,   573,     7,     8,    77,\n",
       "             48,   142,  1633,  1502,  2509,     4]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])},\n",
       " {'input_ids': tensor([[    2,  1544,    20, 31342,  6790, 25871,   101,    21,    86,     5,\n",
       "              6,  1107,   538,    22, 25328,  4404,   108, 28907,     4,  1270,\n",
       "              9, 28801,    17,   301,    13,   882,  1457,    10,     6,   865,\n",
       "              5,   376,  1636,    44,   106,  9147,    17, 20322,    13,    33,\n",
       "             73,    15,     6,   172,     5,  1963,   101,     4,  1379,   810,\n",
       "           2109,     8, 20494,  1963,   101,    28,  3519,    10,    35,   312,\n",
       "              4,   920,     6,  1159,     5,  1430,     9,  8245,     7,     6,\n",
       "             89,     5,  1270,     9, 28801,    10,   726,   625,     8,  1319,\n",
       "             42,    17,   460,     4,    18,   431,    45,  1270,     9, 28801,\n",
       "             89,   166,     8,    83,   585,     8,  1272,     5,    28,    17,\n",
       "            330,     4,   967,     7,     6,    92,     5,  1270,     9, 28801,\n",
       "             25,    49,   730,     7,  1399,     7,     8,  1833,     5,  1963,\n",
       "            101,    19,   424,    23,  3864, 32769,     8, 30050,   576,     4,\n",
       "            605,  1270,     9, 28801,    90,  4655,    10,  1963,   101,   625,\n",
       "              8,    49,   829,   635,    13,     6,  2669,   175,   625,     8,\n",
       "           4641,    42,     4,    18,    89,     5,  1270,     9, 28801,    17,\n",
       "             90,    73,    15,     6, 13018,   469,     8,   921,  1272,     5,\n",
       "             28,     7,  1112,  1270,     9, 28801,    17,   396,    10,     6,\n",
       "             76,   865,     8,    83,  1272,     5,  1963,   101,     4,  2318,\n",
       "              7,     6,  5557,     5,  1270,     9, 28801,    90,  3931,    49,\n",
       "            730,     7,  1399,     7,     8,  1833,     5,  1963,   101,     7,\n",
       "            247,   106,  5588,    90,   719,    55,   622,   721,     4,  4299,\n",
       "          12036,    36,    17,     6,   738,   552,     5,  1270,     9, 28801,\n",
       "             70,   106,  3931,    96,    25,     6,   172,     5,  1963,   101,\n",
       "              4]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1]])},\n",
       " {'input_ids': tensor([[    2,  1544,    20,   172,     5,  1963,   101,     4,  2213,    18,\n",
       "           5557,     5,  1270,     9, 28801,  3385,    27,    32,   646,  1428,\n",
       "           3595,     8,    14,   186,  1402,    23,  1816,  4299, 12036,    36,\n",
       "             10,  1963,   101,     7,    46,   841,   409,   520,   872,    16,\n",
       "           1963,   101,     4]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])},\n",
       " {'input_ids': tensor([[    2,  1544,    20, 10175,   101,    12,  1914,    11,    21,    14,\n",
       "           3230,    76,   667,    23,  1519,  2150,   680,     4,   801,     7,\n",
       "           6120,    14,   125,     9,   530,  9367,    12,  3150,   447,    11,\n",
       "            144,    22, 19125,     6,   822,   585,     5,     6,   235,   186,\n",
       "             16,   221,   145,     5,  1914,    21,   193,    16,  1136,   536,\n",
       "              4,    37,    35,    38,     7,    60,   789,  3150,   447,   438,\n",
       "              5,   944,     9,  9450,    12,   261,  2103, 13750,    11,     8,\n",
       "          13929,     9,   867,    12,   230,  7287, 29366,    11,  1914,    23,\n",
       "          12519, 23754,  3899,   186,   317,     7,   253,    29,    28,    15,\n",
       "           1131,  1914,   264,     6,   437,  5344,     5,   564,     9,  7905,\n",
       "            216,     4, 21525,    15,  3396, 12867,  9645,     6,  5302,   296,\n",
       "              5,  7009,     8,    88,     6,   186,   381,   134,     4,    18,\n",
       "           3150,   447,   438,  1108,     6,   364,   322,  1840,     8,  3743,\n",
       "            585,     5,     6,    76,    10,  1411,     8,  1993,     6,   813,\n",
       "           1760,   585,     5,     6,   235,   538,     4,   967,     7,    74,\n",
       "           3150,   447,   438,   111,   107,   415,    50, 13929,    30, 12358,\n",
       "             53,     4,    18,   944,     9,  9450,   261,  2103, 13750,  3150,\n",
       "            447,   144,  2405,    14,   292,     5,   384,    29,   767,   186,\n",
       "            750,    13, 10185,   186,   865,     7,     8,     6,   172,     5,\n",
       "           3889, 29105,     9,   867,  1914,    17,   120,    50, 13929,     4,\n",
       "             18,   384,     5,     6,    65,  3150,   447,   438,    13,  4409,\n",
       "           4986,     8, 12358,    19,   242,    13,   161,   138,    10,    28,\n",
       "             15,  1131,  1914,     4,   124,   176,  5585,  1914,   438,   374,\n",
       "           2362,   312,    25,     6,   397,  1064,    53,   133,     8,   422,\n",
       "              4]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1]])},\n",
       " {'input_ids': tensor([[    2,  1544,    20,  2974,    20,  8573,     7,  1630, 18812,  5493,\n",
       "             20,  8213,  1072, 16307,  9165,    20, 19471,  2268,   353, 16942,\n",
       "             20,     3,  1119, 27730,    20,  5522,  1306, 15989, 17725,    20,\n",
       "          11299,  4951,    26, 10677,  4163,    20, 14103,    76,  2298,    20,\n",
       "           8213,  1072, 16307,    21,    14,   839,   952,    46,  3235,  1395,\n",
       "             10,  1129,    45,   297,     8,   887,   113,     5,   118,     7,\n",
       "              8,  2321,    15,   367,   132,    13,  3626,  2561,   197, 11171,\n",
       "              7, 16724,     7,     8,  4663,  1434,     4,  3302,   310,   367,\n",
       "             34,  1689,  1022,  1354,   353,     7,   375,   435,     7,     8,\n",
       "           2723,     4,  3061,  5469,    20,    52,   143,    14,   195,     5,\n",
       "             14,  1630,     9,   206,     9,   421,   649,  2714,  1906,    15,\n",
       "           2268,   353,    34,  2658,     7,    95,    13,    47,    14,   417,\n",
       "           8874,    25,   675,   450,    34,     6,    94,     5,  2126,     4,\n",
       "            943,   345,   450,     7,    14,   348,  1027,   284,  1235,    17,\n",
       "          17848,  4025,     7,  2714, 13765,    16,  3612,   549,    25,   450,\n",
       "              7,    15,   682,  1306,  1408,  7516,   215,   211,     7,     8,\n",
       "           7516,   219,   383,     4,    18,   125,    72,    67,    32,   951,\n",
       "            233,     9, 15439,   166,     4, 18059,    62,    19,  1258,   755,\n",
       "             27,  5193,  2577,  2700,   997,    15, 10443, 16307,     4,  2726,\n",
       "             20,    56,   290,  9361,    32,  3122,  1689,     5, 10443, 16307,\n",
       "              4,   499,   839,     7, 19734,   399,     9,   549,    21,   864,\n",
       "            130,  1133,    27,    32, 13082,  1710,    25,   450,   100,     4,\n",
       "            920, 10734,   488,     9,   160,     8,  1442,  1306,    62,     7,\n",
       "             28,    71,  3336,   428,    30, 18015,     9,   996,   162,  2180,\n",
       "              7,  1423,  1173,    13,    14,  1348]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1, 1, 1, 1, 1]])},\n",
       " {'input_ids': tensor([[   2, 1544,   20, 1173,   13,   14, 1348,  294,   94,    4]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])},\n",
       " {'input_ids': tensor([[    2,  1544,    20,  6790,    20,  1904,   100,    47,   756,    22,\n",
       "          25480,    21,   120,    23,  2394,    10,  1098,     8,  1671,   101,\n",
       "             49,   829,     8,   106,   275,   166,    63,  2431,    27,    14,\n",
       "           9210,  1400,    16,   186,  2394,     8,    53,   497,    10,  1098,\n",
       "              8,  1671,   101,     4,    52,   300,     6,   133,     5, 25480,\n",
       "             13,    10,   307,  2394,    10,  9929,   549,    49,   829,     7,\n",
       "              8,   346,   275, 25480,    87,    10,  9929,   549,    28,     7,\n",
       "            362,  9929,   549,  1098,     8,  1671,   101,    28,     7,     8,\n",
       "            514,   433,     4,    52,  2307,     6,   485,     5,   275, 25480,\n",
       "              8,   133,    13,  2120,    10,  9929,   549,     4,  4622,  5952,\n",
       "             20, 23999, 15733,   549,    49,   829,   313,  3948,     7,   313,\n",
       "           5222,   316,     9,    36,     7, 15590,  1949,     9,    36,     7,\n",
       "              8,  3828,   316,     9,    40,    19,   183,    15,   583,    13,\n",
       "            953,   866,     5,  2394,    30, 24546,     7,    51,    26, 37850,\n",
       "          19729,     4,  8126, 37613,   980,    10,     6,  8272,    17,   268,\n",
       "             23,  2423,   576,     4,  9287, 25480,    80,     8,   560,    19,\n",
       "            395,    23,  2035,  5152,     8,  2088,   857,     9,   808,     7,\n",
       "            170,     4,  3145, 25480,    87,    10,    28,    12,   169,    43,\n",
       "           1949,    44,  1531,  9929,   549,     7,   777,  1098,     8,  1671,\n",
       "            101,    11,     8,   433,    12,   169,    43,  1205,    11,    19,\n",
       "            268,    23,  2423,     4,   605,    20, 16012,    57,   102,    96,\n",
       "             25, 25480,    80,     8,   560,   166,    10,  9929,   549,    42,\n",
       "              4,  2016,  3828,   316,     9,    40,  3774, 25480,     7,     8,\n",
       "            223,    17,   102,    99,   918,    23,  2394,     4]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])},\n",
       " {'input_ids': tensor([[    2,  1544,    20,    99,   918,    23,  2394,     4,  3145, 25480,\n",
       "             87,    10,    28,     5,  1633,  9929,   549,     8,  1098,     8,\n",
       "           1671,   101,     7,    61,    41,    10, 17603,  9929,   549,     7,\n",
       "             19,    90,   128,    48,    10,   433,     4,    37,    28,    15,\n",
       "          17603,  9929,   549,  1333,  6179,  2120,    12,   169,    43,  1083,\n",
       "             11,     7,    14,    75,    12,   361,   770,    11,  2843,   275,\n",
       "          25480,   166,    17,    14,    99,  3003,     5,   921,   133,    13,\n",
       "           2120,    12,   767,   133,   134,     7,   466,    24,   666,  2217,\n",
       "             24,    44,    68,    43, 10535,    11,     7,    46,  1254,    99,\n",
       "             10,  2258,    78,     4,  2213,    20,   339,    62,   756,    22,\n",
       "              6,  2843,   275, 25480,   166,    63,    33,    14,   745,  3859,\n",
       "              5,   133,    13,  2120,    10,  9929,   549,     4]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])},\n",
       " {'input_ids': tensor([[    2,  1544,    20,  4149,   993, 26497,    12, 16215,    11,    21,\n",
       "             14,   520,  3383,    16,    28,    15,  1850, 39914,   469,  3612,\n",
       "            549,    12,  2260,    11,    30,  1633,   284,  1636,     4, 12360,\n",
       "            161,    28,   149,   970,  1477,    29, 16215,   888,  3167,     4,\n",
       "          22833,  1399,  1166,   189,    12, 16459,    11,  2183,    21,    32,\n",
       "            548,    80,   532,    10,    28,    15,   284,   101,     7,    61,\n",
       "            102,    84,    25,   106,  1428,  3320,    10,    28,  1332, 16215,\n",
       "           2785,     4,   556,     7,    60,   464, 16459,   246,   285,    27,\n",
       "             14,   182,  3859,    10,    28,  1332, 16215,    16,   235,     8,\n",
       "            809,  1072,  3836,     4, 16459,   246,   285,    19,   268,    23,\n",
       "          10274,  6750,    10,   297,    28,    12,  2260,    20,   169,    43,\n",
       "           1520,     7,   284,  1973,    20,   169,    43,   402,    11,   356,\n",
       "              8,    36,   262,    50, 16215,    27,   126,    27,    10,  1800,\n",
       "            514,   433,     4,  2374,   285,     5, 16459,   214,    41,  1657,\n",
       "             45,    28,     8,   514,   433,     4,  3856,     7,    10,     6,\n",
       "           3797,     5,    28,    15,  1288,   186,   436,     7,    90,    77,\n",
       "             28,    67,    88, 16459,   285,     4,   565,    15,    32,  1193,\n",
       "            186,   133,    13, 16215,   162,   111,  1597,   285,     5,   246,\n",
       "          16459,    97,    13,    28,   149,   214,    41,  3276,     4, 16459,\n",
       "            285,    34,   262,    36,    50, 16215,    19,    90,   128,    97,\n",
       "             13,   775,   285,     4,  6822,     7,   775, 16459,   285,  1316,\n",
       "              6,  1367,  8040,   474,    12,  5932,   434,  1329,    26,   573,\n",
       "             11, 11483,   387,    27,    14,    99,     8,   646,  1428]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])},\n",
       " {'input_ids': tensor([[    2,  1544,    20,    99,     8,   646,  1428,  1400,    16,    14,\n",
       "            239,   626,   294,    12,  1929,    11,   277, 16215,    20,    28,\n",
       "             15,   951, 16459,   285,   111,    14,    90,   239,   770,  1929,\n",
       "              5,   129,   440,   656,   217,    97,    13,    28,  2004,     6,\n",
       "           8040,   474,    12,   770,  1929,    20,  2106,   249,   217,     7,\n",
       "            110,    43, 18958,    11,     4,  8193, 16459,   246,   285,    31,\n",
       "             73,    15,   186,   436,     5, 10443,  3836,     8,  1566,   497,\n",
       "              5,    28,    15,   284,   101,  1333, 16215,     4]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1, 1, 1, 1, 1]])},\n",
       " {'input_ids': tensor([[    2,  1544,    20,  1788,  3635,    31,  3012,    27,    14,  3902,\n",
       "           1123,    16,   101,    53,     4,    37,     6,   404,    38,     7,\n",
       "             60,  3353,    16,   257,    73,    15,     6,  1272,     5,    28,\n",
       "             15,   417,  2700,     8,  2215,   650,    15,   742,     9,  1906,\n",
       "             49,  3578,     8,   220,   458, 10193,  3645,     8,  4428,  9665,\n",
       "            657,  3501,    27,   182,   186,  1635,    16,   560,  3635,    10,\n",
       "            417,  2700,    12, 14387,  1291,    11,     4,  1360,   642,     5,\n",
       "            564,  3397,   291,    22,    28,    15,   238,     9,   469, 14387,\n",
       "           1291,     7,    75,   564,    49,  3578,     7,    75,   564,  8343,\n",
       "             89,     7,     8,   141,   186,   971,  2295,   693,  1477,    29,\n",
       "            560,  2492,     4,   967,     7,    60,   220,   309,  2302,    22,\n",
       "             71,    33,    69,    13,   574,   560,  2492, 11103,     4,    52,\n",
       "             72,   220,  1423,   706,   344,     9,   101,   588,    16,   895,\n",
       "             41,  2000,    16,  2492,    23,  1160,     5,   344,     9,   101,\n",
       "            254,  1711,  1959,     4,  1830,     7,    60,   917,    14,   176,\n",
       "           3116,    16,   560,  1561,    53,   872,    16, 14387,  1291,     8,\n",
       "           8643,     6,   923,     5,  3357,     8, 10039,  1075,     4]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1]])},\n",
       " {'input_ids': tensor([[    2,  1544,    20,  2298,    20, 11715,  6184,   766,  8639,    12,\n",
       "          19272,  3758,    11,     7,    14,   456,  1838,     5,  9358,     9,\n",
       "           1226, 16540,     7,  1560,  1395,    70, 14404,  2669,    13,  1563,\n",
       "           6184,   766,     4,    56,  1453,   256,     8,  1834,     9,    78,\n",
       "           1070,    13,   464,     6,   586,     5,  3714,   766,  1021,    12,\n",
       "          30037,   326,    11,    10,  1460, 19272,  3758,   512,    70,  9358,\n",
       "              9,  1226, 16540,     4,  1379,    20,  5832,   618,  1769,     5,\n",
       "           4826,     7,  9741,     7,  5331, 30697,     7,  6520,     5,  6989,\n",
       "              7,     8,  5269, 10059,  1786,  3754,    85,     7,  2278,   627,\n",
       "             17,   178,    59,     6,  1065,     3,    12,   766,  1021,    11,\n",
       "            780,  1521,  1021,   780,  1321,  1021,   780,  1563,  6184,   766,\n",
       "            558,    12, 16540,   780,  2606,    11,     4,     3,  2930,   497,\n",
       "             17,   828, 19272,  3758,   512,     4,  5791,   415,    19,   522,\n",
       "              7,  1004,     7,     8,   211,     8,   383,  1490,   358,    16,\n",
       "          30037,   326,    44,   563,    50, 16540,    44,   191,     5, 10549,\n",
       "           1502,  2509,    44,  1322,    94,    44,     8,   714,     5,   503,\n",
       "           2478,     4,   605,    20,  1089,  3857, 15818,   100,     7,   508,\n",
       "            100,  4950, 30485,    28,    19,  4140,    16,  1773,    78,     4,\n",
       "             18, 19272,  3758,   512,    17,    90,   200,    12,  1643,   391,\n",
       "            135,   780,   139, 10824,     7,   203,    24,  1176,   928,   135,\n",
       "            266,   139,  8564,     3,  9480,     7,   110,   109,   394,    11,\n",
       "              7,     6,   191,     5, 10549,  8414,  1502,  2509,    17,    90,\n",
       "            128,    12,   187,   419,  1541,   249,     7,   203,    24,   266,\n",
       "          28919,     3,  2073,  1738,     7,   110,   109,   506,    11,     7,\n",
       "              8,     6,   134,     5,  5833, 34544,    17,    90,   200,    12,\n",
       "            780,  9696,     7,   203,    24,   266,  3104,     3, 11278,     7,\n",
       "            110,    43,   763,    11]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])},\n",
       " {'input_ids': tensor([[    2,  1544,    20,   110,    43,   763,    11,    10,     6, 30037,\n",
       "            326,    64,    48,    10,     6,   155,     9, 30037,   326,    64,\n",
       "              4,    18,   335,     5,  4516,    12,   780, 11947,     7,   203,\n",
       "             24,   266,  6249,     3, 21456,     7,   110,    43, 10382,    11,\n",
       "              7,  3043,    12,   780, 11838,     7,   203,    24,   266, 10791,\n",
       "              3, 19090,     7,   110,    43, 10234,    11,     7,     8,  1322,\n",
       "             94,    12,   187,   419,  2646,  1974,     7,   203,    24,   266,\n",
       "          37850, 14774,   504,     3, 22754,   203,     7,   110,    43, 11278,\n",
       "             11,    19,    41,    90,   107,    45,     6,    65,   137,     4,\n",
       "             18,   187,   522,     7,  1004,     7,     8,   211,     8,   383,\n",
       "           1490,   358,    16, 30037,   326,    19,   351,  3814,    24,    12,\n",
       "            583,     3, 15816,    24,    11,     7,   482,  4400,    24,    12,\n",
       "            351,  4974,     3,   359,    24,    11,     7, 22764,    24,    12,\n",
       "            583,     3,   359,    24,    11,     7,     8,   483,  5722,    24,\n",
       "             12,   393,  5953,     3,   482,  6044,    24,    11,     7,   170,\n",
       "              4,  2213,    20, 30037,   326,    17,    14,  3547,     8,   327,\n",
       "            332,    13,  6151, 19272,  3758,     7,   664, 14404,     7,     8,\n",
       "            897,  5833, 34544,    50, 16820, 16540,    16,  2606,   101,     7,\n",
       "            905, 30037,   326,   214,    41,   378,    99,  1477,    10,  1460,\n",
       "           4516,     7,  3043,     7,  1322,    94,     7,     8,   714,     5,\n",
       "            503,  2478,     4]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])},\n",
       " {'input_ids': tensor([[    2,  1544,    20, 10626,    20,    18,  7667,  8690,  2476,  2093,\n",
       "            527,   427, 24779,    10,  4451,    14,   252, 39914,   117,  6779,\n",
       "             13,   574,     6,  4792,     5,  1989,   308,  1613,   608,  1416,\n",
       "           4597,     7,   284,     7,   429,    30,  3800,     4]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])}]"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "token_chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[    2,  1544,    20, 10626,    20,    18,  7667,  8690,  2476,  2093,\n",
       "           527,   427, 24779,    10,  4451,    14,   252, 39914,   117,  6779,\n",
       "            13,   574,     6,  4792,     5,  1989,   308,  1613,   608,  1416,\n",
       "          4597,     7,   284,     7,   429,    30,  3800,     4]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])}"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "biogpttokenizer(context, return_tensors='pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = biogptmodel.generate(**token_chunks[0], max_length = 512)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'</s>context: Microvascular density (MVD), a marker for tumor angiogenesis, has been demonstrated to have prognostic significance in various malignancies. Previous studies have demonstrated that MVD is an independent prognostic factor in pancreatic adenocarcinoma and that longer survival is associated with hypovascular tumors. The prognostic importance of MVD in pancreatic neuroendocrine tumor (NET) has not been documented. We evaluated MVD in pancreatic NET and correlated it with clinicopathologic features and patient outcome to determine whether MVD is a useful prognostic indicator for these patients. Twenty-five pancreatic NETs from our archival files resected between 1981 and 2000 were identified. The mean MVD was determined for each tumor from the 3 most vascularized 200 × fields. Clinical follow-up ranged from 1 to 19 years, with a mean of 4.9 years. At last follow-up, 6 patients were dead of disease, 10 patients were alive without disease, 4 patients were alive with disease, and 5 patients were alive with disease status unknown. Mean MVD ranged from 43 to 527 microvessels per 200 × field. MVD did not correlate with tumor size, the examined histologic parameters, or patient outcome. MVD in pancreatic NET does not correlate with the clinicohistologic features of pancreatic NET. </s>'"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "biogpttokenizer.decode(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = biogptmodel(**token_chunks[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'BioGptForCausalLM' object has no attribute 'encode'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[83], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m src_text \u001b[39m=\u001b[39m new_texts[\u001b[39m0\u001b[39m][\u001b[39m0\u001b[39m]\n\u001b[0;32m----> 2\u001b[0m src_tokens \u001b[39m=\u001b[39m biogptmodel\u001b[39m.\u001b[39;49mencode(src_text)\n\u001b[1;32m      3\u001b[0m generate \u001b[39m=\u001b[39m biogptmodel\u001b[39m.\u001b[39mgenerate([src_tokens], beam\u001b[39m=\u001b[39margs\u001b[39m.\u001b[39mbeam)[\u001b[39m0\u001b[39m]\n\u001b[1;32m      4\u001b[0m output \u001b[39m=\u001b[39m biogptmodel\u001b[39m.\u001b[39mdecode(generate[\u001b[39m0\u001b[39m][\u001b[39m\"\u001b[39m\u001b[39mtokens\u001b[39m\u001b[39m\"\u001b[39m])\n",
      "File \u001b[0;32m~/work/therapeutic_accelerator/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1614\u001b[0m, in \u001b[0;36mModule.__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   1612\u001b[0m     \u001b[39mif\u001b[39;00m name \u001b[39min\u001b[39;00m modules:\n\u001b[1;32m   1613\u001b[0m         \u001b[39mreturn\u001b[39;00m modules[name]\n\u001b[0;32m-> 1614\u001b[0m \u001b[39mraise\u001b[39;00m \u001b[39mAttributeError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39m'\u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m object has no attribute \u001b[39m\u001b[39m'\u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(\n\u001b[1;32m   1615\u001b[0m     \u001b[39mtype\u001b[39m(\u001b[39mself\u001b[39m)\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m, name))\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'BioGptForCausalLM' object has no attribute 'encode'"
     ]
    }
   ],
   "source": [
    "src_text = new_texts[0][0]\n",
    "src_tokens = biogptmodel.encode(src_text)\n",
    "generate = biogptmodel.generate([src_tokens], beam=args.beam)[0]\n",
    "output = biogptmodel.decode(generate[0][\"tokens\"])\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['T_destination',\n",
       " '__annotations__',\n",
       " '__call__',\n",
       " '__class__',\n",
       " '__delattr__',\n",
       " '__dict__',\n",
       " '__dir__',\n",
       " '__doc__',\n",
       " '__eq__',\n",
       " '__format__',\n",
       " '__ge__',\n",
       " '__getattr__',\n",
       " '__getattribute__',\n",
       " '__gt__',\n",
       " '__hash__',\n",
       " '__init__',\n",
       " '__init_subclass__',\n",
       " '__le__',\n",
       " '__lt__',\n",
       " '__module__',\n",
       " '__ne__',\n",
       " '__new__',\n",
       " '__reduce__',\n",
       " '__reduce_ex__',\n",
       " '__repr__',\n",
       " '__setattr__',\n",
       " '__setstate__',\n",
       " '__sizeof__',\n",
       " '__str__',\n",
       " '__subclasshook__',\n",
       " '__weakref__',\n",
       " '_apply',\n",
       " '_auto_class',\n",
       " '_backward_compatibility_gradient_checkpointing',\n",
       " '_backward_hooks',\n",
       " '_backward_pre_hooks',\n",
       " '_buffers',\n",
       " '_call_impl',\n",
       " '_convert_head_mask_to_5d',\n",
       " '_create_repo',\n",
       " '_expand_inputs_for_generation',\n",
       " '_extract_past_from_model_output',\n",
       " '_forward_hooks',\n",
       " '_forward_hooks_with_kwargs',\n",
       " '_forward_pre_hooks',\n",
       " '_forward_pre_hooks_with_kwargs',\n",
       " '_from_config',\n",
       " '_get_backward_hooks',\n",
       " '_get_backward_pre_hooks',\n",
       " '_get_decoder_start_token_id',\n",
       " '_get_files_timestamps',\n",
       " '_get_logits_processor',\n",
       " '_get_logits_warper',\n",
       " '_get_name',\n",
       " '_get_resized_embeddings',\n",
       " '_get_resized_lm_head',\n",
       " '_get_stopping_criteria',\n",
       " '_hook_rss_memory_post_forward',\n",
       " '_hook_rss_memory_pre_forward',\n",
       " '_init_weights',\n",
       " '_initialize_weights',\n",
       " '_is_full_backward_hook',\n",
       " '_is_hf_initialized',\n",
       " '_keep_in_fp32_modules',\n",
       " '_keys_to_ignore_on_load_missing',\n",
       " '_keys_to_ignore_on_load_unexpected',\n",
       " '_keys_to_ignore_on_save',\n",
       " '_load_from_state_dict',\n",
       " '_load_pretrained_model',\n",
       " '_load_pretrained_model_low_mem',\n",
       " '_load_state_dict_post_hooks',\n",
       " '_load_state_dict_pre_hooks',\n",
       " '_maybe_initialize_input_ids_for_generation',\n",
       " '_maybe_warn_non_full_backward_hook',\n",
       " '_merge_criteria_processor_list',\n",
       " '_modules',\n",
       " '_named_members',\n",
       " '_no_split_modules',\n",
       " '_non_persistent_buffers_set',\n",
       " '_parameters',\n",
       " '_prepare_attention_mask_for_generation',\n",
       " '_prepare_decoder_input_ids_for_generation',\n",
       " '_prepare_encoder_decoder_kwargs_for_generation',\n",
       " '_prepare_model_inputs',\n",
       " '_register_load_state_dict_pre_hook',\n",
       " '_register_state_dict_hook',\n",
       " '_reorder_cache',\n",
       " '_replicate_for_data_parallel',\n",
       " '_resize_token_embeddings',\n",
       " '_save_to_state_dict',\n",
       " '_set_default_torch_dtype',\n",
       " '_set_gradient_checkpointing',\n",
       " '_skip_keys_device_placement',\n",
       " '_slow_forward',\n",
       " '_state_dict_hooks',\n",
       " '_state_dict_pre_hooks',\n",
       " '_tie_encoder_decoder_weights',\n",
       " '_tie_or_clone_weights',\n",
       " '_update_model_kwargs_for_generation',\n",
       " '_upload_modified_files',\n",
       " '_validate_model_class',\n",
       " '_validate_model_kwargs',\n",
       " '_version',\n",
       " 'add_memory_hooks',\n",
       " 'add_module',\n",
       " 'adjust_logits_during_generation',\n",
       " 'apply',\n",
       " 'assisted_decoding',\n",
       " 'base_model',\n",
       " 'base_model_prefix',\n",
       " 'beam_sample',\n",
       " 'beam_search',\n",
       " 'bfloat16',\n",
       " 'biogpt',\n",
       " 'buffers',\n",
       " 'call_super_init',\n",
       " 'can_generate',\n",
       " 'children',\n",
       " 'compute_transition_scores',\n",
       " 'config',\n",
       " 'config_class',\n",
       " 'constrained_beam_search',\n",
       " 'contrastive_search',\n",
       " 'cpu',\n",
       " 'create_extended_attention_mask_for_decoder',\n",
       " 'cuda',\n",
       " 'device',\n",
       " 'disable_input_require_grads',\n",
       " 'double',\n",
       " 'dtype',\n",
       " 'dummy_inputs',\n",
       " 'dump_patches',\n",
       " 'enable_input_require_grads',\n",
       " 'estimate_tokens',\n",
       " 'eval',\n",
       " 'extra_repr',\n",
       " 'float',\n",
       " 'floating_point_ops',\n",
       " 'forward',\n",
       " 'framework',\n",
       " 'from_pretrained',\n",
       " 'generate',\n",
       " 'generation_config',\n",
       " 'get_buffer',\n",
       " 'get_extended_attention_mask',\n",
       " 'get_extra_state',\n",
       " 'get_head_mask',\n",
       " 'get_input_embeddings',\n",
       " 'get_memory_footprint',\n",
       " 'get_output_embeddings',\n",
       " 'get_parameter',\n",
       " 'get_position_embeddings',\n",
       " 'get_submodule',\n",
       " 'gradient_checkpointing_disable',\n",
       " 'gradient_checkpointing_enable',\n",
       " 'greedy_search',\n",
       " 'group_beam_search',\n",
       " 'half',\n",
       " 'init_weights',\n",
       " 'invert_attention_mask',\n",
       " 'ipu',\n",
       " 'is_gradient_checkpointing',\n",
       " 'is_loaded_in_4bit',\n",
       " 'is_loaded_in_8bit',\n",
       " 'is_parallelizable',\n",
       " 'is_quantized',\n",
       " 'load_state_dict',\n",
       " 'main_input_name',\n",
       " 'modules',\n",
       " 'name_or_path',\n",
       " 'named_buffers',\n",
       " 'named_children',\n",
       " 'named_modules',\n",
       " 'named_parameters',\n",
       " 'num_parameters',\n",
       " 'output_projection',\n",
       " 'parameters',\n",
       " 'post_init',\n",
       " 'prepare_inputs_for_generation',\n",
       " 'prune_heads',\n",
       " 'push_to_hub',\n",
       " 'register_backward_hook',\n",
       " 'register_buffer',\n",
       " 'register_for_auto_class',\n",
       " 'register_forward_hook',\n",
       " 'register_forward_pre_hook',\n",
       " 'register_full_backward_hook',\n",
       " 'register_full_backward_pre_hook',\n",
       " 'register_load_state_dict_post_hook',\n",
       " 'register_module',\n",
       " 'register_parameter',\n",
       " 'register_state_dict_pre_hook',\n",
       " 'requires_grad_',\n",
       " 'reset_memory_hooks_state',\n",
       " 'resize_position_embeddings',\n",
       " 'resize_token_embeddings',\n",
       " 'retrieve_modules_from_names',\n",
       " 'reverse_bettertransformer',\n",
       " 'sample',\n",
       " 'save_pretrained',\n",
       " 'set_extra_state',\n",
       " 'set_input_embeddings',\n",
       " 'set_output_embeddings',\n",
       " 'share_memory',\n",
       " 'state_dict',\n",
       " 'supports_gradient_checkpointing',\n",
       " 'tie_weights',\n",
       " 'to',\n",
       " 'to_bettertransformer',\n",
       " 'to_empty',\n",
       " 'train',\n",
       " 'training',\n",
       " 'type',\n",
       " 'warnings_issued',\n",
       " 'xpu',\n",
       " 'zero_grad']"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dir(biogptmodel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'int' object has no attribute 'replace'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[74], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m biogpttokenizer\u001b[39m.\u001b[39;49mconvert_tokens_to_string(test\u001b[39m.\u001b[39;49mtolist()[\u001b[39m0\u001b[39;49m])\n",
      "File \u001b[0;32m~/work/therapeutic_accelerator/.venv/lib/python3.10/site-packages/transformers/models/biogpt/tokenization_biogpt.py:239\u001b[0m, in \u001b[0;36mBioGptTokenizer.convert_tokens_to_string\u001b[0;34m(self, tokens)\u001b[0m\n\u001b[1;32m    237\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"Converts a sequence of tokens (string) in a single string.\"\"\"\u001b[39;00m\n\u001b[1;32m    238\u001b[0m \u001b[39m# remove BPE\u001b[39;00m\n\u001b[0;32m--> 239\u001b[0m tokens \u001b[39m=\u001b[39m [t\u001b[39m.\u001b[39mreplace(\u001b[39m\"\u001b[39m\u001b[39m \u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39m\"\u001b[39m)\u001b[39m.\u001b[39mreplace(\u001b[39m\"\u001b[39m\u001b[39m</w>\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39m \u001b[39m\u001b[39m\"\u001b[39m) \u001b[39mfor\u001b[39;00m t \u001b[39min\u001b[39;00m tokens]\n\u001b[1;32m    240\u001b[0m tokens \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mjoin(tokens)\u001b[39m.\u001b[39msplit()\n\u001b[1;32m    241\u001b[0m \u001b[39m# detokenize\u001b[39;00m\n",
      "File \u001b[0;32m~/work/therapeutic_accelerator/.venv/lib/python3.10/site-packages/transformers/models/biogpt/tokenization_biogpt.py:239\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    237\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"Converts a sequence of tokens (string) in a single string.\"\"\"\u001b[39;00m\n\u001b[1;32m    238\u001b[0m \u001b[39m# remove BPE\u001b[39;00m\n\u001b[0;32m--> 239\u001b[0m tokens \u001b[39m=\u001b[39m [t\u001b[39m.\u001b[39;49mreplace(\u001b[39m\"\u001b[39m\u001b[39m \u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39m\"\u001b[39m)\u001b[39m.\u001b[39mreplace(\u001b[39m\"\u001b[39m\u001b[39m</w>\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39m \u001b[39m\u001b[39m\"\u001b[39m) \u001b[39mfor\u001b[39;00m t \u001b[39min\u001b[39;00m tokens]\n\u001b[1;32m    240\u001b[0m tokens \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mjoin(tokens)\u001b[39m.\u001b[39msplit()\n\u001b[1;32m    241\u001b[0m \u001b[39m# detokenize\u001b[39;00m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'int' object has no attribute 'replace'"
     ]
    }
   ],
   "source": [
    "biogpttokenizer.convert_tokens_to_string(test.tolist()[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'BioGptForCausalLM' object has no attribute 'convert_tokens_to_string'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[60], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m biogptmodel\u001b[39m.\u001b[39;49mconvert_tokens_to_string(test[\u001b[39m0\u001b[39m])\n",
      "File \u001b[0;32m~/work/therapeutic_accelerator/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1614\u001b[0m, in \u001b[0;36mModule.__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   1612\u001b[0m     \u001b[39mif\u001b[39;00m name \u001b[39min\u001b[39;00m modules:\n\u001b[1;32m   1613\u001b[0m         \u001b[39mreturn\u001b[39;00m modules[name]\n\u001b[0;32m-> 1614\u001b[0m \u001b[39mraise\u001b[39;00m \u001b[39mAttributeError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39m'\u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m object has no attribute \u001b[39m\u001b[39m'\u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(\n\u001b[1;32m   1615\u001b[0m     \u001b[39mtype\u001b[39m(\u001b[39mself\u001b[39m)\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m, name))\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'BioGptForCausalLM' object has no attribute 'convert_tokens_to_string'"
     ]
    }
   ],
   "source": [
    "biogptmodel.convert_tokens_to_string(test[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "File \u001b[0;32m~/work/therapeutic_accelerator/.venv/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:254\u001b[0m, in \u001b[0;36mBatchEncoding.__getattr__\u001b[0;34m(self, item)\u001b[0m\n\u001b[1;32m    253\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 254\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdata[item]\n\u001b[1;32m    255\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mKeyError\u001b[39;00m:\n",
      "\u001b[0;31mKeyError\u001b[0m: 'shape'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[42], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m results \u001b[39m=\u001b[39m []\n\u001b[1;32m      2\u001b[0m \u001b[39mfor\u001b[39;00m chunk \u001b[39min\u001b[39;00m token_chunks:\n\u001b[1;32m      3\u001b[0m     \u001b[39m# chunk_text = tokenizer.convert_tokens_to_string(chunk)\u001b[39;00m\n\u001b[0;32m----> 4\u001b[0m     result \u001b[39m=\u001b[39m biogptmodel\u001b[39m.\u001b[39;49mgenerate(chunk)\n\u001b[1;32m      5\u001b[0m     results\u001b[39m.\u001b[39mappend(result)\n",
      "File \u001b[0;32m~/work/therapeutic_accelerator/.venv/lib/python3.10/site-packages/torch/utils/_contextlib.py:115\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[39m@functools\u001b[39m\u001b[39m.\u001b[39mwraps(func)\n\u001b[1;32m    113\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mdecorate_context\u001b[39m(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[1;32m    114\u001b[0m     \u001b[39mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 115\u001b[0m         \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/work/therapeutic_accelerator/.venv/lib/python3.10/site-packages/transformers/generation/utils.py:1297\u001b[0m, in \u001b[0;36mGenerationMixin.generate\u001b[0;34m(self, inputs, generation_config, logits_processor, stopping_criteria, prefix_allowed_tokens_fn, synced_gpus, assistant_model, streamer, **kwargs)\u001b[0m\n\u001b[1;32m   1289\u001b[0m \u001b[39m# 3. Define model inputs\u001b[39;00m\n\u001b[1;32m   1290\u001b[0m \u001b[39m# inputs_tensor has to be defined\u001b[39;00m\n\u001b[1;32m   1291\u001b[0m \u001b[39m# model_input_name is defined if model-specific keyword input is passed\u001b[39;00m\n\u001b[1;32m   1292\u001b[0m \u001b[39m# otherwise model_input_name is None\u001b[39;00m\n\u001b[1;32m   1293\u001b[0m \u001b[39m# all model-specific keyword inputs are removed from `model_kwargs`\u001b[39;00m\n\u001b[1;32m   1294\u001b[0m inputs_tensor, model_input_name, model_kwargs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_prepare_model_inputs(\n\u001b[1;32m   1295\u001b[0m     inputs, generation_config\u001b[39m.\u001b[39mbos_token_id, model_kwargs\n\u001b[1;32m   1296\u001b[0m )\n\u001b[0;32m-> 1297\u001b[0m batch_size \u001b[39m=\u001b[39m inputs_tensor\u001b[39m.\u001b[39;49mshape[\u001b[39m0\u001b[39m]\n\u001b[1;32m   1299\u001b[0m \u001b[39m# 4. Define other model kwargs\u001b[39;00m\n\u001b[1;32m   1300\u001b[0m model_kwargs[\u001b[39m\"\u001b[39m\u001b[39moutput_attentions\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m generation_config\u001b[39m.\u001b[39moutput_attentions\n",
      "File \u001b[0;32m~/work/therapeutic_accelerator/.venv/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:256\u001b[0m, in \u001b[0;36mBatchEncoding.__getattr__\u001b[0;34m(self, item)\u001b[0m\n\u001b[1;32m    254\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdata[item]\n\u001b[1;32m    255\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mKeyError\u001b[39;00m:\n\u001b[0;32m--> 256\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mAttributeError\u001b[39;00m\n",
      "\u001b[0;31mAttributeError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "results = []\n",
    "for chunk in token_chunks:\n",
    "    # chunk_text = tokenizer.convert_tokens_to_string(chunk)\n",
    "    result = biogptmodel.generate(chunk)\n",
    "    results.append(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "process_large_text()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Microvascular density (MVD), a marker for tumor angiogenesis, has been demonstrated to have prognostic significance in various malignancies. Previous studies have demonstrated that MVD is an independent prognostic factor in pancreatic adenocarcinoma and that longer survival is associated with hypovascular tumors. The prognostic importance of MVD in pancreatic neuroendocrine tumor (NET) has not been documented. We evaluated MVD in pancreatic NET and correlated it with clinicopathologic features and patient outcome to determine whether MVD is a useful prognostic indicator for these patients. Twenty-five pancreatic NETs from our archival files resected between 1981 and 2000 were identified. The mean MVD was determined for each tumor from the 3 most vascularized 200 × fields. Clinical follow-up ranged from 1 to 19 years, with a mean of 4.9 years. At last follow-up, 6 patients were dead of disease, 10 patients were alive without disease, 4 patients were alive with disease, and 5 patients were alive with disease status unknown. Mean MVD ranged from 43 to 527 microvessels per 200 × field. MVD did not correlate with tumor size, the examined histologic parameters, or patient outcome. MVD in pancreatic NET does not correlate with the clinicohistologic features evaluated in this study or with the patient outcome and is not a useful prognostic indicator in these patients. These results suggest that factors other than the simple number of microvessels are important in determining pancreatic NET behavior. However, most tumors were highly vascular, and additional studies may be helpful to clarify further the role of vascularity and assess the utility of antiangiogenic agents in the treatment of pancreatic NET.'"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results['documents'][0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['context</w>',\n",
       " ':</w>',\n",
       " 'Microvascular</w>',\n",
       " 'density</w>',\n",
       " '(</w>',\n",
       " 'MVD</w>',\n",
       " ')</w>',\n",
       " ',</w>',\n",
       " 'a</w>',\n",
       " 'marker</w>',\n",
       " 'for</w>',\n",
       " 'tumor</w>',\n",
       " 'angiogenesis</w>',\n",
       " ',</w>',\n",
       " 'has</w>',\n",
       " 'been</w>',\n",
       " 'demonstrated</w>',\n",
       " 'to</w>',\n",
       " 'have</w>',\n",
       " 'prognostic</w>',\n",
       " 'significance</w>',\n",
       " 'in</w>',\n",
       " 'various</w>',\n",
       " 'malignancies</w>',\n",
       " '.</w>',\n",
       " 'Previous</w>',\n",
       " 'studies</w>',\n",
       " 'have</w>',\n",
       " 'demonstrated</w>',\n",
       " 'that</w>',\n",
       " 'MVD</w>',\n",
       " 'is</w>',\n",
       " 'an</w>',\n",
       " 'independent</w>',\n",
       " 'prognostic</w>',\n",
       " 'factor</w>',\n",
       " 'in</w>',\n",
       " 'pancreatic</w>',\n",
       " 'adenocarcinoma</w>',\n",
       " 'and</w>',\n",
       " 'that</w>',\n",
       " 'longer</w>',\n",
       " 'survival</w>',\n",
       " 'is</w>',\n",
       " 'associated</w>',\n",
       " 'with</w>',\n",
       " 'hypo',\n",
       " 'vascular</w>',\n",
       " 'tumors</w>',\n",
       " '.</w>',\n",
       " 'The</w>',\n",
       " 'prognostic</w>',\n",
       " 'importance</w>',\n",
       " 'of</w>',\n",
       " 'MVD</w>',\n",
       " 'in</w>',\n",
       " 'pancreatic</w>',\n",
       " 'neuroendocrine</w>',\n",
       " 'tumor</w>',\n",
       " '(</w>',\n",
       " 'NET</w>',\n",
       " ')</w>',\n",
       " 'has</w>',\n",
       " 'not</w>',\n",
       " 'been</w>',\n",
       " 'documented</w>',\n",
       " '.</w>',\n",
       " 'We</w>',\n",
       " 'evaluated</w>',\n",
       " 'MVD</w>',\n",
       " 'in</w>',\n",
       " 'pancreatic</w>',\n",
       " 'NET</w>',\n",
       " 'and</w>',\n",
       " 'correlated</w>',\n",
       " 'it</w>',\n",
       " 'with</w>',\n",
       " 'clinicopathologic</w>',\n",
       " 'features</w>',\n",
       " 'and</w>',\n",
       " 'patient</w>',\n",
       " 'outcome</w>',\n",
       " 'to</w>',\n",
       " 'determine</w>',\n",
       " 'whether</w>',\n",
       " 'MVD</w>',\n",
       " 'is</w>',\n",
       " 'a</w>',\n",
       " 'useful</w>',\n",
       " 'prognostic</w>',\n",
       " 'indicator</w>',\n",
       " 'for</w>',\n",
       " 'these</w>',\n",
       " 'patients</w>',\n",
       " '.</w>',\n",
       " 'Twenty</w>',\n",
       " '@-@</w>',\n",
       " 'five</w>',\n",
       " 'pancreatic</w>',\n",
       " 'NETs</w>',\n",
       " 'from</w>',\n",
       " 'our</w>',\n",
       " 'archival</w>',\n",
       " 'files</w>',\n",
       " 'resected</w>',\n",
       " 'between</w>',\n",
       " '1981</w>',\n",
       " 'and</w>',\n",
       " '2000</w>',\n",
       " 'were</w>',\n",
       " 'identified</w>',\n",
       " '.</w>',\n",
       " 'The</w>',\n",
       " 'mean</w>',\n",
       " 'MVD</w>',\n",
       " 'was</w>',\n",
       " 'determined</w>',\n",
       " 'for</w>',\n",
       " 'each</w>',\n",
       " 'tumor</w>',\n",
       " 'from</w>',\n",
       " 'the</w>',\n",
       " '3</w>',\n",
       " 'most</w>',\n",
       " 'vascularized</w>',\n",
       " '200</w>',\n",
       " '×</w>',\n",
       " 'fields</w>',\n",
       " '.</w>',\n",
       " 'Clinical</w>',\n",
       " 'follow</w>',\n",
       " '@-@</w>',\n",
       " 'up</w>',\n",
       " 'ranged</w>',\n",
       " 'from</w>',\n",
       " '1</w>',\n",
       " 'to</w>',\n",
       " '19</w>',\n",
       " 'years</w>',\n",
       " ',</w>',\n",
       " 'with</w>',\n",
       " 'a</w>',\n",
       " 'mean</w>',\n",
       " 'of</w>',\n",
       " '4.9</w>',\n",
       " 'years</w>',\n",
       " '.</w>',\n",
       " 'At</w>',\n",
       " 'last</w>',\n",
       " 'follow</w>',\n",
       " '@-@</w>',\n",
       " 'up</w>',\n",
       " ',</w>',\n",
       " '6</w>',\n",
       " 'patients</w>',\n",
       " 'were</w>',\n",
       " 'dead</w>',\n",
       " 'of</w>',\n",
       " 'disease</w>',\n",
       " ',</w>',\n",
       " '10</w>',\n",
       " 'patients</w>',\n",
       " 'were</w>',\n",
       " 'alive</w>',\n",
       " 'without</w>',\n",
       " 'disease</w>',\n",
       " ',</w>',\n",
       " '4</w>',\n",
       " 'patients</w>',\n",
       " 'were</w>',\n",
       " 'alive</w>',\n",
       " 'with</w>',\n",
       " 'disease</w>',\n",
       " ',</w>',\n",
       " 'and</w>',\n",
       " '5</w>',\n",
       " 'patients</w>',\n",
       " 'were</w>',\n",
       " 'alive</w>',\n",
       " 'with</w>',\n",
       " 'disease</w>',\n",
       " 'status</w>',\n",
       " 'unknown</w>',\n",
       " '.</w>',\n",
       " 'Mean</w>',\n",
       " 'MVD</w>',\n",
       " 'ranged</w>',\n",
       " 'from</w>',\n",
       " '43</w>',\n",
       " 'to</w>',\n",
       " '5',\n",
       " '27</w>',\n",
       " 'microvessels</w>',\n",
       " 'per</w>',\n",
       " '200</w>',\n",
       " '×</w>',\n",
       " 'field</w>',\n",
       " '.</w>',\n",
       " 'MVD</w>',\n",
       " 'did</w>',\n",
       " 'not</w>',\n",
       " 'correlate</w>',\n",
       " 'with</w>',\n",
       " 'tumor</w>',\n",
       " 'size</w>',\n",
       " ',</w>',\n",
       " 'the</w>',\n",
       " 'examined</w>',\n",
       " 'histologic</w>',\n",
       " 'parameters</w>',\n",
       " ',</w>',\n",
       " 'or</w>',\n",
       " 'patient</w>',\n",
       " 'outcome</w>',\n",
       " '.</w>',\n",
       " 'MVD</w>',\n",
       " 'in</w>',\n",
       " 'pancreatic</w>',\n",
       " 'NET</w>',\n",
       " 'does</w>',\n",
       " 'not</w>',\n",
       " 'correlate</w>',\n",
       " 'with</w>',\n",
       " 'the</w>',\n",
       " 'clinic',\n",
       " 'o',\n",
       " 'histologic</w>',\n",
       " 'features</w>',\n",
       " 'evaluated</w>',\n",
       " 'in</w>',\n",
       " 'this</w>',\n",
       " 'study</w>',\n",
       " 'or</w>',\n",
       " 'with</w>',\n",
       " 'the</w>',\n",
       " 'patient</w>',\n",
       " 'outcome</w>',\n",
       " 'and</w>',\n",
       " 'is</w>',\n",
       " 'not</w>',\n",
       " 'a</w>',\n",
       " 'useful</w>',\n",
       " 'prognostic</w>',\n",
       " 'indicator</w>',\n",
       " 'in</w>',\n",
       " 'these</w>',\n",
       " 'patients</w>',\n",
       " '.</w>',\n",
       " 'These</w>',\n",
       " 'results</w>',\n",
       " 'suggest</w>',\n",
       " 'that</w>',\n",
       " 'factors</w>',\n",
       " 'other</w>',\n",
       " 'than</w>',\n",
       " 'the</w>',\n",
       " 'simple</w>',\n",
       " 'number</w>',\n",
       " 'of</w>',\n",
       " 'microvessels</w>',\n",
       " 'are</w>',\n",
       " 'important</w>',\n",
       " 'in</w>',\n",
       " 'determining</w>',\n",
       " 'pancreatic</w>',\n",
       " 'NET</w>',\n",
       " 'behavior</w>',\n",
       " '.</w>',\n",
       " 'However</w>',\n",
       " ',</w>',\n",
       " 'most</w>',\n",
       " 'tumors</w>',\n",
       " 'were</w>',\n",
       " 'highly</w>',\n",
       " 'vascular</w>',\n",
       " ',</w>',\n",
       " 'and</w>',\n",
       " 'additional</w>',\n",
       " 'studies</w>',\n",
       " 'may</w>',\n",
       " 'be</w>',\n",
       " 'helpful</w>',\n",
       " 'to</w>',\n",
       " 'clarify</w>',\n",
       " 'further</w>',\n",
       " 'the</w>',\n",
       " 'role</w>',\n",
       " 'of</w>',\n",
       " 'vascularity</w>',\n",
       " 'and</w>',\n",
       " 'assess</w>',\n",
       " 'the</w>',\n",
       " 'utility</w>',\n",
       " 'of</w>',\n",
       " 'antiangiogenic</w>',\n",
       " 'agents</w>',\n",
       " 'in</w>',\n",
       " 'the</w>',\n",
       " 'treatment</w>',\n",
       " 'of</w>',\n",
       " 'pancreatic</w>',\n",
       " 'NET</w>',\n",
       " '.</w>']"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "biogpttokenizer.tokenize(\"context: \" + results['documents'][0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-07-18 14:09:49.142086: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-07-18 14:09:50.108675: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "tokens = []\n",
    "\n",
    "# Split text if neeeded\n",
    "# chunks = text_splitter.split_text(context)\n",
    "\n",
    "# Process large text for context\n",
    "for text in results['documents'][0]: \n",
    "    context = \"context: \" + text\n",
    "    tokens.append(biogpttokenizer(context, return_tensors='tf'))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'context: Aim: The Italian Piedmont region sponsored in 2005 a population‐based registry to assess the epidemiology of childhood chronic organ failure involving kidneys, liver, heart or lungs.'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# \n",
    "encoding_gpt = biogpttokenizer([question + context], return_tensors='tf', max_length=3000, truncation=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_large_text_with_questions(large_text, questions, max_chunk_tokens):\n",
    "    # Tokenize the large text\n",
    "    tokens = tokenizer.tokenize(large_text)\n",
    "\n",
    "    # Split tokens into chunks\n",
    "    chunks = []\n",
    "    current_chunk = []\n",
    "    for token in tokens:\n",
    "        if len(current_chunk) + len(token) > max_chunk_tokens:\n",
    "            chunks.append(current_chunk)\n",
    "            current_chunk = []\n",
    "        current_chunk.append(token)\n",
    "    if current_chunk:\n",
    "        chunks.append(current_chunk)\n",
    "\n",
    "    # Process each question and chunk pair\n",
    "    answers = []\n",
    "    for question, chunk in zip(questions, chunks):\n",
    "        # Concatenate question with the chunk\n",
    "        context_question_input = question + \" \" + tokenizer.convert_tokens_to_string(chunk)\n",
    "        \n",
    "        # Tokenize the combined input\n",
    "        context_question_tokens = tokenizer.tokenize(context_question_input)\n",
    "        \n",
    "        # Generate the answer from the model\n",
    "        answer = large_language_model.generate(context_question_tokens)\n",
    "        answers.append(answer)\n",
    "\n",
    "    # Combine answers if needed\n",
    "    final_answer = combine_answers(answers)\n",
    "\n",
    "    return final_answer\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Answer question"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from langchain.output_parsers import PydanticOutputParser\n",
    "# from pydantic import BaseModel, Field, validator\n",
    "# from typing import List"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "template = \"\"\"Answer the following question\n",
    "\n",
    "\n",
    "Question:\n",
    "{question}\n",
    "\n",
    "\n",
    "Answers:\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'parser' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 8\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mlangchain\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mprompts\u001b[39;00m \u001b[39mimport\u001b[39;00m PromptTemplate, ChatPromptTemplate, HumanMessagePromptTemplate\n\u001b[1;32m      3\u001b[0m template\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mAnswer the user question.\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m{format_instructions}\u001b[39;00m\u001b[39m\\n\u001b[39;00m\u001b[39m{query}\u001b[39;00m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m,\n\u001b[1;32m      5\u001b[0m prompt \u001b[39m=\u001b[39m PromptTemplate(\n\u001b[1;32m      6\u001b[0m     template\u001b[39m=\u001b[39m template,\n\u001b[1;32m      7\u001b[0m     input_variables\u001b[39m=\u001b[39m[\u001b[39m\"\u001b[39m\u001b[39mquery\u001b[39m\u001b[39m\"\u001b[39m],\n\u001b[0;32m----> 8\u001b[0m     partial_variables\u001b[39m=\u001b[39m{\u001b[39m\"\u001b[39m\u001b[39mformat_instructions\u001b[39m\u001b[39m\"\u001b[39m: parser\u001b[39m.\u001b[39mget_format_instructions()}\n\u001b[1;32m      9\u001b[0m )\n",
      "\u001b[0;31mNameError\u001b[0m: name 'parser' is not defined"
     ]
    }
   ],
   "source": [
    "from langchain.prompts import PromptTemplate, ChatPromptTemplate, HumanMessagePromptTemplate\n",
    "\n",
    "template=\"Answer the user question.\\n{format_instructions}\\n{query}\\n\",\n",
    "\n",
    "prompt = PromptTemplate(\n",
    "    template= template,\n",
    "    input_variables=[\"query\"],\n",
    "    partial_variables={\"format_instructions\": parser.get_format_instructions()}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "multi_template = \"\"\"Answer the following questions one at a time.\n",
    "\n",
    "\n",
    "Questions:\n",
    "{questions}\n",
    "\n",
    "\n",
    "Answers:\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import LLMChain, TransformChain\n",
    "from langchain.chains import SequentialChain\n",
    "\n",
    "llm_chain = LLMChain(\n",
    "    prompt=prompt,\n",
    "    llm=biogptmodel,\n",
    "    output_key=\"json_string\",\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def parse_output(inputs: dict) -> dict:\n",
    "    text = inputs[\"json_string\"]\n",
    "    return {\"result\": parser.parse(text)}\n",
    "\n",
    "transform_chain = TransformChain(\n",
    "    input_variables=[\"json_string\"],\n",
    "    output_variables=[\"result\"],\n",
    "    transform=parse_output\n",
    ")\n",
    "\n",
    "chain = SequentialChain(\n",
    "    input_variables=[\"joke_query\"],\n",
    "    output_variables=[\"result\"],\n",
    "    chains=[llm_chain, transform_chain],\n",
    ")\n",
    "\n",
    "chain.run(query=\"Tell me a joke.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(llm_chain.run(qs_str))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create Chain for Multiple Questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extra Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_large_text(text, max_chunk_tokens):\n",
    "    # Tokenize the input text\n",
    "    tokens = tokenizer.tokenize(text)\n",
    "\n",
    "    # Split tokens into chunks\n",
    "    chunks = []\n",
    "    current_chunk = []\n",
    "    for token in tokens:\n",
    "        if len(current_chunk) + len(token) > max_chunk_tokens:\n",
    "            chunks.append(current_chunk)\n",
    "            current_chunk = []\n",
    "        current_chunk.append(token)\n",
    "    if current_chunk:\n",
    "        chunks.append(current_chunk)\n",
    "\n",
    "    # Process each chunk through the model\n",
    "    results = []\n",
    "    for chunk in chunks:\n",
    "        chunk_text = tokenizer.convert_tokens_to_string(chunk)\n",
    "        result = large_language_model.generate(chunk_text)\n",
    "        results.append(result)\n",
    "\n",
    "    # Combine results if needed\n",
    "    final_result = combine_results(results)\n",
    "\n",
    "    return final_result\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
