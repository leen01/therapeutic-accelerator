{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6d0db0aa",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "baec4770",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import (\n",
    "    T5Tokenizer,\n",
    "    TFT5Model,\n",
    "    TFT5ForConditionalGeneration,\n",
    "    # TFAutoModel,\n",
    "    AutoTokenizer,\n",
    "    TFBertModel,\n",
    "    AutoModel,\n",
    "    BertTokenizer,\n",
    ")\n",
    "import os\n",
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import zipfile as zf\n",
    "from glob import glob\n",
    "import sentencepiece\n",
    "from metapub import PubMedFetcher\n",
    "from semanticscholar import SemanticScholar\n",
    "from metapub import FindIt\n",
    "import requests\n",
    "import urllib\n",
    "import json\n",
    "\n",
    "# from keras.saving.hdf5_format import save_attributes_to_hdf5_group\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '-1'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d65d39ce",
   "metadata": {},
   "source": [
    "## Tokenizer & Model Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "304645fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading pytorch_model.bin: 100%|██████████| 433M/433M [00:11<00:00, 37.0MB/s] \n",
      "Downloading (…)okenizer_config.json: 100%|██████████| 136/136 [00:00<00:00, 639kB/s]\n",
      "Downloading (…)solve/main/vocab.txt: 100%|██████████| 213k/213k [00:00<00:00, 39.7MB/s]\n",
      "Downloading (…)cial_tokens_map.json: 100%|██████████| 112/112 [00:00<00:00, 563kB/s]\n"
     ]
    }
   ],
   "source": [
    "bio_bert_model = AutoModel.from_pretrained(\"gsarti/biobert-nli\")\n",
    "bio_bert_tokenizer = AutoTokenizer.from_pretrained(\"gsarti/biobert-nli\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ee271843",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading (…)solve/main/vocab.txt: 100%|██████████| 232k/232k [00:00<00:00, 107MB/s]\n",
      "Downloading (…)okenizer_config.json: 100%|██████████| 28.0/28.0 [00:00<00:00, 60.0kB/s]\n",
      "Downloading (…)lve/main/config.json: 100%|██████████| 570/570 [00:00<00:00, 783kB/s]\n",
      "Downloading model.safetensors: 100%|██████████| 892M/892M [00:05<00:00, 159MB/s] \n",
      "2023-07-12 02:41:46.221066: W tensorflow/tsl/framework/cpu_allocator_impl.cc:83] Allocation of 98697216 exceeds 10% of free system memory.\n",
      "2023-07-12 02:41:46.778897: W tensorflow/tsl/framework/cpu_allocator_impl.cc:83] Allocation of 98697216 exceeds 10% of free system memory.\n",
      "2023-07-12 02:41:46.822624: W tensorflow/tsl/framework/cpu_allocator_impl.cc:83] Allocation of 98697216 exceeds 10% of free system memory.\n",
      "2023-07-12 02:41:52.458014: W tensorflow/tsl/framework/cpu_allocator_impl.cc:83] Allocation of 98697216 exceeds 10% of free system memory.\n",
      "2023-07-12 02:41:55.165300: W tensorflow/tsl/framework/cpu_allocator_impl.cc:83] Allocation of 98697216 exceeds 10% of free system memory.\n",
      "All PyTorch model weights were used when initializing TFT5ForConditionalGeneration.\n",
      "\n",
      "All the weights of TFT5ForConditionalGeneration were initialized from the PyTorch model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFT5ForConditionalGeneration for predictions without further training.\n",
      "/home/ubuntu/work/therapeutic_accelerator/.venv/lib/python3.10/site-packages/transformers/models/t5/tokenization_t5.py:163: FutureWarning: This tokenizer was incorrectly instantiated with a model max length of 512 which will be corrected in Transformers v5.\n",
      "For now, this behavior is kept to avoid breaking backwards compatibility when padding/encoding with `truncation is True`.\n",
      "- Be aware that you SHOULD NOT rely on t5-base automatically truncating your input to 512 when padding/encoding.\n",
      "- If you want to encode/pad to sequences longer than 512 you can either instantiate this tokenizer with `model_max_length` or pass `max_length` when encoding/padding.\n",
      "- To avoid this warning, please instantiate this tokenizer with `model_max_length` set to your preferred value.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "original_bert_tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "T5Abstract_model = TFT5ForConditionalGeneration.from_pretrained(\"t5-base\")\n",
    "T5tokens = T5Tokenizer.from_pretrained(\"t5-base\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11181285",
   "metadata": {},
   "source": [
    "## Data Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "085d059e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get abstracts from postgres\n",
    "# Base code\n",
    "%run /home/ubuntu/work/therapeutic_accelerator/scripts/base.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fd7a4261",
   "metadata": {},
   "outputs": [],
   "source": [
    "table_name = 'abstracts'\n",
    "query = f''' SELECT * FROM {table_name};'''\n",
    "\n",
    "def query_to_df(query):\n",
    "    return pd.read_sql(query, engine)\n",
    "\n",
    "\n",
    "abstracts = query_to_df(query)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fe463f6",
   "metadata": {},
   "source": [
    "## Model Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b96f09da",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_example = abstracts.loc[0, \"abstract\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5a0f4c15",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_sequence_length = 512\n",
    "embedding_size = 200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "bec0ae4a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [22636, 3268, 446, 12396, 87, 4209, 5767, 519, 3240, 53, 4139, 7, 12, 1994, 13324, 16, 186, 25049, 32, 1409, 4597, 52, 1528, 10461, 5, 17716, 6472, 15282, 43, 5111, 2485, 18, 858, 18, 23993, 5097, 5767, 519, 17324, 7, 16, 25049, 32, 23, 26, 1874, 7, 1374, 12, 6676, 8463, 257, 13, 5097, 5767, 6355, 84, 164, 4221, 3, 9, 1055, 12206, 2387, 5, 3, 3626, 3, 9, 5014, 17021, 38, 8735, 6, 62, 3, 22529, 604, 948, 18042, 28, 20459, 1756, 581, 796, 2387, 19166, 12, 2862, 4845, 3919, 13, 19921, 53, 8, 3, 10791, 1756, 13, 5097, 5767, 5787, 2224, 8046, 130, 856, 16742, 26, 28, 1151, 2250, 379, 5097, 5767, 519, 18, 4246, 920, 793, 14804, 41, 17345, 61, 18, 8725, 90, 1598, 11658, 87, 120, 1167, 10207, 9, 2358, 2356, 11, 2329, 508, 3, 7662, 4885, 25049, 32, 75, 63, 1225, 41, 434, 13011, 61, 90, 1598, 11658, 2640, 12, 6570, 70, 1418, 12, 19921, 5097, 5767, 519, 3, 19787, 12973, 257, 11, 5097, 5767, 519, 8976, 2358, 5931, 2020, 5, 101, 4313, 446, 12396, 6, 3, 51, 16442, 6, 454, 7, 102, 2394, 11, 3190, 439, 20197, 7, 38, 815, 295, 20197, 7, 13, 321, 549, 382, 11, 26569, 5097, 5767, 519, 1756, 5, 37, 454, 7, 102, 2394, 20197, 3, 5171, 630, 7675, 115, 47, 1385, 1231, 44, 3, 5503, 8, 5931, 2020, 13, 26569, 5097, 5767, 519, 3, 17345, 2358, 2356, 11, 301, 13011, 90, 1598, 11658, 1868, 5977, 5, 3, 23255, 15, 7675, 115, 13665, 8, 3, 19787, 12973, 257, 13, 26569, 5097, 5767, 519, 44, 3, 476, 2518, 11116, 3, 10339, 446, 12396, 12989, 13853, 439, 357, 20197, 3, 52, 3090, 4172, 17, 77, 23, 115, 141, 3915, 13577, 447, 4710, 30, 26569, 5097, 5767, 519, 3, 19787, 12973, 257, 5, 5433, 6, 15617, 3, 6475, 454, 7, 102, 2394, 6, 446, 12396, 11, 3, 51, 16442, 20197, 7, 130, 72, 1231, 44, 3, 5503, 2358, 5931, 2020, 145, 712, 4373, 5, 421, 7469, 504, 2433, 6315, 12, 19921, 5097, 5767, 519, 1756, 11, 3130, 454, 7, 102, 2394, 38, 3, 9, 12206, 2387, 16, 25049, 32, 1409, 4597, 52, 1528, 10461, 28, 6439, 17, 13830, 1676, 5097, 5767, 5787, 1], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "T5tokens(text_example)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "46758dca",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BertModel(\n",
       "  (embeddings): BertEmbeddings(\n",
       "    (word_embeddings): Embedding(28996, 768, padding_idx=0)\n",
       "    (position_embeddings): Embedding(512, 768)\n",
       "    (token_type_embeddings): Embedding(2, 768)\n",
       "    (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (encoder): BertEncoder(\n",
       "    (layer): ModuleList(\n",
       "      (0-11): 12 x BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (intermediate_act_fn): GELUActivation()\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (pooler): BertPooler(\n",
       "    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "    (activation): Tanh()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bio_bert_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "cdf8655d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instead of summarization can be used for classification of papers\n",
    "def biobert_classifier(\n",
    "    embedding_size=200,\n",
    "    input_dimensions=3,\n",
    "    hidden_layers=0,\n",
    "    max_sequence_length=512,\n",
    "    learning_rate=0.01,\n",
    "):\n",
    "    input_ids = tf.keras.layers.Input(shape=embedding_size, name=\"input_ids\")\n",
    "    token_type_ids = tf.keras.layers.Input(\n",
    "        shape=embedding_size, name=\"token_type_id\")\n",
    "    attention_mask = tf.keras.layers.Input(\n",
    "        shape=embedding_size, name=\"attention_mask\")\n",
    "\n",
    "    model_inputs = {\n",
    "        \"input_ids\": input_ids,\n",
    "        \"token_type_ids\": token_type_ids,\n",
    "        \"attention_mask\": attention_mask,\n",
    "    }\n",
    "\n",
    "    embedding_matrix = tf.keras.layers.Embedding(200)\n",
    "\n",
    "    normalization_layer = tf.keras.layers.BatchNormalization()\n",
    "\n",
    "    attention_layer = tf.keras.layers.Attention()\n",
    "\n",
    "    pooler_layer = bio_bert_model(model_inputs)[0]\n",
    "\n",
    "    dense_layer = tf.keras.layers.Dense(100, activation=\"relu\")(pooler_layer)\n",
    "\n",
    "    dropout_layer = tf.keras.layers.Dropout(0.3)(dense_layer)\n",
    "\n",
    "    final_layer = tf.keras.layers.Dense(1, activation=\"relu\")(dropout_layer)\n",
    "\n",
    "    classification_layer = tf.keras.layers.Dense(\n",
    "        1, activation=\"sigmoid\")(final_layer)\n",
    "\n",
    "    model = tf.keras.Model(\n",
    "        inputs=[input_ids, token_type_ids, attention_mask],\n",
    "        outputs=[classification_layer],\n",
    "    )\n",
    "\n",
    "    model.compile(\n",
    "        optimizer=tf.keras.optimizers.Adam(learning_rate=learning_rate),\n",
    "        loss=tf.keras.losses.BinaryCrossentropy,\n",
    "        metrics=[tf.keras.metrics.Accuracy, tf.keras.metrics.Precision],\n",
    "    )\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1ff80d61",
   "metadata": {},
   "outputs": [],
   "source": [
    "summary_task_prefix = \"Summarize :\"\n",
    "qa_task_prefix = \"Question :\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "5a7bbdea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# T5 Abstractive Text Summarization Model\n",
    "def t5summary_model(tokenizer, text, t5model):\n",
    "    summarize = \"summarize: \"\n",
    "    encoding = tokenizer([summarize + text], return_tensors=\"tf\")\n",
    "    output = t5model.generate(\n",
    "        encoding.input_ids,\n",
    "        num_beams=3,\n",
    "        no_repeat_ngram_size=2,\n",
    "        top_k=10,\n",
    "        top_p=80,\n",
    "        max_length=50,\n",
    "        min_length=30,\n",
    "    )\n",
    "    return [\n",
    "        tokenizer.decode(\n",
    "            w, skip_special_tokens=True, clean_up_tokenization_spaces=True\n",
    "        )\n",
    "        for w in output\n",
    "    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "18aecef7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['constitutive JAK/STAT3 signaling contributes to disease progression. gain-of-function mutations in lymphoid cancers lead to hyperactivation of STAT3, a study has shown.']"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results = t5summary_model(T5tokens, text_example, T5Abstract_model)\n",
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "90e55b9b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check if summary is less than abstract\n",
    "len(text_example) > len(results[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10b7d142",
   "metadata": {},
   "source": [
    "## Training New Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af9f9900",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from transformers import BioGptModel, BioGptConfig, BioGptTokenizer\n",
    "\n",
    "biogpttokenizer = BioGptTokenizer.from_pretrained(\"microsoft/biogpt\")\n",
    "biogptmodel = BioGptModel.from_pretrained(\"microsoft/biogpt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98e9f30c",
   "metadata": {},
   "source": [
    "### Q&A Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "190c1015",
   "metadata": {},
   "outputs": [],
   "source": [
    "# BioBERT or BERT Q&A or Clincal-T5-Large"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecf4783a",
   "metadata": {},
   "source": [
    "### Extractive Summary Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b414ab51",
   "metadata": {},
   "outputs": [],
   "source": [
    "# T5 or T5v1 or Clincal-T5-Large or Bio-GPT"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da8c816c",
   "metadata": {},
   "source": [
    "### Classification Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10eda67e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# BERT or Bio-GPT"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33841278",
   "metadata": {},
   "source": [
    "### NER Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "772b0ed7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# BioELECTRA"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
