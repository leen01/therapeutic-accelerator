{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "\n",
    "# os.system('wget %s' % xl_links[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# url = 'https://ontobee.org/'\n",
    "\n",
    "# source_code = requests.get(url)\n",
    "# plain_text = source_code.content\n",
    "# soup = BeautifulSoup(plain_text, \"html.parser\")\n",
    "# links = soup.findAll('a', {'title': 'Excel XLSX File'})\n",
    "# print(len(links))\n",
    "# xl_links = [link.get('href') for link in links]\n",
    "\n",
    "# # extract string between two strings, in this case, 'format=' and '&'\n",
    "# file_names = [x.split('https://ontobee.org/listTerms/')[1].split(r'?format=')[0] for x in xl_links]\n",
    "\n",
    "# files = list(zip(xl_links, file_names))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# track files that could not be read in case needed later\n",
    "unread_files = []\n",
    "\n",
    "# download files\n",
    "# for x in tqdm(files, leave = False): \n",
    "#     try: \n",
    "#         resp = requests.get(x[0])\n",
    "#         path = ''.join(['/home/ubuntu/work/backup/terms/', f'{x[1]}.xlsx'])\n",
    "#         if os.path.exists(path) != True:\n",
    "#             with open(path, 'wb') as f:\n",
    "#                 f.write(resp.content)\n",
    "#     except: \n",
    "#         print(\"Could not read: \", x[0])\n",
    "#         unread_files.append(x[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dask import dataframe as dd\n",
    "import dask\n",
    "from dask.delayed import delayed\n",
    "from dask.diagnostics import ProgressBar\n",
    "from glob import glob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dask.delayed\n",
    "def try_to_read(file): \n",
    "    \"\"\" Try to read in file, if not, return empty dataframe \"\"\"\n",
    "    try: \n",
    "        df = pd.read_excel(file, engine='openpyxl', dtype=str).astype(str)\n",
    "        return df\n",
    "    except: \n",
    "        return pd.DataFrame(columns = ['Term IRI', 'Term label', 'Parent term IRI', 'Parent term label','Alternative term', 'Definition']).astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "xl_files = glob('/home/ubuntu/work/backup/terms/*.xlsx')\n",
    "\n",
    "output = []\n",
    "\n",
    "for x in xl_files:\n",
    "    parts = dask.delayed(try_to_read)(x)\n",
    "    # filter_df = dask.delayed(get_techniques)(parts)\n",
    "    output.append(parts)\n",
    "\n",
    "# convert to a single dataframe\n",
    "df_total = dd.from_delayed(output)\n",
    "\n",
    "# df_total.visualize()\n",
    "\n",
    "with ProgressBar():\n",
    "    ddf = df_total.compute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create new column names with camel case\n",
    "from re import sub\n",
    "\n",
    "def camel_case(s):\n",
    "  s = sub(r\"(_|-)+\", \" \", s).title().replace(\" \", \"\")\n",
    "  return ''.join([s[0].lower(), s[1:]])\n",
    "\n",
    "new_headers = [camel_case(x) for x in ddf.columns.tolist()]\n",
    "col_map = dict(zip(ddf.columns.tolist(), new_headers))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_camel_case(text):\n",
    "    text = text.replace(\"-\", \" \").replace(\"_\", \" \")\n",
    "    words = text.split()\n",
    "    return \"\".join([w.capitalize() if w != words[0] else w for w in words])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['termIri',\n",
       " 'termLabel',\n",
       " 'parentTermIri',\n",
       " 'parentTermLabel',\n",
       " 'alternativeTerm',\n",
       " 'definition']"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[to_camel_case(x) for x in ddf.columns.tolist()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rename columns\n",
    "ddf.rename(columns=col_map, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_techniques(df): \n",
    "    \"\"\" Filter for techniques \"\"\"\n",
    "    df = df.loc[df['parentTermLabel'].str.contains('assay', case=False, na=False), ['termLabel', 'parentTermLabel', 'alternativeTerm', 'definition']].reset_index(drop=True)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pattern = 'assay'\n",
    "# ddf[ddf['Parent term label'].str.contains(pattern, case = False, na=False)]\n",
    "\n",
    "# filter dataframe to only include assay terms. CSV will be uploaded to postgres DB\n",
    "csv_path = '/home/ubuntu/work/backup/terms/assay_terms.csv'\n",
    "get_techniques(ddf).to_csv(csv_path, index=False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Upload terms to database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "import yaml\n",
    "from sqlalchemy import create_engine, text, MetaData, Table, Column, Integer, String\n",
    "\n",
    "# set up\n",
    "with open(\"/home/ubuntu/work/therapeutic_accelerator/config/main.yaml\", \"r\") as f:\n",
    "    config = yaml.load(f, Loader=yaml.FullLoader)\n",
    "    \n",
    "with open(\"/home/ubuntu/work/therapeutic_accelerator/config/keys.yaml\", \"r\") as f:\n",
    "    keys = yaml.load(f, Loader=yaml.FullLoader)\n",
    "\n",
    "# Create engine to connect to database\n",
    "engine = create_engine(f'postgresql://postgres:{keys[\"postgres\"]}@{config[\"database\"][\"host\"]}:5432/postgres')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['termLabel', 'parentTermLabel', 'alternativeTerm', 'definition']"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Read header from CSV to create columns\n",
    "from csv import DictReader\n",
    "\n",
    "with open(csv_path, 'r') as f:\n",
    "    d_reader = DictReader(f)\n",
    "\n",
    "    #get fieldnames from DictReader object and store in list\n",
    "    headers = d_reader.fieldnames\n",
    "    \n",
    "headers"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Delete old table if necessary to replace with new table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "table_name = 'assayterms' # should be all lower case to avoid postgres issues"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "delete_table = True\n",
    "\n",
    "if delete_table == True:\n",
    "    sql = text(f''' \n",
    "        DROP TABLE IF EXISTS {table_name};\n",
    "    ''')\n",
    "\n",
    "    with engine.connect() as conn: \n",
    "        query = conn.execute(sql)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Table in DB first before uploading\n",
    "metadata_obj = MetaData()\n",
    "\n",
    "# Create abstracts metadata\n",
    "abstracts = Table(\n",
    "    table_name,\n",
    "    metadata_obj,\n",
    "    Column(\"termLabel\", String, nullable = True), # has to be integer so that dask can partition the table\n",
    "    Column(\"parentTermLabel\", String, nullable=True),\n",
    "    Column(\"alternativeTerm\", String, nullable=True),\n",
    "    Column(\"definition\", String, nullable=True)\n",
    ")\n",
    "\n",
    "metadata_obj.create_all(engine)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Has to be run in the terminal because you have to supply the password  \n",
    "```psql --host=database-1.cuaho2dof33c.us-east-1.rds.amazonaws.com \\\n",
    "--port=5432 --username=postgres --password \\\n",
    "--dbname=postgres"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\\copy assayTerms FROM /home/ubuntu/work/backup/terms/assay_terms.csv WITH (FORMAT CSV, HEADER true, DELIMITER ',');\n"
     ]
    }
   ],
   "source": [
    "# Upload CSV to DB\n",
    "## Copy and paste the output below into the postgres terminal. \n",
    "## HEADER true means that the csv file contains the header. Number of columns in csv has to match exactly with DB columns\n",
    "print(rf\"\\copy {table_name} FROM {csv_path} WITH (FORMAT CSV, HEADER true, DELIMITER ',');\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
