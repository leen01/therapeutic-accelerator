{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "\n",
    "# for using configuration files like yamls. This is to help key our keys safe\n",
    "import yaml # for configuration files \n",
    "import hydra\n",
    "from omegaconf import DictConfig, OmegaConf\n",
    "\n",
    "import multiprocessing\n",
    "import requests\n",
    "\n",
    "import urllib\n",
    "import os\n",
    "import json\n",
    "\n",
    "\n",
    "# unzip files\n",
    "import gzip\n",
    "import shutil\n",
    "\n",
    "# @hydra.main(config_path=\"../conf\", config_name=\"main\", version_base=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "with open(\"../config/main.yaml\", \"r\") as f:\n",
    "    config = yaml.load(f, Loader=yaml.FullLoader)\n",
    "\n",
    "with open(\"../config/keys.yaml\", \"r\") as f:\n",
    "    keys = yaml.load(f, Loader=yaml.FullLoader)\n",
    "\n",
    "root_path = \"/home/nick_lee_berkeley_edu/\"\n",
    "mount_path = os.path.join(root_path, \"mount-folder\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from semanticscholar import SemanticScholar\n",
    "sch = SemanticScholar(api_key=keys['s2_api_key'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# download paper\n",
    "def get_paper(url, file_path): \n",
    "    \"\"\" url to the file and the file_name to download it as \"\"\"\n",
    "    if os.path.isfile(file_path) == False:\n",
    "            try: \n",
    "                urllib.request.urlretrieve(url, file_path)\n",
    "                urllib.request.urlcleanup()\n",
    "            except: \n",
    "                print(f\"Error for {file_path}\")\n",
    "        else: \n",
    "            print(f\"{i}, File Exists\")\n",
    "    \n",
    "\n",
    "def unzip_file(zip_file_path, ext_file_path): \n",
    "    \"\"\" make sure the current working directory is set to where you want the files \"\"\"\n",
    "    if os.path.isfile(ext_file_path) == False:\n",
    "        try: \n",
    "            with gzip.open(zip_file_path, 'rb') as f_in:\n",
    "                with open(ext_file_path, 'wb') as f_out:\n",
    "                    shutil.copyfileobj(f_in, f_out)\n",
    "        except: \n",
    "            print(f\"Was not able to extract file {ext_file_path}\")\n",
    "    else:\n",
    "        print(f\"{i}, File Exists\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Multithreading\n",
    "def download(link, filelocation):\n",
    "    r = requests.get(link, stream=True)\n",
    "    with open(filelocation, 'wb') as f:\n",
    "        for chunk in r.iter_content(1024):\n",
    "            if chunk:\n",
    "                f.write(chunk)\n",
    "\n",
    "def createNewDownloadThread(link, filelocation):\n",
    "    download_thread = threading.Thread(target=download, args=(link,filelocation))\n",
    "    download_thread.start()\n",
    "\n",
    "# for i in range(0,5):\n",
    "#     file = \"C:\\\\test\" + str(i) + \".png\"\n",
    "#     print file\n",
    "#     createNewDownloadThread(\"http://stackoverflow.com/users/flair/2374517.png\", file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Get Papers and latests releases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get info about the papers dataset\n",
    "papers = requests.get(config['semantic_scholar']['papers'],\n",
    "                      headers={'x-api-key':keys['x-api-key']}).json()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get info about the latest release\n",
    "latest_release = requests.get(config['semantic_scholar']['latest']).json()\n",
    "\n",
    "# Get info about past releases\n",
    "dataset_ids = requests.get(config['semantic_scholar']['release']).json()\n",
    "earliest_release = requests.get(f\"http://api.semanticscholar.org/datasets/v1/release/{dataset_ids[0]}\").json()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Download Files\n",
    "Create the file paths for the zipped and extracted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "download_flag = False\n",
    "extract_flag = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create base file names\n",
    "file_names = [f\"papers-part{n}.jsonl.gz\" for n in range(len(papers['files']))]\n",
    "\n",
    "# create zipped file paths\n",
    "paper_zip = [os.path.join(mount_path, \"zipped\", f) for f in file_names]\n",
    "\n",
    "# create extracted file paths\n",
    "paper_fn = [os.path.join(mount_path, \"extracted\", f.strip(\"\\.gz\")) for f in file_names]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Takes ~30 minutes\n",
    "if download_flag == True: \n",
    "    with multiprocessing.Pool() as pool:\n",
    "        pool.starmap(unzip_file, zip(papers['files'], paper_zip), chunksize=10)\n",
    "    # downloads the files directly into the google cloud bucket\n",
    "    for i, (url, file_path) in tqdm(enumerate(zip(papers['files'], paper_zip))): \n",
    "        if os.path.isfile(file_path) == False:\n",
    "            try: \n",
    "                get_paper(url, file_path)\n",
    "            except: \n",
    "                print(f\"Error for {file_path}\")\n",
    "        else: \n",
    "            print(f\"{i}, File Exists\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "## Multiprocessing to extract multiple files at once\n",
    "# Takes ~FOREVER minutes\n",
    "if extract_flag == True: \n",
    "    with multiprocessing.Pool() as pool:\n",
    "        pool.starmap(unzip_file, zip(paper_zip, paper_fn), chunksize=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Takes ~ FOREVER mins\n",
    "if extract_flag == True: \n",
    "    for tqdm(file_path) in paper_zip:\n",
    "        if os.path.isfile(file_path) == False:\n",
    "            unzip_file(file_path)\n",
    "        else: \n",
    "            print(f\"{i}, File Exists\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parse JSON file to upload into database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['/home/nick_lee_berkeley_edu/mount-folder/extracted/papers-part0.jsonl',\n",
       " '/home/nick_lee_berkeley_edu/mount-folder/extracted/papers-part1.jsonl',\n",
       " '/home/nick_lee_berkeley_edu/mount-folder/extracted/papers-part10.jsonl',\n",
       " '/home/nick_lee_berkeley_edu/mount-folder/extracted/papers-part11.jsonl',\n",
       " '/home/nick_lee_berkeley_edu/mount-folder/extracted/papers-part12.jsonl',\n",
       " '/home/nick_lee_berkeley_edu/mount-folder/extracted/papers-part2.jsonl',\n",
       " '/home/nick_lee_berkeley_edu/mount-folder/extracted/papers-part20.jsonl',\n",
       " '/home/nick_lee_berkeley_edu/mount-folder/extracted/papers-part21.jsonl',\n",
       " '/home/nick_lee_berkeley_edu/mount-folder/extracted/papers-part22.jsonl',\n",
       " '/home/nick_lee_berkeley_edu/mount-folder/extracted/papers-part3.jsonl',\n",
       " '/home/nick_lee_berkeley_edu/mount-folder/extracted/papers-part4.jsonl',\n",
       " '/home/nick_lee_berkeley_edu/mount-folder/extracted/papers-part5.jsonl',\n",
       " '/home/nick_lee_berkeley_edu/mount-folder/extracted/papers-part6.jsonl',\n",
       " '/home/nick_lee_berkeley_edu/mount-folder/extracted/papers-part7.jsonl']"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attribute_files = glob.glob(\"\".join([mount_path, '/extracted/*?.jsonl']))\n",
    "attribute_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6971179"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(json_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def json_to_df(j): \n",
    "    \"\"\" Create dataframe to upload into database \"\"\"\n",
    "    return pd.DataFrame([json.loads(j)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Upload Data to Postgres DB"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create connection to database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sqlalchemy.orm import Session\n",
    "from sqlalchemy import select, Table, Column, Integer, String, Boolean, MetaData\n",
    "from sqlalchemy.dialects.postgresql import JSONB\n",
    "from sqlalchemy.types import ARRAY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<sqlalchemy.engine.base.Connection at 0x7f9e2cba2260>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# connection libraries\n",
    "from google.cloud.sql.connector import Connector, IPTypes\n",
    "import pg8000\n",
    "import sqlalchemy\n",
    "\n",
    "# connect to goolge cloud postgres db\n",
    "def connect_with_connector() -> sqlalchemy.engine.base.Engine:\n",
    "    \"\"\"\n",
    "    Initializes a connection pool for a Cloud SQL instance of Postgres.\n",
    "\n",
    "    Uses the Cloud SQL Python Connector package.\n",
    "    \"\"\"\n",
    "    # Note: Saving credentials in environment variables is convenient, but not\n",
    "    # secure - consider a more secure solution such as\n",
    "    # Cloud Secret Manager (https://cloud.google.com/secret-manager) to help\n",
    "    # keep secrets safe.\n",
    "\n",
    "    instance_connection_name = config['database']['connection'] # i.e demo-project:us-central1:demo-instance\n",
    "    db_user = config['database']['user']\n",
    "    db_pass = config['database']['password']\n",
    "    db_name = config['database']['name']\n",
    "\n",
    "    ip_type = IPTypes.PRIVATE # if os.environ.get(\"PRIVATE_IP\") else IPTypes.PUBLIC\n",
    "\n",
    "    # initialize Cloud SQL Python Connector object\n",
    "    connector = Connector()\n",
    "\n",
    "    def getconn() -> pg8000.dbapi.Connection:\n",
    "        conn: pg8000.dbapi.Connection = connector.connect(\n",
    "            instance_connection_name,\n",
    "            \"pg8000\",\n",
    "            db=db_name,\n",
    "            user=db_user,\n",
    "            password = db_pass,\n",
    "            # enable_iam_auth=True,\n",
    "            ip_type=ip_type\n",
    "        )\n",
    "        return conn\n",
    "\n",
    "    # The Cloud SQL Python Connector can be used with SQLAlchemy\n",
    "    # using the 'creator' argument to 'create_engine'\n",
    "    pool = sqlalchemy.create_engine(\n",
    "        \"postgresql+pg8000://\",\n",
    "        creator=getconn,\n",
    "        # [START_EXCLUDE]\n",
    "        # Pool size is the maximum number of permanent connections to keep.\n",
    "        pool_size=5,\n",
    "        # Temporarily exceeds the set pool_size if no connections are available.\n",
    "        max_overflow=2,\n",
    "        # The total number of concurrent connections for your application will be\n",
    "        # a total of pool_size and max_overflow.\n",
    "        # 'pool_timeout' is the maximum number of seconds to wait when retrieving a\n",
    "        # new connection from the pool. After the specified amount of time, an\n",
    "        # exception will be thrown.\n",
    "        pool_timeout=30,  # 30 seconds\n",
    "        # 'pool_recycle' is the maximum number of seconds a connection can persist.\n",
    "        # Connections that live longer than the specified amount of time will be\n",
    "        # re-established\n",
    "        pool_recycle=1800,  # 30 minutes\n",
    "        # [END_EXCLUDE]\n",
    "    )\n",
    "    return pool\n",
    "\n",
    "pool = connect_with_connector()\n",
    "pool.connect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "meta = MetaData()\n",
    "\n",
    "articles = Table(\n",
    "    'article_attributes', meta,\n",
    "    Column('corpusid', Integer, primary_key = True),\n",
    "    Column('externalids', String),\n",
    "    Column('url', String),\n",
    "    Column('title', Integer),\n",
    "    Column('authors', ARRAY(JSONB)),\n",
    "    Column('venue', String),\n",
    "    Column('publicationvenueid', Integer),\n",
    "    Column('year', String),\n",
    "    Column('referencecount', Integer),\n",
    "    Column('citationcount', Integer),\n",
    "    Column('influentialcitationcount', Integer),\n",
    "    Column('isopenaccess', Boolean),\n",
    "    Column('s2fieldsofstudy', ARRAY(JSONB)),\n",
    "    Column('publicationtypes', Integer),\n",
    "    Column('publicationdate', Integer),\n",
    "    Column('journal', JSONB),\n",
    "    Column('updated', String)    \n",
    ")\n",
    "\n",
    "# create table in database\n",
    "meta.create_all(pool)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def df_to_db(df): \n",
    "    with pool.connect() as db_conn:\n",
    "        df.to_sql('article_attributes', con = db_conn, if_exists='append', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import orjson # for faster reading of json\n",
    "import jsonlines # for opening jsonl files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_df(df): \n",
    "    df.year = df.year.astype(\"Int64\")\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test connection \n",
    "# connect to connection pool\n",
    "with pool.connect() as db_conn:\n",
    "    # create ratings table in our sandwiches database\n",
    "    results = db_conn.execute(sqlalchemy.text(\"SELECT * FROM article_attributes\")).fetchall()\n",
    "    \n",
    "    # # show results\n",
    "    # for row in results:\n",
    "    #     print(row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "from zipfile import ZipFile\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "z_files = []\n",
    "for root, dirs, files in os.walk(mount_path, topdown=False):\n",
    "    for name in files:\n",
    "        z_files.append(os.path.join(root, name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "z_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gzip\n",
    "import shutil\n",
    "with gzip.open(z_files[0], 'rb') as f:\n",
    "    file_content = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get embeddings\n",
    "from typing import Dict, List\n",
    "import json\n",
    "\n",
    "import requests\n",
    "\n",
    "\n",
    "URL = \"https://model-apis.semanticscholar.org/specter/v1/invoke\"\n",
    "MAX_BATCH_SIZE = 16\n",
    "\n",
    "def chunks(lst, chunk_size=MAX_BATCH_SIZE):\n",
    "    \"\"\"Splits a longer list to respect batch size\"\"\"\n",
    "    for i in range(0, len(lst), chunk_size):\n",
    "        yield lst[i : i + chunk_size]\n",
    "\n",
    "\n",
    "SAMPLE_PAPERS = [\n",
    "    {\n",
    "        \"paper_id\": \"A\",\n",
    "        \"title\": \"Angiotensin-converting enzyme 2 is a functional receptor for the SARS coronavirus\",\n",
    "        \"abstract\": \"Spike (S) proteins of coronaviruses ...\",\n",
    "    },\n",
    "    {\n",
    "        \"paper_id\": \"B\",\n",
    "        \"title\": \"Hospital outbreak of Middle East respiratory syndrome coronavirus\",\n",
    "        \"abstract\": \"Between April 1 and May 23, 2013, a total of 23 cases of MERS-CoV ...\",\n",
    "    },\n",
    "]\n",
    "\n",
    "\n",
    "def embed(papers):\n",
    "    embeddings_by_paper_id: Dict[str, List[float]] = {}\n",
    "\n",
    "    for chunk in chunks(papers):\n",
    "        # Allow Python requests to convert the data above to JSON\n",
    "        response = requests.post(URL, json=chunk)\n",
    "\n",
    "        if response.status_code != 200:\n",
    "            raise RuntimeError(\"Sorry, something went wrong, please try later!\")\n",
    "\n",
    "        for paper in response.json()[\"preds\"]:\n",
    "            embeddings_by_paper_id[paper[\"paper_id\"]] = paper[\"embedding\"]\n",
    "\n",
    "    return embeddings_by_paper_id\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    all_embeddings = embed(SAMPLE_PAPERS)\n",
    "\n",
    "    # Prints { 'A': [4.089589595794678, ...], 'B': [-0.15814849734306335, ...] }\n",
    "    print(all_embeddings)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "base"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
