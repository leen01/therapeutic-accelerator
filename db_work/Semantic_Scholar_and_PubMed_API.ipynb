{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/leen01/therapeutic_accelerator/blob/main/Semantic_Scholar_and_PubMed_API.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3023a7dc",
      "metadata": {
        "scrolled": true,
        "id": "3023a7dc"
      },
      "outputs": [],
      "source": [
        "!pip install metapub\n",
        "import pandas as pd\n",
        "from metapub import PubMedFetcher"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "112afaee",
      "metadata": {
        "id": "112afaee"
      },
      "source": [
        "***Use Pubmed API to get attributes of article (not full text)***"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9de146bf",
      "metadata": {
        "id": "9de146bf"
      },
      "outputs": [],
      "source": [
        "keyword=\"37248708\" #enter pubmedid, keyword, title\n",
        "num_of_articles=3\n",
        "\n",
        "fetch = PubMedFetcher()\n",
        "\n",
        "# get the  PMID for first 3 articles with keyword sepsis\n",
        "pmids = fetch.pmids_for_query(keyword, retmax=num_of_articles)\n",
        "\n",
        "# get  articles\n",
        "articles = {}\n",
        "for pmid in pmids:\n",
        "    articles[pmid] = fetch.article_by_pmid(pmid)\n",
        "    \n",
        "# get abstract for each article:\n",
        "abstracts = {}\n",
        "for pmid in pmids:\n",
        "    abstracts[pmid] = fetch.article_by_pmid(pmid).abstract\n",
        "Abstract = pd.DataFrame(list(abstracts.items()),columns = ['pmid','Abstract'])\n",
        "Abstract\n",
        "\n",
        "titles = {}\n",
        "for pmid in pmids:\n",
        "    titles[pmid] = fetch.article_by_pmid(pmid).title\n",
        "Title = pd.DataFrame(list(titles.items()),columns = ['pmid','Title'])\n",
        "Title\n",
        "\n",
        "authors = {}\n",
        "for pmid in pmids:\n",
        "    authors[pmid] = fetch.article_by_pmid(pmid).authors\n",
        "Author = pd.DataFrame(list(authors.items()),columns = ['pmid','Author'])\n",
        "Author\n",
        "\n",
        "years = {}\n",
        "for pmid in pmids:\n",
        "    years[pmid] = fetch.article_by_pmid(pmid).year\n",
        "Year = pd.DataFrame(list(years.items()),columns = ['pmid','Year'])\n",
        "Year\n",
        "\n",
        "volumes = {}\n",
        "for pmid in pmids:\n",
        "    volumes[pmid] = fetch.article_by_pmid(pmid).volume\n",
        "Volume = pd.DataFrame(list(volumes.items()),columns = ['pmid','Volume'])\n",
        "Volume\n",
        "\n",
        "issues = {}\n",
        "for pmid in pmids:\n",
        "    issues[pmid] = fetch.article_by_pmid(pmid).issue\n",
        "Issue = pd.DataFrame(list(issues.items()),columns = ['pmid','Issue'])\n",
        "Issue\n",
        "\n",
        "journals = {}\n",
        "for pmid in pmids:\n",
        "    journals[pmid] = fetch.article_by_pmid(pmid).journal\n",
        "Journal = pd.DataFrame(list(journals.items()),columns = ['pmid','Journal'])\n",
        "Journal\n",
        "\n",
        "citations = {}\n",
        "for pmid in pmids:\n",
        "    citations[pmid] = fetch.article_by_pmid(pmid).citation\n",
        "Citation = pd.DataFrame(list(citations.items()),columns = ['pmid','Citation'])\n",
        "Citation\n",
        "\n",
        "links={}\n",
        "for pmid in pmids:\n",
        "    links[pmid] = \"https://pubmed.ncbi.nlm.nih.gov/\"+pmid+\"/\"\n",
        "Link = pd.DataFrame(list(links.items()),columns = ['pmid','Link'])\n",
        "Link\n",
        "\n",
        "data_frames = [Title,Abstract,Author,Year,Volume,Issue,Journal,Citation,Link]\n",
        "from functools import reduce\n",
        "df_merged = reduce(lambda  left,right: pd.merge(left,right,on=['pmid'],\n",
        "                                            how='outer'), data_frames)\n",
        "df_merged"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "edc748bd",
      "metadata": {
        "id": "edc748bd"
      },
      "source": [
        "***Get URL for Pubmed articles IF they are publically available***"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8e416377",
      "metadata": {
        "id": "8e416377"
      },
      "outputs": [],
      "source": [
        "from metapub import FindIt\n",
        "from metapub import FindIt\n",
        "\n",
        "src = FindIt('18381613')\n",
        "if src.url is None: print(src.reason)\n",
        "else print(src.url)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1d5eaa43",
      "metadata": {
        "id": "1d5eaa43"
      },
      "source": [
        "***Public Semantic Scholar API to pull various attributes***"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4bd0e867",
      "metadata": {
        "id": "4bd0e867"
      },
      "outputs": [],
      "source": [
        "!pip install semanticscholar\n",
        "from semanticscholar import SemanticScholar\n",
        "sch = SemanticScholar()\n",
        "\n",
        "\n",
        "sch = SemanticScholar(api_key='jvN8XitMY59Wtoqm3FZnc94qRHnsKw3z3hLNh7XB')\n",
        "\n",
        "paper = sch.get_paper('d0bc1501ae6f54dd16534e651d90d2aeeeb1cfc1')  #'10.1093/mind/lix.236.433')\n",
        "\n",
        "paper.referenceCount #number of references in this one, can help us create some sort of pagerank type algo\n",
        "paper.citationCount #number of papers that cited this one\n",
        "paper.externalIds \n",
        "paper.abstract\n",
        "paper.embedding\n",
        "\n",
        "#full list of fields below\n",
        "#https://api.semanticscholar.org/api-docs/graph#tag/Paper-Data/operation/post_graph_get_papers \n",
        "\n",
        "#documentation on how they do embeddings\n",
        "#https://github.com/allenai/specter"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1cecc17a",
      "metadata": {
        "id": "1cecc17a"
      },
      "outputs": [],
      "source": [
        "from semanticscholar import SemanticScholar\n",
        "s2_api_key = 'T4i5jTJcbn3dPd6IyMPF92ICHvz7wMBa8BNRvnC8'\n",
        "sch = SemanticScholar(api_key=s2_api_key)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a0751afb",
      "metadata": {
        "id": "a0751afb"
      },
      "source": [
        "**Semantic Scholar API to download full dataset of papers**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "88c665f7",
      "metadata": {
        "id": "88c665f7"
      },
      "outputs": [],
      "source": [
        "import requests\n",
        "import urllib\n",
        "import os\n",
        "\n",
        "# Get info about the latest release\n",
        "latest_release = requests.get(\"http://api.semanticscholar.org/datasets/v1/release/latest\").json()\n",
        "\n",
        "# Get info about past releases\n",
        "dataset_ids = requests.get(\"http://api.semanticscholar.org/datasets/v1/release\").json()\n",
        "earliest_release = requests.get(f\"http://api.semanticscholar.org/datasets/v1/release/{dataset_ids[0]}\").json()\n",
        "\n",
        "# Get info about the papers dataset\n",
        "papers = requests.get(\"http://api.semanticscholar.org/datasets/v1/release/latest/dataset/papers\",\n",
        "                      headers={'x-api-key':'jvN8XitMY59Wtoqm3FZnc94qRHnsKw3z3hLNh7XB'}).json()\n",
        "\n",
        "# Download the first part of the dataset\n",
        "# urllib.request.urlretrieve(papers['files'][0], \"papers-part0.jsonl.gz\") \n",
        "# above is crashing my session, may need to request an increase in data\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "18178922",
      "metadata": {
        "id": "18178922",
        "outputId": "379fc21e-165d-4f73-9869-7f304080f7ba"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[\n",
            "  {\n",
            "    \"paperId\": \"649def34f8be52c8b66281af98ae884c09aef38b\",\n",
            "    \"title\": \"Construction of the Literature Graph in Semantic Scholar\",\n",
            "    \"referenceCount\": 27,\n",
            "    \"citationCount\": 308\n",
            "  },\n",
            "  {\n",
            "    \"paperId\": \"f712fab0d58ae6492e3cdfc1933dae103ec12d5d\",\n",
            "    \"title\": \"Reinfection and low cross-immunity as drivers of epidemic resurgence under high seroprevalence: a model-based approach with application to Amazonas, Brazil\",\n",
            "    \"referenceCount\": 13,\n",
            "    \"citationCount\": 0\n",
            "  }\n",
            "]\n"
          ]
        }
      ],
      "source": [
        "import json \n",
        "\n",
        "r = requests.post(\n",
        "    'https://api.semanticscholar.org/graph/v1/paper/batch',\n",
        "    params={'fields': 'referenceCount,citationCount,title'},\n",
        "    json={\"ids\": [\"649def34f8be52c8b66281af98ae884c09aef38b\", \"ARXIV:2106.15928\"]}\n",
        ")\n",
        "print(json.dumps(r.json(), indent=2))\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c3694c3f",
      "metadata": {
        "id": "c3694c3f"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.15"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}