{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import dask\n",
    "import dask.dataframe as dd\n",
    "\n",
    "import logging\n",
    "\n",
    "from dask import distributed\n",
    "import re\n",
    "\n",
    "# with open(\"/home/ubuntu/work/therapeutic_accelerator/scripts/base.py\") as f:\n",
    "#     exec(f.read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create embeddings function with specter model\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained('allenai/specter')\n",
    "model = AutoModel.from_pretrained('allenai/specter')\n",
    "\n",
    "from chromadb.api.types import Documents, EmbeddingFunction, Embeddings\n",
    "\n",
    "class specter_ef(EmbeddingFunction):\n",
    "    def __init__(self, model, tokenizer):\n",
    "        self.model = model\n",
    "        self.tokenizer = tokenizer\n",
    "    \n",
    "    def embed_documents(self, texts: Documents) -> Embeddings:\n",
    "        \n",
    "        text_list = [re.sub(\"\\n\", \" \", p) for p in texts]\n",
    "        texts = [re.sub(\"\\s\\s+\", \" \", t) for t in text_list]\n",
    "        \n",
    "        # embed the documents somehow\n",
    "        embeddings = []\n",
    "        \n",
    "        for text in texts:\n",
    "            inputs = tokenizer(text, padding=True, truncation=True, return_tensors=\"pt\", max_length=512)\n",
    "            result = model(**inputs)\n",
    "            embeddings.append(result.last_hidden_state[:, 0, :])\n",
    "        \n",
    "        return embeddings\n",
    "    \n",
    "specter_embeder = specter_ef(model, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create dask cluster\n",
    "dask.config.set(scheduler='processes')  # overwrite default with multiprocessing scheduler\n",
    "\n",
    "cluster = distributed.LocalCluster(name='local', n_workers=7, memory_limit = '4GiB', threads_per_worker=4)  # Launches a scheduler and workers locally\n",
    "client = distributed.client._get_global_client() or distributed.Client(cluster)\n",
    "\n",
    "client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "# import tiktoken\n",
    "\n",
    "# @dask.delayed\n",
    "def token_len(text): \n",
    "    \"\"\" Get the length of tokens from text\"\"\"\n",
    "    tokens = tokenizer(text, padding=True, truncation=True, return_tensors=\"pt\", max_length=512)['input_ids'][0]\n",
    "    return len(tokens)\n",
    "    \n",
    "chunk_size = 2000\n",
    "\n",
    "# create text splitters for processing the texts\n",
    "text_splitter = CharacterTextSplitter(\n",
    "    separator = \"\\n\\n\",\n",
    "    chunk_size = chunk_size,\n",
    "    chunk_overlap  = 20,\n",
    "    length_function = token_len\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import chromadb\n",
    "from chromadb.config import Settings\n",
    "\n",
    "# Create chroma client\n",
    "chroma = chromadb.Client(Settings(chroma_api_impl=\"rest\",\n",
    "                                  chroma_server_host=\"54.210.84.192\", # EC2 instance public IPv4\n",
    "                                  chroma_server_http_port=8000))\n",
    "\n",
    "print(\"Nanosecond heartbeat on server\", chroma.heartbeat()) # returns a nanosecond heartbeat. Useful for making sure the client remains connected.\n",
    "\n",
    "# Check Existing connections\n",
    "chroma.list_collections()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "collection = chroma.get_or_create_collection(\"specter_abstracts\", embedding_function=specter_ef(model, tokenizer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_text(row):\n",
    "    \"\"\" Split text into chunks \"\"\"\n",
    "    return text_splitter.split_text(row['abstract'])\n",
    "\n",
    "def create_doc(splited_text, corpusid):\n",
    "    \"\"\" Create documents for each chunk \"\"\"\n",
    "        \n",
    "    try:\n",
    "        docs = {\n",
    "            \"documents\": splited_text, # list of all documents [doc1, doc2, doc3, ...]\n",
    "            'ids': [f'{corpusid}-{i}' for i in range(len(splited_text))], # list of all ids [id1, id2, id3, ...]\n",
    "            'metadatas': [{'corpusid': int(corpusid), 'chunk': i} for i in range(len(splited_text))] # list of dictionaries with metadata for each document\n",
    "        }\n",
    "        return docs\n",
    "\n",
    "    except Exception as e:\n",
    "        logging.error(e)\n",
    "        \n",
    "def add_to_collection(docs, collection):\n",
    "    \"\"\" Add documents to collection \"\"\"\n",
    "    \n",
    "    try:\n",
    "        collection.add(**docs)\n",
    "        return True\n",
    "    \n",
    "    except Exception as e:\n",
    "        logging.error(e)\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dask functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def df_main(row, collection): \n",
    "    \"\"\" Main workflow \"\"\"\n",
    "    \n",
    "    splited_text = split_text(row)\n",
    "    \n",
    "    docs = create_doc(splited_text, row['corpusId'])\n",
    "    \n",
    "    docs['embeddings'] = specter_embeder.embed_documents(docs['documents'])[0][0].tolist()\n",
    "    \n",
    "    addition_results = add_to_collection(docs, collection)\n",
    "    \n",
    "    return docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def over_ddf(ddf, collection): \n",
    "    return ddf.apply(lambda x: x.apply(main), axis=1, meta=('x', 'object'), collection=collection)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "abstracts = pd.read_csv(\"/home/ubuntu/work/data/abstracts.csv\")\n",
    "abstracts.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "abstracts = abstracts.map_partitions(over_ddf, args = (collection), meta=('docs', 'object'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "abstracts.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = abstracts.apply(main, axis=1, args=(collection,))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "collection.get(\n",
    "    include=['documents']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s3 = boto3.resource('s3')\n",
    "\n",
    "# Print out bucket names\n",
    "for bucket in s3.buckets.all():\n",
    "    print(bucket.name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "torch.save(test, '/home/ubuntu/work/bucket/tensors_abstracts/tensor0-0.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import dask processingbar\n",
    "from dask.diagnostics import ProgressBar\n",
    "\n",
    "with ProgressBar():\n",
    "    tokens = df['abstract'].apply(tokenize_abstracts, meta=('abstract', 'object')).compute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized = client.map(tokenize_abstracts, df['abstract'])\n",
    "inputs = client.map(run_inputs, tokenized)\n",
    "embeddings = client.submit(get_embeddings, inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import dask processingbar\n",
    "from dask.diagnostics import ProgressBar\n",
    "\n",
    "with ProgressBar():\n",
    "    abstract_embeddings = ddf['abstract'].apply(get_embeddings, meta=('abstract', 'object')).compute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "chroma",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
