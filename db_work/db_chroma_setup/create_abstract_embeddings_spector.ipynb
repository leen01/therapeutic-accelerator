{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import dask\n",
    "import dask.dataframe as dd\n",
    "\n",
    "# with open(\"/home/ubuntu/work/therapeutic_accelerator/scripts/base.py\") as f:\n",
    "#     exec(f.read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create embeddings function with specter model\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained('allenai/specter')\n",
    "model = AutoModel.from_pretrained('allenai/specter')\n",
    "\n",
    "# @dask.delayed\n",
    "def tokenize_abstracts(abstracts):\n",
    "    inputs = tokenizer(abstracts, padding=True, truncation=True, return_tensors=\"pt\", max_length=512)\n",
    "    return inputs\n",
    "\n",
    "# @dask.delayed\n",
    "def get_embeddings(inputs):\n",
    "    result = model(**inputs).last_hidden_state[:, 0, :]\n",
    "    return result\n",
    "\n",
    "# @dask.delayed\n",
    "# def get_embeddings(result):\n",
    "#     embeddings = result.last_hidden_state[:, 0, :]\n",
    "#     return embeddings\n",
    "\n",
    "# def full_embedding_pipeline(abstract):\n",
    "#     inputs = tokenizer(abstracts, padding=True, truncation=True, return_tensors=\"pt\", max_length=512)\n",
    "#     embeddings = model(**inputs).last_hidden_state[:, 0, :]\n",
    "#     return embeddings\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dask.distributed import Client, LocalCluster, progress\n",
    "\n",
    "cluster = LocalCluster(name='local', n_workers=5, memory_limit = '4GiB', threads_per_worker=2)  # Launches a scheduler and workers locally\n",
    "client = Client(cluster)  # Connect to distributed cluster and override default\n",
    "\n",
    "# client = Client(threads_per_worker=4, n_workers=10)\n",
    "client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = dd.read_parquet('/home/ubuntu/work/data/abstracts/abstracts-1.parquet', columns = 'abstract', blocksize = '200MB')\n",
    "abstracts = df.compute()\n",
    "len(abstracts)\n",
    "\n",
    "# turn backinto dask dataframe\n",
    "dd_abs = dd.from_pandas(abstracts, npartitions=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = dd_abs.apply(tokenize_abstracts, meta=('abstract', 'object'))\n",
    "tokens = tokens.compute()\n",
    "tokens = dd.from_pandas(tokens, npartitions=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ipycytoscape\n",
    "\n",
    "# visualize the low level Dask graph after optimizations\n",
    "tokens.visualize(optimize_graph=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = tokens.apply(get_embeddings, meta=('embeddings', 'object'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = embeddings.compute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # dask read sql table\n",
    "\n",
    "# df = dd.read_sql_table('abstracts',\n",
    "#                        f'postgresql://postgres:{keys[\"postgres\"]}@{config[\"database\"][\"host\"]}:5432/postgres',\n",
    "#                        index_col='id',\n",
    "#                        npartitions=100)\n",
    "\n",
    "# df.shape\n",
    "\n",
    "\n",
    "# trying to write out to parquet to make things faster. Maybe it can be done in chunks?\n",
    "# name_function = lambda x: f\"abstracts-{x}.parquet\"\n",
    "# df.to_parquet('/home/ubuntu/work/data/abstracts/', name_function = name_function)\n",
    "\n",
    "# # df = df.compute()\n",
    "# # df.reset_index(drop = True, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "futures = []\n",
    "for i in range(100):\n",
    "    futures.append(client.submit(full_embedding_pipeline, abstracts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s3 = boto3.resource('s3')\n",
    "\n",
    "# Print out bucket names\n",
    "for bucket in s3.buckets.all():\n",
    "    print(bucket.name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "torch.save(test, '/home/ubuntu/work/bucket/tensors_abstracts/tensor0-0.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import dask processingbar\n",
    "from dask.diagnostics import ProgressBar\n",
    "\n",
    "with ProgressBar():\n",
    "    tokens = df['abstract'].apply(tokenize_abstracts, meta=('abstract', 'object')).compute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized = client.map(tokenize_abstracts, df['abstract'])\n",
    "inputs = client.map(run_inputs, tokenized)\n",
    "embeddings = client.submit(get_embeddings, inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import dask processingbar\n",
    "from dask.diagnostics import ProgressBar\n",
    "\n",
    "with ProgressBar():\n",
    "    abstract_embeddings = ddf['abstract'].apply(get_embeddings, meta=('abstract', 'object')).compute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import chromadb\n",
    "from chromadb.config import Settings\n",
    "from chromadb.utils import embedding_functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create chroma client\n",
    "chroma = chromadb.Client(Settings(chroma_api_impl=\"rest\",\n",
    "                                  chroma_server_host=\"54.175.241.78\", # EC2 instance public IPv4\n",
    "                                  chroma_server_http_port=8000))\n",
    "\n",
    "print(\"Nanosecond heartbeat on server\", chroma.heartbeat()) # returns a nanosecond heartbeat. Useful for making sure the client remains connected.\n",
    "\n",
    "# Check Existing connections\n",
    "chroma.list_collections()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "chroma",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
