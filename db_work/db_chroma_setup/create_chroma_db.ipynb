{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import chromadb\n",
    "from chromadb.config import Settings\n",
    "from chromadb.utils import embedding_functions\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "import sqlalchemy as sa\n",
    "import json\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import T5Tokenizer\n",
    "\n",
    "max_sequence_length = 1200\n",
    "embedding_size = 200\n",
    "\n",
    "# Create tokenizer for T5 model\n",
    "T5tokens = T5Tokenizer.from_pretrained('t5-base', model_max_length = max_sequence_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# I got tired of copying the same code all the time\n",
    "# loads config for main parts and creates engine for sqlalchemy\n",
    "%run /home/ubuntu/work/therapeutic_accelerator/scripts/create_sqlalchemy_engine.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create chroma client\n",
    "chroma = chromadb.Client(Settings(chroma_api_impl=\"rest\",\n",
    "                                  chroma_server_host=\"54.175.241.78\", # EC2 instance public IPv4\n",
    "                                  chroma_server_http_port=8000))\n",
    "\n",
    "print(\"Nanosecond heartbeat on server\", chroma.heartbeat()) # returns a nanosecond heartbeat. Useful for making sure the client remains connected.\n",
    "\n",
    "# Check Existing connections\n",
    "chroma.list_collections()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Embedding fuctions\n",
    "# from transformers import AutoTokenizer, AutoModel\n",
    "\n",
    "# # AllenAI Specter\n",
    "# tokenizer = AutoTokenizer.from_pretrained('allenai/specter')\n",
    "# model = AutoModel.from_pretrained('allenai/specter')\n",
    "\n",
    "# def get_embeddings(test):\n",
    "#     inputs = tokenizer(test, padding=True, truncation=True, return_tensors=\"pt\", max_length=512)\n",
    "#     result = model(**inputs)\n",
    "#     embeddings = result.last_hidden_state[:, 0, :]\n",
    "#     print (embeddings)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sentence Transformers all-MiniLM-L6-v2 \n",
    "default_ef = embedding_functions.DefaultEmbeddingFunction()\n",
    "\n",
    "# Create collection to store embeddings with T5 sentence transformer\n",
    "def create_collection(chroma, name, metadata = {\"hnsw:space\":\"cosine\"}, embedding_function = default_ef):\n",
    "    try:\n",
    "        chroma.create_collection(name=name, metadata=metadata, embedding_function=embedding_function)\n",
    "    except Exception as e:\n",
    "        logging.error(e)\n",
    "\n",
    "collection = chroma.get_or_create_collection(\"fulltext\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create embeddings table in SQL for abstracts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# chroma.delete_collection('abstract_sentence')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Add data to collection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dictionary(text):\n",
    "    \"\"\" Turn string containing list of dictionaries into a dictionary\"\"\"\n",
    "    \n",
    "    # remove new line characters\n",
    "    categories = re.sub(r'[\\[\\]\\'\\\\]', '', text)\n",
    "\n",
    "    # remove outer brackets, quotes, and split on commas\n",
    "    categories = categories.strip('{}').strip('\"').split('\",\"')\n",
    "\n",
    "    # create list with unique values from category\n",
    "    # categories = pd.Series([json.loads(t)['category'] for t in categories]).unique().tolist()\n",
    "    categories = [json.loads(t) for t in categories]\n",
    "    \n",
    "    return categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get attributes table for the metadata embeddings\n",
    "table_name = 'fulltext'\n",
    "\n",
    "sql = sa.text(f''' \n",
    "    SELECT * from {table_name} LEFT JOIN attributes ON ({table_name}.corpusid = CAST(attributes.corpusid as text));\n",
    "''')\n",
    "\n",
    "with engine.connect() as conn: \n",
    "    query = conn.execute(sql)\n",
    "    \n",
    "ft = pd.DataFrame(query.fetchall())\n",
    "\n",
    "# remove unncecessary columns\n",
    "# ft.drop(columns = ['paperId', 'corpusId', 'index', 'id'], inplace = True)\n",
    "\n",
    "# turn strings into list of dictionaries\n",
    "ft['s2fieldsofstudy'] = ft['s2fieldsofstudy'].apply(create_dictionary).apply(lambda x: pd.Series([d['category'] for d in x]).unique().tolist())\n",
    "ft['authors'] = ft['authors'].apply(create_dictionary)\n",
    "\n",
    "fulltext = ft.text\n",
    "fulltext.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Get attributes table for the metadata embeddings\n",
    "# table_name = 'abstracts_encodings'\n",
    "\n",
    "# sql = sa.text(f''' \n",
    "#     SELECT * from {table_name} LEFT JOIN attributes ON ({table_name}.\"corpusId\" = CAST(attributes.corpusid as text)) LIMIT 10;\n",
    "# ''')\n",
    "\n",
    "# with engine.connect() as conn: \n",
    "#     query = conn.execute(sql)\n",
    "    \n",
    "# att = pd.DataFrame(query.fetchall())\n",
    "\n",
    "# # remove unncecessary columns\n",
    "# att.drop(columns = ['paperId', 'corpusId', 'index', 'id'], inplace = True)\n",
    "\n",
    "# # turn strings into list of dictionaries\n",
    "# att['s2fieldsofstudy'] = att['s2fieldsofstudy'].apply(create_dictionary).apply(lambda x: pd.Series([d['category'] for d in x]).unique().tolist())\n",
    "# att['authors'] = att['authors'].apply(create_dictionary)\n",
    "\n",
    "# abstracts = att.abstract\n",
    "# abstracts.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Get attributes table for the metadata embeddings\n",
    "# table_name = 'abstracts_encodings'\n",
    "\n",
    "# sql = sa.text(f''' \n",
    "#     SELECT * from {table_name} LIMIT 10;\n",
    "# ''')\n",
    "\n",
    "# with engine.connect() as conn: \n",
    "#     query = conn.execute(sql)\n",
    "    \n",
    "# att = pd.DataFrame(query.fetchall())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sqlalchemy import Table, Column, Integer, String, MetaData, ARRAY\n",
    "\n",
    "# # Create a metadata object\n",
    "# metadata = MetaData()\n",
    "\n",
    "# # Define a table using the metadata object\n",
    "# abstract_encodings = Table(\n",
    "#     'fulltext_embeddings',\n",
    "#     metadata,\n",
    "#     Column('paperId', String, primary_key=True),\n",
    "#     Column('corpusId', Integer),\n",
    "#     Column('abstract', String),\n",
    "#     Column('input_ids', ARRAY(Integer)),\n",
    "#     Column('attention_mask', ARRAY(Integer))\n",
    "# )\n",
    "\n",
    "# # Create the table in the database\n",
    "# metadata.create_all(engine)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # # Get attributes table for the metadata embeddings\n",
    "# table_name = 'abstracts_encodings'\n",
    "\n",
    "# # sql query text that sets the creates an index column\n",
    "# sql = text(''' \n",
    "#     CREATE INDEX id \n",
    "#     ON abstracts_encodings(\"corpusId\");\n",
    "# ''')\n",
    "\n",
    "# with engine.connect() as conn: \n",
    "#     query = conn.execute(sql)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sqlalchemy.ext.automap import automap_base\n",
    "\n",
    "# Reflect the existing database schema\n",
    "Base = automap_base()\n",
    "Base.prepare(engine, reflect=True)\n",
    "\n",
    "# Access the existing table you want to create a class for\n",
    "TableClass = Base.classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "attributes = TableClass.attributes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sqlalchemy import Column, Integer, String, ForeignKey\n",
    "from sqlalchemy.orm import declarative_base\n",
    "from sqlalchemy.orm import relationship\n",
    "\n",
    "# Define the database connection\n",
    "Base = declarative_base()\n",
    "\n",
    "# Define the models for the tables\n",
    "class abstract_encodings(Base):\n",
    "    __tablename__ = 'abstracts_encodings'\n",
    "    id = Column(Integer, primary_key=True)\n",
    "    paperId = Column(String, primary_key=True)\n",
    "    corpusId = Column(Integer)\n",
    "    abstract = Column(String)\n",
    "    input_ids = Column(ARRAY(Integer))\n",
    "    attention_mask = Column(ARRAY(Integer))\n",
    "    attributes = relationship(ForeignKey(\"attributes.corpusid\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sqlalchemy import select\n",
    "stmt = (\n",
    "    select(abstract_encodings)\n",
    "    .join(abstract_encodings.attributes)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sqlalchemy.orm import sessionmaker\n",
    "Session = sessionmaker(bind=engine)\n",
    "session = Session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import dask.dataframe as dd\n",
    "# from dask.diagnostics import ProgressBar\n",
    "# import pandas as pd\n",
    "\n",
    "# # Read the joined tables using Dask\n",
    "# with ProgressBar():\n",
    "#     df = dd.read_sql_query(query, database_uri, index_col='idx_corpusid')\n",
    "\n",
    "# # Convert Dask dataframe to pandas dataframe\n",
    "# # df = df.compute()\n",
    "\n",
    "# # Print the resulting dataframe\n",
    "# # print(df.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# With dask dataframe to partition the query into pieces and prevent maxing out machine\n",
    "import dask.dataframe as dd\n",
    "\n",
    "table_name = 'fulltext'\n",
    "\n",
    "sql = sa.text(f''' \n",
    "    SELECT * from {table_name} LEFT JOIN attributes ON ({table_name}.corpusid = CAST(attributes.corpusid as text));\n",
    "''')\n",
    "\n",
    "df = dd.read_sql_query(sql, str(engine.url), index_col = 'index', head_rows=10)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing embedding creation for abstracts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.text_splitter import CharacterTextSplitter, RecursiveCharacterTextSplitter\n",
    "import tiktoken\n",
    "\n",
    "def token_len(text): \n",
    "    \"\"\" Get the length of tokens from text\"\"\"\n",
    "    tokens = T5tokens.encode(text)\n",
    "    return len(tokens)\n",
    "    \n",
    "# create text splitters for processing the texts\n",
    "text_splitter = CharacterTextSplitter(\n",
    "    separator = \"\\n\\n\",\n",
    "    chunk_size = 400,\n",
    "    chunk_overlap  = 200,\n",
    "    length_function = token_len,\n",
    ")\n",
    "\n",
    "recursive_splitter = RecursiveCharacterTextSplitter(\n",
    "    separators = [\"\\n\\n\", \"\\n\", \".\", \"?\", \"!\"],\n",
    "    chunk_size = 400,\n",
    "    chunk_overlap  = 20,\n",
    "    length_function = token_len,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get all the other attributes, excluding the abstracts, encodings and attention mask\n",
    "meta_df = ft.loc[:, ['corpusid', 'title', 'referencecount', 'citationcount', 'influentialcitationcount']]\n",
    "\n",
    "# create metadata for object\n",
    "metadata = meta_df.iloc[0, :].to_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metadata"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create Document class for creating embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "\n",
    "# create a document class that will split the text into chunks and add metadata, create embeddings, and create ids\n",
    "@dataclass\n",
    "class Document:\n",
    "    text: str\n",
    "    metadata: dict\n",
    "\n",
    "    def __init__(self, text, metadata, **kwargs):\n",
    "        self.text = text\n",
    "        self.metadata = metadata\n",
    "        self.embedder = embedding_functions.DefaultEmbeddingFunction()\n",
    "        self.__dict__.update(kwargs)\n",
    "\n",
    "    def __str__(self) -> str:\n",
    "        ...\n",
    "\n",
    "    # def __repr__(self) -> str:\n",
    "    #     return self.documents, self.ids, self.embeddings, self.metadata\n",
    "\n",
    "    def create_documents(self) -> list:\n",
    "        \"\"\"Split text into chunks and add metadata\"\"\"\n",
    "        self.documents = text_splitter.create_documents([self.text])\n",
    "        \n",
    "        # add metadata to each document\n",
    "        for i, d in enumerate(self.documents):\n",
    "            d.metadata = self.metadata | {\"chunk_id\": i}\n",
    "        \n",
    "        self.papers = [d.page_content for d in self.documents]\n",
    "        \n",
    "        self.metadatas = [d.metadata for d in self.documents]\n",
    "        \n",
    "        \"\"\"Create embeddings for each chunk\"\"\"\n",
    "        self.embeddings = self.embedder(self.papers)\n",
    "        \n",
    "        \"\"\"Create unique ids for each chunk\"\"\"\n",
    "        self.ids = [\n",
    "            f\"{d.metadata['corpusid']}_{i}\" for i,d in enumerate(self.documents)\n",
    "        ]        \n",
    "        \n",
    "    def tokenize(self) -> list:\n",
    "        \"\"\" tokenize text of chunks to store\"\"\"\n",
    "        self.tokenized_text = [T5tokens.encode(d) for d in self.papers]\n",
    "\n",
    "    def main(self) -> tuple:\n",
    "        \"\"\"Run all the functions\"\"\"\n",
    "        self.create_documents()\n",
    "\n",
    "        # to import into add to collection function easier. Loop over documents to create list of dictionaries to add to collection. \n",
    "        self.rep = {\n",
    "            \"documents\": self.papers, # list of all documents [doc1, doc2, doc3, ...]\n",
    "            'embeddings': self.embeddings, # list of list for all embeddings [[emb1, emb2], [emb3, ...],...]\n",
    "            'ids': self.ids, # list of all ids [id1, id2, id3, ...]\n",
    "            'metadatas': self.metadatas # list of dictionaries with metadata for each document\n",
    "        }\n",
    "        \n",
    "        return self.rep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = Document(att['abstract'][0], metadata = metadata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "keep_cols = ['corpusid', 'title', 'referencecount', 'citationcount', 'influentialcitationcount']\n",
    "\n",
    "for i, k in ft.iterrows():\n",
    "    try: \n",
    "        doc = Document(k['text'], k[keep_cols].to_dict())\n",
    "        collection.add(**doc.main())\n",
    "    except: \n",
    "        print(f\"Error with {k['corpusid']}\")\n",
    "        continue"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Query the collections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "collection.get(\n",
    "    include=['metadatas']   \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# query example of finding articles by corpus id\n",
    "collection.get(\n",
    "    where={\"corpusid\": \"237156001\"},\n",
    "    include=[\"documents\", \"embeddings\", \"metadatas\"]\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Llama index"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using Llama index for ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index import VectorStoreIndex, SimpleDirectoryReader\n",
    "from llama_index.vector_stores import ChromaVectorStore\n",
    "from IPython.display import Markdown, display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.storage.storage_context import StorageContext\n",
    "\n",
    "vector_store = ChromaVectorStore(chroma_collection=collection)\n",
    "storage_context = StorageContext.from_defaults(vector_store=vector_store)\n",
    "index = VectorStoreIndex.from_documents(documents, storage_context=storage_context)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Different Embeddings"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create embeddings to upload to chroma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_size = 200\n",
    "\n",
    "# T5Abstract_model = TFT5ForConditionalGeneration.from_pretrained('t5-base')\n",
    "T5tokens = T5Tokenizer.from_pretrained('t5-base')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custome Embedding Function\n",
    "from chromadb.api.types import Documents, EmbeddingFunction, Embeddings\n",
    "\n",
    "class MyEmbeddingFunction(EmbeddingFunction):\n",
    "    def __call__(self, texts: Documents) -> Embeddings:\n",
    "        # embed the documents somehow\n",
    "        model = T5Model.from_pretrained(\"t5-small\")\n",
    "        tok = T5Tokenizer.from_pretrained(\"t5-small\")\n",
    "\n",
    "        enc = tok(texts, return_tensors=\"pt\")\n",
    "\n",
    "        # forward pass through encoder only\n",
    "        output = model.encoder(\n",
    "            input_ids=enc[\"input_ids\"], \n",
    "            attention_mask=enc[\"attention_mask\"], \n",
    "            return_dict=True\n",
    "        )\n",
    "        # get the final hidden states\n",
    "        embeddings = output.last_hidden_state\n",
    "        return embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def t5summary_model(tokenizer, text, t5model):\n",
    "    summarize = \"summarize: \"\n",
    "    encoding = tokenizer([summarize+text], return_tensors='tf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bio bert toeknizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create collection to store embeddings with T5\n",
    "collection = chroma_client.create_collection(\n",
    "    name=\"abstract_collection_t5\",\n",
    "    metadata={\"hnsw:space\":\"cosine\"}) #customize distance method of embedding space \n",
    "\n",
    "\n",
    "# Get collection\n",
    "collection = chroma_client.get_collection(name=\"my_collection\", embedding_function=emb_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Embeddings with Sentence Transformers\n",
    "sentence_transformer_ef = embedding_functions.SentenceTransformerEmbeddingFunction(model_name=\"all-MiniLM-L6-v2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Embeddings with OpenAI\n",
    "openai_ef = embedding_functions.OpenAIEmbeddingFunction(\n",
    "                api_key=\"YOUR_API_KEY\",\n",
    "                model_name=\"text-embedding-ada-002\"\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Biogpt tokenizer\n",
    "biogpttokenizer = BioGptTokenizer.from_pretrained(\"microsoft/biogpt\")\n",
    "biogptmodel = BioGptForCausalLM.from_pretrained(\"microsoft/biogpt\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Queries\n",
    "https://docs.trychroma.com/usage-guide"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Query Embeddings\n",
    "collection.query(\n",
    "    query_embeddings=[[11.1, 12.1, 13.1],[1.1, 2.3, 3.2] ...]\n",
    "    n_results=10,\n",
    "    where={\"metadata_field\": \"is_equal_to_this\"},\n",
    "    where_document={\"$contains\":\"search_string\"}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# query by ids\n",
    "collection.get(\n",
    "    ids=[\"id1\", \"id2\", \"id3\", ...],\n",
    "    where={\"style\": \"style1\"}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Query by texts\n",
    "collection.query(\n",
    "    query_texts=[\"doc10\", \"thus spake zarathustra\", ...]\n",
    "    n_results=10,\n",
    "    where={\"metadata_field\": \"is_equal_to_this\"},\n",
    "    where_document={\"$contains\":\"search_string\"}\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "chroma",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
