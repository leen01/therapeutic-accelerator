{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import chromadb\n",
    "from chromadb.config import Settings\n",
    "from chromadb.utils import embedding_functions\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "import sqlalchemy as sa\n",
    "import json\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import T5Tokenizer\n",
    "\n",
    "max_sequence_length = 1200\n",
    "embedding_size = 200\n",
    "\n",
    "# Create tokenizer for T5 model\n",
    "T5tokens = T5Tokenizer.from_pretrained('t5-base', model_max_length = max_sequence_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# I got tired of copying the same code all the time\n",
    "# loads config for main parts and creates engine for sqlalchemy\n",
    "%run /home/ubuntu/work/therapeutic_accelerator/scripts/create_sqlalchemy_engine.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nanosecond heartbeat on server 1688240068852252346000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[Collection(name=langchain_store), Collection(name=abstracts)]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create chroma client\n",
    "chroma = chromadb.Client(Settings(chroma_api_impl=\"rest\",\n",
    "                                  chroma_server_host=\"52.87.156.250\", # EC2 instance public IPv4\n",
    "                                  chroma_server_http_port=8000))\n",
    "\n",
    "print(\"Nanosecond heartbeat on server\", chroma.heartbeat()) # returns a nanosecond heartbeat. Useful for making sure the client remains connected.\n",
    "\n",
    "# Check Existing connections\n",
    "chroma.list_collections()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Embedding fuctions\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "\n",
    "# AllenAI Specter\n",
    "tokenizer = AutoTokenizer.from_pretrained('allenai/specter')\n",
    "model = AutoModel.from_pretrained('allenai/specter')\n",
    "\n",
    "def get_embeddings(test):\n",
    "    inputs = tokenizer(test, padding=True, truncation=True, return_tensors=\"pt\", max_length=512)\n",
    "    result = model(**inputs)\n",
    "    embeddings = result.last_hidden_state[:, 0, :]\n",
    "    print (embeddings)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sentence Transformers all-MiniLM-L6-v2 \n",
    "default_ef = embedding_functions.DefaultEmbeddingFunction()\n",
    "\n",
    "# Create collection to store embeddings with T5 sentence transformer\n",
    "def create_collection(chroma, name, metadata = {\"hnsw:space\":\"cosine\"}, embedding_function = default_ef):\n",
    "    try:\n",
    "        chroma.create_collection(name=name, metadata=metadata, embedding_function=embedding_function)\n",
    "    except Exception as e:\n",
    "        logging.error(e)\n",
    "\n",
    "collection = chroma.get_or_create_collection(\"abstracts\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create embeddings table in SQL for abstracts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# chroma.delete_collection('abstract_sentence')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Add data to collection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dictionary(text):\n",
    "    \"\"\" Turn string containing list of dictionaries into a dictionary\"\"\"\n",
    "    \n",
    "    # remove new line characters\n",
    "    categories = re.sub(r'[\\[\\]\\'\\\\]', '', text)\n",
    "\n",
    "    # remove outer brackets, quotes, and split on commas\n",
    "    categories = categories.strip('{}').strip('\"').split('\",\"')\n",
    "\n",
    "    # create list with unique values from category\n",
    "    # categories = pd.Series([json.loads(t)['category'] for t in categories]).unique().tolist()\n",
    "    categories = [json.loads(t) for t in categories]\n",
    "    \n",
    "    return categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    TO THE EDITOR: We acknowledge Brewer and colle...\n",
       "1    Treatment of Alzheimer's disease sometimes use...\n",
       "2    Case report A 37-year-old woman was admitted a...\n",
       "3    Parkinson's disease (PD) is a common neurodege...\n",
       "4    Surgical site infection (SSI) is one of the mo...\n",
       "Name: abstract, dtype: object"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get attributes table for the metadata embeddings\n",
    "table_name = 'abstracts_encodings'\n",
    "\n",
    "sql = sa.text(f''' \n",
    "    SELECT * from {table_name} LEFT JOIN attributes ON ({table_name}.\"corpusId\" = CAST(attributes.corpusid as text)) LIMIT 10;\n",
    "''')\n",
    "\n",
    "with engine.connect() as conn: \n",
    "    query = conn.execute(sql)\n",
    "    \n",
    "att = pd.DataFrame(query.fetchall())\n",
    "\n",
    "# remove unncecessary columns\n",
    "att.drop(columns = ['paperId', 'corpusId', 'index', 'id'], inplace = True)\n",
    "\n",
    "# turn strings into list of dictionaries\n",
    "att['s2fieldsofstudy'] = att['s2fieldsofstudy'].apply(create_dictionary).apply(lambda x: pd.Series([d['category'] for d in x]).unique().tolist())\n",
    "att['authors'] = att['authors'].apply(create_dictionary)\n",
    "\n",
    "abstracts = att.abstract\n",
    "abstracts.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get attributes table for the metadata embeddings\n",
    "table_name = 'abstracts_encodings'\n",
    "\n",
    "sql = sa.text(f''' \n",
    "    SELECT * from {table_name} LIMIT 10;\n",
    "''')\n",
    "\n",
    "with engine.connect() as conn: \n",
    "    query = conn.execute(sql)\n",
    "    \n",
    "att = pd.DataFrame(query.fetchall())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>paperId</th>\n",
       "      <th>corpusId</th>\n",
       "      <th>abstract</th>\n",
       "      <th>input_ids</th>\n",
       "      <th>attention_mask</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>a6b287f3dbb07bf0aa536d4874c76398cf340a0e</td>\n",
       "      <td>87039880</td>\n",
       "      <td>SummaryIn a side-by-side test in Venezuela, 59...</td>\n",
       "      <td>[20698, 1570, 3, 9, 596, 18, 969, 18, 1583, 79...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0a529e2a3fb8d7f4e9de8e0e2fd0e25af09fcf15</td>\n",
       "      <td>35795631</td>\n",
       "      <td>Background: Troublesome faecal incontinence fo...</td>\n",
       "      <td>[23023, 10, 31520, 5529, 3, 89, 9, 15, 1489, 1...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>f21f71491d1820b6ebee4eb7cb2708eff58e6f78</td>\n",
       "      <td>26888798</td>\n",
       "      <td>According to medical historian David Reisman,1...</td>\n",
       "      <td>[2150, 12, 1035, 18637, 1955, 419, 159, 348, 6...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ba78943455484fc0335b1a5ae2c29c7668b1fb04</td>\n",
       "      <td>105030750</td>\n",
       "      <td>Bisphenol A (BPA) usually exists in daily plas...</td>\n",
       "      <td>[6483, 31411, 71, 41, 279, 3965, 61, 1086, 808...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>6507868b89d46332dbd5474bba4565d6e15fea34</td>\n",
       "      <td>233173712</td>\n",
       "      <td>This review highlights the emergent role of th...</td>\n",
       "      <td>[100, 1132, 8285, 8, 13591, 29, 17, 1075, 13, ...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>338b15e85c9a6063cc27ee8660c1d3d35d00be88</td>\n",
       "      <td>210145595</td>\n",
       "      <td>Background With prolonged survival and aging o...</td>\n",
       "      <td>[23023, 438, 22914, 9990, 11, 3, 5855, 13, 760...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>3430d1bb35ed7bbeb39f4ea9a89859a91fe0f974</td>\n",
       "      <td>1965212</td>\n",
       "      <td>BACKGROUND: Fucosylated glycans, including H‐a...</td>\n",
       "      <td>[3, 26953, 18256, 4630, 13110, 10, 6343, 25409...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>3c02f3479bb6cc9625f24073d1b669364d676bbd</td>\n",
       "      <td>219574750</td>\n",
       "      <td>Abstract Tuberculosis (TB) induced by Mycobact...</td>\n",
       "      <td>[20114, 21488, 52, 1497, 32, 7, 159, 41, 9041,...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>cbae9fb8044e7b27bf18039408bee36f56a89f87</td>\n",
       "      <td>174807764</td>\n",
       "      <td>Although long noncoding RNA TUC338 has been ch...</td>\n",
       "      <td>[1875, 307, 529, 9886, 3, 11840, 332, 6463, 51...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>1324dcf3b9b2f89a53e0d132747dd5303e50304b</td>\n",
       "      <td>59062146</td>\n",
       "      <td>Background: Dyspepsia is a prevalent complaint...</td>\n",
       "      <td>[23023, 10, 12991, 7, 855, 102, 7, 23, 9, 19, ...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                    paperId   corpusId  \\\n",
       "0  a6b287f3dbb07bf0aa536d4874c76398cf340a0e   87039880   \n",
       "1  0a529e2a3fb8d7f4e9de8e0e2fd0e25af09fcf15   35795631   \n",
       "2  f21f71491d1820b6ebee4eb7cb2708eff58e6f78   26888798   \n",
       "3  ba78943455484fc0335b1a5ae2c29c7668b1fb04  105030750   \n",
       "4  6507868b89d46332dbd5474bba4565d6e15fea34  233173712   \n",
       "5  338b15e85c9a6063cc27ee8660c1d3d35d00be88  210145595   \n",
       "6  3430d1bb35ed7bbeb39f4ea9a89859a91fe0f974    1965212   \n",
       "7  3c02f3479bb6cc9625f24073d1b669364d676bbd  219574750   \n",
       "8  cbae9fb8044e7b27bf18039408bee36f56a89f87  174807764   \n",
       "9  1324dcf3b9b2f89a53e0d132747dd5303e50304b   59062146   \n",
       "\n",
       "                                            abstract  \\\n",
       "0  SummaryIn a side-by-side test in Venezuela, 59...   \n",
       "1  Background: Troublesome faecal incontinence fo...   \n",
       "2  According to medical historian David Reisman,1...   \n",
       "3  Bisphenol A (BPA) usually exists in daily plas...   \n",
       "4  This review highlights the emergent role of th...   \n",
       "5  Background With prolonged survival and aging o...   \n",
       "6  BACKGROUND: Fucosylated glycans, including H‐a...   \n",
       "7  Abstract Tuberculosis (TB) induced by Mycobact...   \n",
       "8  Although long noncoding RNA TUC338 has been ch...   \n",
       "9  Background: Dyspepsia is a prevalent complaint...   \n",
       "\n",
       "                                           input_ids  \\\n",
       "0  [20698, 1570, 3, 9, 596, 18, 969, 18, 1583, 79...   \n",
       "1  [23023, 10, 31520, 5529, 3, 89, 9, 15, 1489, 1...   \n",
       "2  [2150, 12, 1035, 18637, 1955, 419, 159, 348, 6...   \n",
       "3  [6483, 31411, 71, 41, 279, 3965, 61, 1086, 808...   \n",
       "4  [100, 1132, 8285, 8, 13591, 29, 17, 1075, 13, ...   \n",
       "5  [23023, 438, 22914, 9990, 11, 3, 5855, 13, 760...   \n",
       "6  [3, 26953, 18256, 4630, 13110, 10, 6343, 25409...   \n",
       "7  [20114, 21488, 52, 1497, 32, 7, 159, 41, 9041,...   \n",
       "8  [1875, 307, 529, 9886, 3, 11840, 332, 6463, 51...   \n",
       "9  [23023, 10, 12991, 7, 855, 102, 7, 23, 9, 19, ...   \n",
       "\n",
       "                                      attention_mask  \n",
       "0  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...  \n",
       "1  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...  \n",
       "2  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...  \n",
       "3  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...  \n",
       "4  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...  \n",
       "5  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...  \n",
       "6  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...  \n",
       "7  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...  \n",
       "8  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...  \n",
       "9  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...  "
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "att"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sqlalchemy import Table, Column, Integer, String, MetaData, ARRAY\n",
    "\n",
    "# Create a metadata object\n",
    "metadata = MetaData()\n",
    "\n",
    "# Define a table using the metadata object\n",
    "abstract_encodings = Table(\n",
    "    'abstracts_encodings',\n",
    "    metadata,\n",
    "    Column('paperId', String, primary_key=True),\n",
    "    Column('corpusId', Integer),\n",
    "    Column('abstract', String),\n",
    "    Column('input_ids', ARRAY(Integer)),\n",
    "    Column('attention_mask', ARRAY(Integer))\n",
    ")\n",
    "\n",
    "# Create the table in the database\n",
    "metadata.create_all(engine)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # # Get attributes table for the metadata embeddings\n",
    "# table_name = 'abstracts_encodings'\n",
    "\n",
    "# # sql query text that sets the creates an index column\n",
    "# sql = text(''' \n",
    "#     CREATE INDEX id \n",
    "#     ON abstracts_encodings(\"corpusId\");\n",
    "# ''')\n",
    "\n",
    "# with engine.connect() as conn: \n",
    "#     query = conn.execute(sql)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_5028/1542492159.py:5: SADeprecationWarning: The AutomapBase.prepare.reflect parameter is deprecated and will be removed in a future release.  Reflection is enabled when AutomapBase.prepare.autoload_with is passed.\n",
      "  Base.prepare(engine, reflect=True)\n"
     ]
    }
   ],
   "source": [
    "from sqlalchemy.ext.automap import automap_base\n",
    "\n",
    "# Reflect the existing database schema\n",
    "Base = automap_base()\n",
    "Base.prepare(engine, reflect=True)\n",
    "\n",
    "# Access the existing table you want to create a class for\n",
    "TableClass = Base.classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "attributes = TableClass.attributes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sqlalchemy import Column, Integer, String, ForeignKey\n",
    "from sqlalchemy.orm import declarative_base\n",
    "from sqlalchemy.orm import relationship\n",
    "\n",
    "# Define the database connection\n",
    "Base = declarative_base()\n",
    "\n",
    "# Define the models for the tables\n",
    "class abstract_encodings(Base):\n",
    "    __tablename__ = 'abstracts_encodings'\n",
    "    id = Column(Integer, primary_key=True)\n",
    "    paperId = Column(String, primary_key=True)\n",
    "    corpusId = Column(Integer)\n",
    "    abstract = Column(String)\n",
    "    input_ids = Column(ARRAY(Integer))\n",
    "    attention_mask = Column(ARRAY(Integer))\n",
    "    attributes = relationship(ForeignKey(\"attributes.corpusid\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sqlalchemy import select\n",
    "stmt = (\n",
    "    select(abstract_encodings)\n",
    "    .join(abstract_encodings.attributes)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sqlalchemy.orm import sessionmaker\n",
    "Session = sessionmaker(bind=engine)\n",
    "session = Session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import dask.dataframe as dd\n",
    "# from dask.diagnostics import ProgressBar\n",
    "# import pandas as pd\n",
    "\n",
    "# # Read the joined tables using Dask\n",
    "# with ProgressBar():\n",
    "#     df = dd.read_sql_query(query, database_uri, index_col='idx_corpusid')\n",
    "\n",
    "# # Convert Dask dataframe to pandas dataframe\n",
    "# # df = df.compute()\n",
    "\n",
    "# # Print the resulting dataframe\n",
    "# # print(df.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# With dask dataframe to partition the query into pieces and prevent maxing out machine\n",
    "import dask.dataframe as dd\n",
    "\n",
    "sql = sa.text(f''' \n",
    "    SELECT * from {table_name} LEFT JOIN attributes ON ({table_name}.\"corpusId\" = CAST(attributes.corpusid as text));\n",
    "''')\n",
    "\n",
    "df = dd.read_sql_query(sql, str(engine.url), index_col = 'index', head_rows=10)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing embedding creation for abstracts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.text_splitter import CharacterTextSplitter, RecursiveCharacterTextSplitter\n",
    "import tiktoken\n",
    "\n",
    "def token_len(text): \n",
    "    \"\"\" Get the length of tokens from text\"\"\"\n",
    "    tokens = T5tokens.encode(text)\n",
    "    return len(tokens)\n",
    "    \n",
    "# create text splitters for processing the texts\n",
    "text_splitter = CharacterTextSplitter(\n",
    "    separator = \"\\n\\n\",\n",
    "    chunk_size = 400,\n",
    "    chunk_overlap  = 200,\n",
    "    length_function = token_len,\n",
    ")\n",
    "\n",
    "recursive_splitter = RecursiveCharacterTextSplitter(\n",
    "    separators = [\"\\n\\n\", \"\\n\", \".\", \"?\", \"!\"],\n",
    "    chunk_size = 400,\n",
    "    chunk_overlap  = 20,\n",
    "    length_function = token_len,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get all the other attributes, excluding the abstracts, encodings and attention mask\n",
    "meta_df = att.loc[:, ['corpusid', 'title', 'referencecount', 'citationcount', 'influentialcitationcount']]\n",
    "\n",
    "# create metadata for object\n",
    "metadata = meta_df.iloc[0, :].to_dict()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create Document class for creating embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "\n",
    "# create a document class that will split the text into chunks and add metadata, create embeddings, and create ids\n",
    "@dataclass\n",
    "class Document:\n",
    "    text: str\n",
    "    metadata: dict\n",
    "\n",
    "    def __init__(self, text, metadata, **kwargs):\n",
    "        self.text = text\n",
    "        self.metadata = metadata\n",
    "        self.embedder = embedding_functions.DefaultEmbeddingFunction()\n",
    "        self.__dict__.update(kwargs)\n",
    "\n",
    "    def __str__(self) -> str:\n",
    "        ...\n",
    "\n",
    "    # def __repr__(self) -> str:\n",
    "    #     return self.documents, self.ids, self.embeddings, self.metadata\n",
    "\n",
    "    def create_documents(self) -> list:\n",
    "        \"\"\"Split text into chunks and add metadata\"\"\"\n",
    "        self.documents = text_splitter.create_documents([self.text])\n",
    "        \n",
    "        # add metadata to each document\n",
    "        for i, d in enumerate(self.documents):\n",
    "            d.metadata = self.metadata | {\"chunk_id\": i}\n",
    "        \n",
    "        self.papers = [d.page_content for d in self.documents]\n",
    "        \n",
    "        self.metadatas = [d.metadata for d in self.documents]\n",
    "        \n",
    "        \"\"\"Create embeddings for each chunk\"\"\"\n",
    "        self.embeddings = self.embedder(self.papers)\n",
    "        \n",
    "        \"\"\"Create unique ids for each chunk\"\"\"\n",
    "        self.ids = [\n",
    "            f\"{d.metadata['corpusid']}_{i}\" for i,d in enumerate(self.documents)\n",
    "        ]        \n",
    "        \n",
    "    def tokenize(self) -> list:\n",
    "        \"\"\" tokenize text of chunks to store\"\"\"\n",
    "        self.tokenized_text = [T5tokens.encode(d) for d in self.papers]\n",
    "\n",
    "    def main(self) -> tuple:\n",
    "        \"\"\"Run all the functions\"\"\"\n",
    "        self.create_documents()\n",
    "\n",
    "        # to import into add to collection function easier. Loop over documents to create list of dictionaries to add to collection. \n",
    "        self.rep = {\n",
    "            \"documents\": self.papers, # list of all documents [doc1, doc2, doc3, ...]\n",
    "            'embeddings': self.embeddings, # list of list for all embeddings [[emb1, emb2], [emb3, ...],...]\n",
    "            'ids': self.ids, # list of all ids [id1, id2, id3, ...]\n",
    "            'metadatas': self.metadatas # list of dictionaries with metadata for each document\n",
    "        }\n",
    "        \n",
    "        return self.rep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = Document(att['abstract'][0], metadata = metadata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "keep_cols = ['corpusid', 'title', 'referencecount', 'citationcount', 'influentialcitationcount']\n",
    "\n",
    "for i, k in att.iterrows():\n",
    "    try: \n",
    "        doc = Document(k['abstract'], k[keep_cols].to_dict())\n",
    "        collection.add(**doc.main())\n",
    "    except: \n",
    "        print(f\"Error with {k['corpusid']}\")\n",
    "        continue"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Query the collections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'ids': [], 'embeddings': None, 'metadatas': None, 'documents': []}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "collection.get(\n",
    "    include=['documents']   \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'ids': [], 'embeddings': [], 'metadatas': [], 'documents': []}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# query example of finding articles by corpus id\n",
    "collection.get(\n",
    "    where={\"corpusid\": \"11858060\"},\n",
    "    include=[\"documents\", \"embeddings\", \"metadatas\"]\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Llama index"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using Llama index for ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index import VectorStoreIndex, SimpleDirectoryReader\n",
    "from llama_index.vector_stores import ChromaVectorStore\n",
    "from IPython.display import Markdown, display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.storage.storage_context import StorageContext\n",
    "\n",
    "vector_store = ChromaVectorStore(chroma_collection=collection)\n",
    "storage_context = StorageContext.from_defaults(vector_store=vector_store)\n",
    "index = VectorStoreIndex.from_documents(documents, storage_context=storage_context)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Different Embeddings"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create embeddings to upload to chroma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_size = 200\n",
    "\n",
    "# T5Abstract_model = TFT5ForConditionalGeneration.from_pretrained('t5-base')\n",
    "T5tokens = T5Tokenizer.from_pretrained('t5-base')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custome Embedding Function\n",
    "from chromadb.api.types import Documents, EmbeddingFunction, Embeddings\n",
    "\n",
    "class MyEmbeddingFunction(EmbeddingFunction):\n",
    "    def __call__(self, texts: Documents) -> Embeddings:\n",
    "        # embed the documents somehow\n",
    "        model = T5Model.from_pretrained(\"t5-small\")\n",
    "        tok = T5Tokenizer.from_pretrained(\"t5-small\")\n",
    "\n",
    "        enc = tok(texts, return_tensors=\"pt\")\n",
    "\n",
    "        # forward pass through encoder only\n",
    "        output = model.encoder(\n",
    "            input_ids=enc[\"input_ids\"], \n",
    "            attention_mask=enc[\"attention_mask\"], \n",
    "            return_dict=True\n",
    "        )\n",
    "        # get the final hidden states\n",
    "        embeddings = output.last_hidden_state\n",
    "        return embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def t5summary_model(tokenizer, text, t5model):\n",
    "    summarize = \"summarize: \"\n",
    "    encoding = tokenizer([summarize+text], return_tensors='tf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bio bert toeknizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create collection to store embeddings with T5\n",
    "collection = chroma_client.create_collection(\n",
    "    name=\"abstract_collection_t5\",\n",
    "    metadata={\"hnsw:space\":\"cosine\"}) #customize distance method of embedding space \n",
    "\n",
    "\n",
    "# Get collection\n",
    "collection = chroma_client.get_collection(name=\"my_collection\", embedding_function=emb_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Embeddings with Sentence Transformers\n",
    "sentence_transformer_ef = embedding_functions.SentenceTransformerEmbeddingFunction(model_name=\"all-MiniLM-L6-v2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Embeddings with OpenAI\n",
    "openai_ef = embedding_functions.OpenAIEmbeddingFunction(\n",
    "                api_key=\"YOUR_API_KEY\",\n",
    "                model_name=\"text-embedding-ada-002\"\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Biogpt tokenizer\n",
    "biogpttokenizer = BioGptTokenizer.from_pretrained(\"microsoft/biogpt\")\n",
    "biogptmodel = BioGptForCausalLM.from_pretrained(\"microsoft/biogpt\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Queries\n",
    "https://docs.trychroma.com/usage-guide"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Query Embeddings\n",
    "collection.query(\n",
    "    query_embeddings=[[11.1, 12.1, 13.1],[1.1, 2.3, 3.2] ...]\n",
    "    n_results=10,\n",
    "    where={\"metadata_field\": \"is_equal_to_this\"},\n",
    "    where_document={\"$contains\":\"search_string\"}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# query by ids\n",
    "collection.get(\n",
    "    ids=[\"id1\", \"id2\", \"id3\", ...],\n",
    "    where={\"style\": \"style1\"}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Query by texts\n",
    "collection.query(\n",
    "    query_texts=[\"doc10\", \"thus spake zarathustra\", ...]\n",
    "    n_results=10,\n",
    "    where={\"metadata_field\": \"is_equal_to_this\"},\n",
    "    where_document={\"$contains\":\"search_string\"}\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "chroma",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
