{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create Embeddings Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "/home/ubuntu/anaconda3/lib/python3.10/site-packages/transformers/models/t5/tokenization_t5.py:163: FutureWarning: This tokenizer was incorrectly instantiated with a model max length of 512 which will be corrected in Transformers v5.\n",
      "For now, this behavior is kept to avoid breaking backwards compatibility when padding/encoding with `truncation is True`.\n",
      "- Be aware that you SHOULD NOT rely on t5-base automatically truncating your input to 512 when padding/encoding.\n",
      "- If you want to encode/pad to sequences longer than 512 you can either instantiate this tokenizer with `model_max_length` or pass `max_length` when encoding/padding.\n",
      "- To avoid this warning, please instantiate this tokenizer with `model_max_length` set to your preferred value.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# import pytorch\n",
    "from transformers import T5Tokenizer # AutoModel, AutoTokenizer, BertTokenizer,BioGptModel, BioGptConfig, BioGptTokenizer\n",
    "T5tokens = T5Tokenizer.from_pretrained('t5-base')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tokenizers and Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# bio_bert_model = AutoModel.from_pretrained(\"gsarti/biobert-nli\")\n",
    "# bio_bert_tokenizer = AutoTokenizer.from_pretrained(\"gsarti/biobert-nli\")\n",
    "# original_bert_tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "# T5Abstract_model = TFT5ForConditionalGeneration.from_pretrained('t5-base')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# biogpttokenizer = BioGptTokenizer.from_pretrained(\"microsoft/biogpt\")\n",
    "# biogptmodel = BioGptModel.from_pretrained(\"microsoft/biogpt\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get Abstracts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import yaml\n",
    "from sqlalchemy import create_engine\n",
    "import pandas as pd\n",
    "import dask.dataframe as dd\n",
    "\n",
    "# set up\n",
    "with open(\"/home/ubuntu/work/therapeutic_accelerator/config/main.yaml\", \"r\") as f:\n",
    "    config = yaml.load(f, Loader=yaml.FullLoader)\n",
    "    \n",
    "with open(\"../config/keys.yaml\", \"r\") as f:\n",
    "    keys = yaml.load(f, Loader=yaml.FullLoader)\n",
    "\n",
    "bucket_path = os.path.join(config['paths']['root'], config['paths']['mount'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create engine to connect to database\n",
    "engine = create_engine(f'postgresql://postgres:{keys[\"postgres\"]}@{config[\"database\"][\"host\"]}:5432/postgres')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'postgresql://postgres:qRd71PqwOsbv62WAvboR@database-1.cuaho2dof33c.us-east-1.rds.amazonaws.com:5432/postgres'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "str(engine.url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'str' object has no attribute 'limit'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[16], line 5\u001b[0m\n\u001b[1;32m      2\u001b[0m sql \u001b[39m=\u001b[39m \u001b[39m'''\u001b[39m\u001b[39m SELECT * FROM abstracts LIMIT 5\u001b[39m\u001b[39m'''\u001b[39m\n\u001b[1;32m      3\u001b[0m \u001b[39m# with engine.connect() as conn: \u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[39m#     # results = conn.execute(query)\u001b[39;00m\n\u001b[0;32m----> 5\u001b[0m ddf \u001b[39m=\u001b[39m dask\u001b[39m.\u001b[39;49mdataframe\u001b[39m.\u001b[39;49mread_sql_query(sql, con \u001b[39m=\u001b[39;49m \u001b[39mf\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39mpostgresql://postgres:\u001b[39;49m\u001b[39m{\u001b[39;49;00mkeys[\u001b[39m\"\u001b[39;49m\u001b[39mpostgres\u001b[39;49m\u001b[39m\"\u001b[39;49m]\u001b[39m}\u001b[39;49;00m\u001b[39m@\u001b[39;49m\u001b[39m{\u001b[39;49;00mconfig[\u001b[39m\"\u001b[39;49m\u001b[39mdatabase\u001b[39;49m\u001b[39m\"\u001b[39;49m][\u001b[39m\"\u001b[39;49m\u001b[39mhost\u001b[39;49m\u001b[39m\"\u001b[39;49m]\u001b[39m}\u001b[39;49;00m\u001b[39m:5432/postgres\u001b[39;49m\u001b[39m'\u001b[39;49m, index_col \u001b[39m=\u001b[39;49m \u001b[39m'\u001b[39;49m\u001b[39mindex\u001b[39;49m\u001b[39m'\u001b[39;49m)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.10/site-packages/dask/dataframe/io/sql.py:121\u001b[0m, in \u001b[0;36mread_sql_query\u001b[0;34m(sql, con, index_col, divisions, npartitions, limits, bytes_per_chunk, head_rows, meta, engine_kwargs, **kwargs)\u001b[0m\n\u001b[1;32m    117\u001b[0m kwargs[\u001b[39m\"\u001b[39m\u001b[39mindex_col\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m index\u001b[39m.\u001b[39mname\n\u001b[1;32m    119\u001b[0m \u001b[39mif\u001b[39;00m head_rows \u001b[39m>\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[1;32m    120\u001b[0m     \u001b[39m# derive metadata from first few rows\u001b[39;00m\n\u001b[0;32m--> 121\u001b[0m     q \u001b[39m=\u001b[39m sql\u001b[39m.\u001b[39;49mlimit(head_rows)\n\u001b[1;32m    122\u001b[0m     head \u001b[39m=\u001b[39m pd\u001b[39m.\u001b[39mread_sql(q, engine, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[1;32m    124\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(head) \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[1;32m    125\u001b[0m         \u001b[39m# no results at all\u001b[39;00m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'str' object has no attribute 'limit'"
     ]
    }
   ],
   "source": [
    "# pull all the abstracts\n",
    "sql = ''' SELECT * FROM abstracts LIMIT 5'''\n",
    "# with engine.connect() as conn: \n",
    "#     # results = conn.execute(query)\n",
    "ddf = dask.dataframe.read_sql_query(sql, con = f'postgresql://postgres:{keys[\"postgres\"]}@{config[\"database\"][\"host\"]}:5432/postgres', index_col = 'index')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "read_sql_table() missing 1 required positional argument: 'index_col'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[15], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m dask\u001b[39m.\u001b[39;49mdataframe\u001b[39m.\u001b[39;49mread_sql_table(\u001b[39m'\u001b[39;49m\u001b[39mfulltext\u001b[39;49m\u001b[39m'\u001b[39;49m, \n\u001b[1;32m      2\u001b[0m                               \u001b[39mf\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39mpostgresql://postgres:\u001b[39;49m\u001b[39m{\u001b[39;49;00mkeys[\u001b[39m\"\u001b[39;49m\u001b[39mpostgres\u001b[39;49m\u001b[39m\"\u001b[39;49m]\u001b[39m}\u001b[39;49;00m\u001b[39m@\u001b[39;49m\u001b[39m{\u001b[39;49;00mconfig[\u001b[39m\"\u001b[39;49m\u001b[39mdatabase\u001b[39;49m\u001b[39m\"\u001b[39;49m][\u001b[39m\"\u001b[39;49m\u001b[39mhost\u001b[39;49m\u001b[39m\"\u001b[39;49m]\u001b[39m}\u001b[39;49;00m\u001b[39m:5432/postgres\u001b[39;49m\u001b[39m'\u001b[39;49m)\n",
      "\u001b[0;31mTypeError\u001b[0m: read_sql_table() missing 1 required positional argument: 'index_col'"
     ]
    }
   ],
   "source": [
    "dask.dataframe.read_sql_table('fulltext', \n",
    "                              f'postgresql://postgres:{keys[\"postgres\"]}@{config[\"database\"][\"host\"]}:5432/postgres', index_col = '')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ddf = dask.dataframe.read_sql_query(sql, con = conn, index_col = 'corpusid')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Turn into dataframe\n",
    "abstracts = pd.DataFrame(results.fetchall())\n",
    "print(abstracts.shape)\n",
    "abstracts = abstracts.dropna(how = 'all', subset='abstract').reset_index(drop = True)\n",
    "print(abstracts.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "abstracts.loc[0, 'abstract']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encode abstracts\n",
    "temp = T5tokens.encode(abstracts.loc[0, 'abstract'], return_tensors=\"pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = abstracts.abstract.apply(len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "sns.histplot(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lengths of Abstracts\n",
    "test.describe()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
