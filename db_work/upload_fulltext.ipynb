{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Upload full text to DB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import yaml\n",
    "from sqlalchemy import create_engine\n",
    "\n",
    "import pandas as pd\n",
    "import dask.dataframe as dd\n",
    "import sys\n",
    "import csv\n",
    "import janitor # to clean df column names to snake case\n",
    "import numpy as np\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "with open(\"/home/ubuntu/work/therapeutic_accelerator/config/main.yaml\", \"r\") as f:\n",
    "    config = yaml.load(f, Loader=yaml.FullLoader)\n",
    "    \n",
    "with open(\"../config/keys.yaml\", \"r\") as f:\n",
    "    keys = yaml.load(f, Loader=yaml.FullLoader)\n",
    "    \n",
    "bucket_path = os.path.join(config['paths']['root'], config['paths']['mount'])\n",
    "\n",
    "# Create engine to connect to database\n",
    "engine = create_engine(f'postgresql://postgres:{keys[\"postgres\"]}@{config[\"database\"][\"host\"]}:5432/postgres')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parse text into sections\n",
    "import json\n",
    "import re\n",
    "\n",
    "def get_section_metadata(text, annotations): \n",
    "    # Pulls the text from the full text based on the indexes passed in as annotations object.\n",
    "    sections_list = []\n",
    "    for i in annotations: \n",
    "        section = {}\n",
    "        section['name'] = text[int(i['start']):int(i['end'])]\n",
    "        section['start'] = i['start']\n",
    "        section['end'] = i['end']\n",
    "        sections_list.append(section)\n",
    "        \n",
    "    return sections_list\n",
    "\n",
    "def find_sections(text_df): \n",
    "    # Create annotations index df to parse through\n",
    "    \n",
    "    sections_index = json.loads(text_df['annotations_sectionheader'][0])\n",
    "    \n",
    "    sections_df = pd.DataFrame(get_section_metadata(text_df['text'][0], sections_index))\n",
    "\n",
    "    # maintain corpus id as Primary Key in DB\n",
    "    sections_df['corpusid'] = text_df.corpusid[0]\n",
    "    \n",
    "    # rename colum for clarity\n",
    "    sections_df = sections_df.rename({'name':'section'}, axis = 1)\n",
    "    \n",
    "    return sections_df\n",
    "\n",
    "def refine_sections(section_df): \n",
    "    # find relevant sections based on pattern(s)\n",
    "    pattern = \"introduction|methods|results|discussion|conclusion\"\n",
    "\n",
    "    # create a new dataframe to hold values. Will reference original to get last section of text\n",
    "    section_filter = section_df.section.str.contains(pat = pattern, regex = True, flags=re.IGNORECASE)\n",
    "    \n",
    "    # print(section_filter)\n",
    "    \n",
    "    if section_filter.isnull().all(): \n",
    "        return True\n",
    "\n",
    "    # only major sections\n",
    "    sections_df_refined = section_df[section_filter]\n",
    "    \n",
    "    # Get indices of sections\n",
    "    indices = sections_df_refined.index.tolist()\n",
    "\n",
    "    # Recode values to reflect text location rather than section header\n",
    "    for i, v in enumerate(indices): \n",
    "        # index of section to start text\n",
    "        start = indices[i]\n",
    "        \n",
    "        # Point to stop text, beginning of next section\n",
    "        # case for last section in entire list\n",
    "        if i == len(indices)-1:  # for the last section\n",
    "            end = indices[i] + 1\n",
    "        else: \n",
    "            end = indices[i+1]\n",
    "        \n",
    "        sections_df_refined.loc[v, 'start'] = section_df.loc[start, 'end']\n",
    "        sections_df_refined.loc[v, 'end'] = section_df.loc[end, 'start']\n",
    "        \n",
    "    sections_df_refined[['start','end']] = sections_df_refined[['start','end']].astype(\"int\")\n",
    "\n",
    "    return sections_df_refined\n",
    "\n",
    "def convert_sections(text, sections_df_refined):\n",
    "    # Get text for sections\n",
    "    for i in sections_df_refined.index.tolist():\n",
    "        start = sections_df_refined.loc[i, 'start']\n",
    "        end = sections_df_refined.loc[i, 'end']\n",
    "        \n",
    "        try: \n",
    "            # pull section text to next major section. Remove any new line characters and white space on ends.\n",
    "            sections_df_refined.loc[i, 'text'] = text[start:end].replace('\\n', ' ').strip()\n",
    "        except: \n",
    "            print(\"could not extract text\")\n",
    "            return \"\"\n",
    "\n",
    "    # flatten dataframe into final form\n",
    "    sections_cleaned = sections_df_refined[['corpusid', 'section', 'text']]\n",
    "    \n",
    "    # convert to dataframe with sections as column names\n",
    "    # sections_cleaned.pivot(index = 'corpusid', columns = 'section', values = 'text').reset_index() \n",
    "    \n",
    "    return sections_cleaned\n",
    "\n",
    "def extract_sections(row):\n",
    "    \n",
    "    if pd.isnull(row.annotations_sectionheader): \n",
    "        return np.nan\n",
    "    \n",
    "    # convert Pandas to Pandas Dataframe for easier access\n",
    "    # for tuple iterator\n",
    "    text_df = pd.DataFrame([dict(row._asdict())])\n",
    "        \n",
    "    # get all sections\n",
    "    sections_df = find_sections(text_df)\n",
    "    \n",
    "    # refine only major sections\n",
    "    try: \n",
    "        sections_df_refined = refine_sections(sections_df)\n",
    "    except: \n",
    "        return np.nan\n",
    "    \n",
    "    if isinstance(sections_df_refined, bool): \n",
    "        # no results found\n",
    "        return np.nan\n",
    "    else: \n",
    "        try: \n",
    "            # convert sections to dataframe with corpusid as the PK and sections column headers\n",
    "            sections_cleaned = convert_sections(text_df.text[0], sections_df_refined)\n",
    "        except: \n",
    "            return np.nan\n",
    "    \n",
    "    return sections_cleaned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in rows/columns with large number of bytes\n",
    "maxInt = sys.maxsize\n",
    "\n",
    "while True:\n",
    "    # decrease the maxInt value by factor 10 \n",
    "    # as long as the OverflowError occurs.\n",
    "\n",
    "    try:\n",
    "        csv.field_size_limit(maxInt)\n",
    "        break\n",
    "    except OverflowError:\n",
    "        maxInt = int(maxInt/10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "csv_file = '/home/ubuntu/work/bucket/fulltext/final_full_text.csv'\n",
    "\n",
    "with open(csv_file, 'r') as f:\n",
    "    d_reader = csv.DictReader(f)\n",
    "\n",
    "    #get fieldnames from DictReader object and store in list\n",
    "    headers = d_reader.fieldnames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['corpusid', 'text', 'annotations_abstract', 'annotations_sectionheader']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Read in csv file with dask to allow such a big file to be read. \n",
    "df = dd.read_csv(csv_file, engine = 'python', usecols=range(2,len(headers)), dtype = str, sample=100000,blocksize=18e6)  \n",
    "\n",
    "# Change column names to snakecase to follow postgres conventions. Otherwise the column names will have to be in quotes during queries\n",
    "df = janitor.clean_names(df)\n",
    "\n",
    "main_cols = ['corpusid', 'text', 'annotations_abstract', 'annotations_sectionheader']\n",
    "\n",
    "df = df[main_cols]\n",
    "df.columns.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "extracted_text = []\n",
    "\n",
    "for row in df.itertuples():\n",
    "    if pd.notnull(row.annotations_sectionheader):\n",
    "        extracted_text.append(extract_sections(row))\n",
    "    else: \n",
    "        continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quick peak at data\n",
    "df_temp = df.partitions[0].compute()\n",
    "df_temp.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "thera",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
